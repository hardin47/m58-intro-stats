---
---
<!-- the two formats are html and revealjs -->

# Clicker Q

to go with **Introduction to Modern Statistics** by Ã‡entinkaya-Rundel & Hardin.  Math 58B - Introduction to Biostatistics.

```{=html}
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
```


---

1. If 16 infants with **no genuine preference** choose 16 toys, what is the most likely number of "helping" toys that will be chosen?[^1]
   (a)	4
   (b)	7
   (c)	8
   (d)	9
   (e)	10


[^1]:  c.	8

---

2. How likely is it that exactly 8 helpers will be chosen (if there is no preference)?[^2]
   (a)	0-15% 
   (b)	16-30% 
   (c)	31-49%
   (d)	50%
   (e)	51-100%


[^2]:  b. 0.196 (19.6% of the time)

---

3. What if we flipped a coin 160 times?  What percent of the time will the simulation flip exactly 80 heads?[^3]
   (a)	0-15% 
   (b)	16-30% 
   (c)	31-49%
   (d)	50%
   (e)	51-100%

[^3]: a. 0.063 (6.3% of the time)

---

4. Is our actual result of 14 (under the coin model)...[^4]
   (a) very surprising?
   (b) somewhat surprising?
   (c) not very surprising?
   
[^4]: a. very surprising (prob of 14 or more is 0.0021)

---

5.	Based on the first handwriting study, can we conclude that cursive causes higher scores (on average)?[^5]
    (a)	Yes
    (b)	No
    (c)	It depends

[^5]: b. No, we can't establish causation from an observational study.

---

6.	Based on the second handwriting study, can we conclude that cursive causes higher scores (on average)?[^6]
    (a)	Yes
    (b)	No
    (c)	It depends

[^6]: a. Yes.  For the exam(s?) under study, cursive caused higher scores on average.

---

7. A possible confounding variable for the handwriting study is:[^7]
   (a) grade of the student (age)
   (b) region of country where the SAT was taken
   (c) academic ability of the student
   (d) gender identity of the student
   (e) number of siblings of the student.

[^7]: You must connect the variable to both the explanatory and response variable.  For me, that is easiest to do with c. academic ability of the student.

---

8. The main reason we randomly assign the explanatory variable is:[^8]
   (a) To get the smallest p-value possible
   (b) To balance the expected causal mechanism across the two groups
   (c) To balance every possible variable except the causal mechanism across the two groups
   (d) So that our sample is representative of the population
   (e) So that the sampling process is unbiased 

[^8]: c. To balance every possible variable except the causal mechanism across the two groups

---

9. The main reason we take random samples from the population is:[^9]
   (a) To get the smallest p-value possible
   (b) To balance the expected causal mechanism across the two groups
   (c) To balance every possible variable except the expected causal mechanism across the two groups
   (d) So that our sample is representative of the population
   (e) So that the sampling process is unbiased

[^9]: d. So that our sample is representative of the population

---

10. Are there effects of second-hand smoke on the health of children?[^10]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^10]: c. unhappily obs study (becuase we want to establish causation)

---

11. Do people tend to spend more money in stores located next to food outlets with pleasing smells?[^11]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^11]: a. definitely obs study (do we care about causation?  maybe.  maybe not.)
---

12. Does cell phone use increase the rate of automobile accidents?[^12]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^12]: c. unhappily obs study

---

13. Do people consume different amounts of ice cream depending on the size of bowl used?[^13]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^13]: b. definitely experiment

---

14.	Which is more effective: diet A or diet B?[^14]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^14]: b. definitely experiment

---

15. Suppose that we record the midterm exam score and the final exam score for every student in a class.  What would the value of the correlation coefficient be if every student in the class scored ten points higher on the final than on the midterm:[^15]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^15]: e. r = 1

---

16. Suppose that we record the midterm exam score and the final exam score for every student in a class.  What would the value of the correlation coefficient be if every student in the class scored five points lower on the final than on the midterm:[^16]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1
    
[^16]: e. r = 1

---

17. Suppose that we record the midterm exam score and the final exam score for every student in a class.  What would the value of the correlation coefficient be if every student in the class scored twice as many points on the final than on the midterm:[^17]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^17]: e. r = 1

---

18. Suppose you guessed every value correctly (guess the correlation applet), what would be the value of the correlation coefficient between your guesses and the actual correlations?[^18]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^18]: e. r = 1

---

19. Suppose each of your guesses was too high by 0.2 from the actual value of the correlation coefficient, what would be the value of the correlation coefficient between your guesses and the actual correlations?[^19]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^19]: e. r = 1

---

20. A correlation coefficient equal to 1 indicates that you are a good guesser.[^20]
    (a) TRUE
    (b) FALSE

[^20]: b. FALSE.  You could get every single value wrong and still have a correlation of one.

---

21. Perfect Correlation... if not for a single outlier  
n = 101 observations:  1 observation in top left, 25 observations in each in of the points near the bottom right.    
The value of the correlation, r, is:[^21]
    (a)	-1   < r < -0.9
    (b)	-0.9 < r < -0.5
    (c)	-0.5 < r <  0.5
    (d)	 0.5 < r <  0.9
    (e)	 0.9 < r <  1


[^21]: c. r = -0.416    
---

```{r}
#| echo: false
#| warning: false
#| message: false
#| 
x_value <- c(rep(17, 25), rep(18, 25), rep(19, 25), rep(20, 25), 1)
y_value <- c(rep(1, 25), rep(2, 25), rep(3, 25), rep(4, 25), 20)

#cor(x_value, y_value)
library(ggplot2)
ggplot(data.frame(x_value, y_value)) + 
  geom_point(aes(x = x_value, y = y_value)) +
  theme(aspect.ratio = 1)
```



---

22. The sum of residuals from the sample mean (no X):[^22] 
$$\sum_{i=1}^n(Y_i - \overline{Y})$$
    (a) is positive
    (b) is negative
    (c) is zero
    (d) is different for every dataset

[^22]: c. always zero

---

23. A good measure of how well the prediction (of the sample mean) fits the data is:[^23]
    (a) $\sum_{i=1}^n(Y_i - \overline{Y})$
    (b) $\sum_{i=1}^n(Y_i - \overline{Y})^2$
    (c) $\sum_{i=1}^n|Y_i - \overline{Y}|$
    (d) $\mbox{median}(Y_i - \overline{Y})$
    (e) $\mbox{median}|Y_i - \overline{Y}|$

[^23]: we usually use b. $\sum_{i=1}^n(Y_i - \overline{Y})^2$ (for calculus and historical reasons), but c. and e. are also totally reasonably answers.

---


24. A good measure of how well the prediction (of the regression line) fits the data is:[^24]
    (a) $\sum_{i=1}^n(Y_i - \hat{Y}_i)$
    (b) $\sum_{i=1}^n(Y_i - \hat{Y}_i)^2$
    (c) $\sum_{i=1}^n|Y_i - \hat{Y}_i|$
    (d) $\mbox{median}(Y_i -\hat{Y}_i)$
    (e) $\mbox{median}|Y_i -\hat{Y}_i|$

[^24]: we usually use b. $\sum_{i=1}^n(Y_i - \hat{Y}_i)^2$ (for calculus and historical reasons), but c. and e. are also totally reasonably answers.

---

25. What math is used to find the value of $m$ that minimizes:[^25]
$$\sum_{i=1}^n(Y_i - m)^2$$
    (a) combinatorics
    (b) derivative
    (c) integral
    (d) linear algebra

[^25]: b. derivative

---

26. $\sum_i(Y_i - \overline{Y})^2$ is sometimes $\geq \sum_i(Y_i - \hat{Y}_i)^2$[^26]
    a. TRUE
    b. FALSE, $\sum_i(Y_i - \overline{Y})^2$ is always $\geq \sum_i(Y_i - \hat{Y}_i)^2$
    c. FALSE, $\sum_i(Y_i - \overline{Y})^2$ is never $\geq \sum_i(Y_i - \hat{Y}_i)^2$

[^26]: b. FALSE, $\sum_i(Y_i - \overline{Y})^2$ is always $\geq \sum_i(Y_i - \hat{Y}_i)^2$

---

27. When writing the regression equation, why is there a hat ( ^) on the response variable?[^27]
    (a) because the prediction is an estimate
    (b) because the prediction is an average
    (c) because the prediction may be due to extrapolation
    (d) a & b
    (e) all of the above

[^27]: d. due to estimation and average

---

28. "Observed data or more extreme" is:[^28]
    (a) fewer than 9
    (b) 9 or fewer
    (c) 9 or more
    (d) more than 9

[^28]: c. 9 or more

---

29. What is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?[^29]
    (a) 0
    (b) 9
    (c) 5.3
    (d) 11
    (e) 15

[^29]: c. 5.3 because (15/31)*11 = 5.3

---

30. In the Botox and Pain Relief example, the p-value is calculated.  What does "probability" refer to?[^30]
    (a) random allocation
    (b) random sample

[^30]: a. random allocation

---

p-value = probability of the observed data or more extreme given the null hypothesis is true.

---

31. What conclusion would you draw from the Back Pain and Botox study?[^31]
    (a) Not enough evidence to conclude that Botox is more effective than the placebo.
    (b) Strong evidence that Botox is equally as effective as the placebo.
    (c) Strong evidence that Botox is more effective than the placebo.
    
[^31]: c. Strong evidence that Botox is more effective than the placebo. p-value was roughly 0.005.

---

32. If we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?[^32]
    (a) Substantially greater than 50%
    (b) Substantially less than 50%
    (c) Very close to 50%

[^32]: c. Close to 50%  (the point estimate is 0.6)

---

33. Material check-in
    (a) So far, so good
    (b) Concepts are good, R is confusing
    (c) R is good, concepts are confusing
    (d) Everything is confusing
    
---

34. People check-in
    (a) So far, so good
    (b) I can go to office hours / mentor sessions / learning community check-ins, but I didn't go this week.
    (c) I can't make the scheduled office hours / mentor sessions / learning community check-ins
    (d) I'm looking for someone to study with
    
---

See Canvas front page for **anonymous** survey / feedback for the class.  Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.

---

35. If communication medium and cheating are independent variables, how many of the email senders (out of 26) would you expect to cheat?[^35]
    (a)	10 (ish)
    (b)	13 (ish)
    (c)	16 (ish)
    (d)	20 (ish)
    (e)	24 (ish)

[^35]: d. 20 (ish), 26*(38/48) = 20.58

---

36. When looking at the null differences, is the observed result of 28.7%:[^36]
    (a)	Very surprising
    (b)	Somewhat surprising
    (c)	Not very surprising

[^36]: b. Somewhat surprising, p-value was 0.04

---

37. Hypothesis: the number of hours that grade-school children spend doing homework predicts their future success on standardized tests.[^37]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^37]: c. alternative, one sided (because probably we are studying that it **increases** their success rate)

---

38. Hypothesis: king cheetahs on average run the same speed as standard spotted cheetahs.[^38]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^38]: a. null, two sided (because I have no idea which cheetah might run faster)

---

39. Hypothesis: the mean length of African elephant tusks has changed over the last 100 years.[^39]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^39]: d. alternative, two sided (because I have no idea whether they've increased or decreased)

---

40. Hypothesis: the risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not.[^40]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^40]: a. null, one sided (because I happen to know that folic acid is thought to prevent facial clefts)

---

41. Hypothesis: caffeine intake during pregnancy affects mean birth weight.[^41]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^41]: c. alternative, one sided (because I happen to know that caffeine is thought to decrease baby's birth weight)

---

42. In this class, the word parameter means:[^42]
    (a) The values in a model
    (b) Numbers that need to be tuned
    (c) A number which is calculated from a sample of data.
    (d) A number which (is almost always unknown and) describes a population.

[^42]: d. A number which (is almost always unknown and) describes a population.

---

43. I know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW/Lab solutions[^43]
     (a) TRUE
     (b) FALSE
     
[^43]: The warm-up solutions and clicker questions are on the main course website.  The HW & Lab solutions are on Canvas under Files.
    
---

44. You have a sample of size n = 50.  You sample with replacement 1000 times (to get 1000 bootstrap resamples). What is the sample size of each bootstrap resample?[^44]
    (a) 50
    (b) 1000

[^44]: a. 50

---

45. You have a sample of size n = 50.  You sample with replacement 1000 times (to get 1000 bootstrap resamples). How many bootstrap statistics will you have?[^45] 
     (a) 50
     (b) 1000

[^45]: b. 1000
---

46. Let's say you take a random sample and compute $\hat{p}=0.3.$  After bootstrapping, you see that the bootstrapped resamples produce almost all the $\hat{p}_{boot}$ within plus or minus 0.01 of your original statistic. It seems that the parameter $p$ is probably:[^46]
	   a. 0.3
	   b. between (0.2, 0.4)
	   c. between (0.29, 0.31)
	   d. between (0.28, 0.32)
	   e. huh?  how can we get $p$ from $\hat{p}?$

[^46]: c. between (0.29, 0.31)

---

47. In a second analysis, I create a 90% CI for the true proportion $p.$ What is the impact (of switching from 95% to 90%) on the CI?[^47]
    (a) narrower
    (b) less likely (long-run) to capture the parameter
    (c) neither
    (d) both

[^47]: d. both.  the intervals will be less likely (long-run) to capture the parameter *and* they will be narrower.

---

48. In a second study, I set out to obtain twice as much data (as in the first study) in order to create a 95% CI for the true proportion $p.$ What is the impact (of the larger sample) on the CI?[^48]
    (a) narrower
    (b) more likely (long-run) to capture the parameter
    (c) neither
    (d) both

[^48]: a. narrower (the sample size will not change the capture rate)

---

49. What is one main reason to use bootstrapping to find a confidence interval?[^49]
    (a) larger coverage probabilities
    (b) narrower intervals
    (c) more resistant to outliers
    (d) can be done for any statistic
    
[^49]: d. can be done for any statistic

---
 
50. 95% CI for the true median mercury:[^50]
    (a) (0.025 mg/kg, 0.975 mg/kg)
    (b) (0.469 mg/kg, 0.053 mg/kg)
    (c) (0.053 mg/kg, 0.469 mg/kg)
    (d) (0.34 mg/kg, 0.56 mg/kg)

[^50]: d. (0.34 mg/kg, 0.56 mg/kg)
---

```{r fig.cap = "From StatKey applet: https://www.lock5stat.com/StatKey/",  out.width="95%", fig.align='center', echo=FALSE}
knitr::include_graphics("images/merc_boot.jpg")
```


---

51. What are the observational units for your individual candy study?[^51]
     a. Color of the candy
     b. Piece of candy
     c. Cup of candy
     d. The Hershey Company
     e. Proportion that are orange

[^51]: b. Piece of candy

---

52. What are the observational units for the class compilation (dotplot)?[^52]
     a. Color of the candy
     b. Piece of candy
     c. Cup of candy
     d. The Hershey Company
     e. Proportion that are orange

[^52]: c. Cup of candy

---

53. How does the sampling distribution for the sample proportion change as n changes (for a fixed p)?[^53]
     a. The spread changes
     b. The symmetry changes
     c. The center changes
     d. The shape changes

[^53]: a. The spread changes

---

54. How does the sampling distribution change as p changes (for a fixed n)?[^54]
     a. The spread changes
     b. The symmetry changes
     c. The center changes
     d. The shape changes
     
[^54]: c. The center changes (the spread also changes a little bit, but mostly the center)

---

55. The Central Limit Theorem says that the distribution of $\hat{p}$ will be approximately normal with what center:[^55]
    (a) $\hat{p}$
    (b) $p$
    (c) 0.5
    (d) 1
    (e) $\sqrt{p(1-p) / n}$

[^55]: b. p

---

56. Would you rather have an extra 20 points on the SAT or an extra 10 points on the ACT?[^56]
     a. +20 on the SAT
     b. +10 on the ACT

[^56]: b. +10 on the ACT

---

57. The standardized score (z-score) counts:[^57]
    (a) the number of standard deviations from the mean
    (b) the number of standard deviations above the mean
    (c) the number of standard deviations below the mean
    (d) the distance from the mean
    (e) the distance from the standard deviation

[^57]: a. the number of standard deviations from the mean

---

58. If the normal distribution is a good model, we would expect the large majority of our z scores to be:[^58]
    (a) within $\pm$ 1 of the mean
    (b) within $\pm$ 2 of the mean
    (c) within $\pm$ 1 
    (d) within $\pm$ 2 

[^58]: d. within $\pm$ 2

---

59. With your cup of candy, you personally got a Z score of:[^59]
    (a) between (-1, 1) (not including 1)
    (b) between (-2, -1] or [1, 2)
    (c) between (-3, -2] or [2, 3)
    (d) -3 or smaller or 3 or above

[^59]: a. or b.  you most likely got between -2 and 2

---

60. Assume n = 100 and p= 0.8  (note: $\sqrt{(0.8 \cdot 0.2)/100} = 0.4/10 = 0.04$)  
What is the largest reasonable distance between $\hat{p}$ and $p$?  
That is, we would expect $\hat{p}$ and $p$ to be no more than _____ apart[^60]
    (a) 0.04
    (b) 0.08
    (c) 0.12
    (d) 0.16
    (e) 0.24

[^60]: b. 0.08 (we usually consider two standard deviations)

---


61. Assume n = 100 and p= 0.8  (note: $\sqrt{(0.8 \cdot 0.2)/100} = 0.4/10 = 0.04$) Which statement is true?[^61]
    (a) 95% of $\hat{p}$ are between (0.76, 0.84)
    (b) 95% of $\hat{p}$ are between (0.72, 0.88)
    (c) 95% of $\hat{p}$ are between (0.68, 0.92)
    (d) 95% of $p$ are between (0.76, 0.84)
    (e) 95% of $p$ are between (0.72, 0.88)

[^61]: b. 95% of $\hat{p}$ are between (0.72, 0.88)

---

62.  If you want a 90% confidence interval for p, your z* multiplier should be[^62]
     (a) less than 1
     (b) less than 2 (but greater than 1)
     (c) equal to 2
     (d) greater than 2 (but less than 3)
     (e) greater than 3

[^62]: b. less than 2 (but greater than 1)

---

63. What is the difference between Z* and a Z score?[^63]
     a. Z score comes from the data, Z* and is a pre-defined unit of measurement.
     b. Z* comes from the data, and Z score is a pre-defined unit of measurement
     c. Z score assumes the null hypothesis is true and Z* doesn't.
     d. Z* assumes the null hypothesis is true, and Z score doesn't

[^63]: a. Z score comes from the data, Z* and is a pre-defined unit of measurement.

---

64. Let's say we are making confidence intervals (not doing a hypothesis test), what is your best guess for $SE(\hat{p})$?[^64]
    (a) $\sqrt{0.5 \cdot (1 - 0.5) / n}$
    (b) $\sqrt{p \cdot (1 - p) / n}$
    (c) $\sqrt{\hat{p} \cdot (1 - \hat{p}) / n}$
    (d) $\sqrt{X \cdot (1 - X) / n}$
    (e) $\sqrt{0.95 \cdot (1 - 0.95) / n}$

[^64]: c. $\sqrt{\hat{p} \cdot (1 - \hat{p}) / n}$

---

65. The following is a correct interpretation of the CI:[^65]  

> 95% confident that the interval includes the sample proportion who believe that the global poverty rate has doubled. 

   a. TRUE
   b. FALSE

[^65]: a. TRUE

---

66.  The following is a correct interpretation of the CI:[^66]  

> If researchers were to select a new sample of 1005 adult Americans, then we're 95% confident that between 56% and 62% of those people would answer "doubled" to the question. 

   a. TRUE
   b. FALSE

[^66]: b. FALSE (we are 95% confident that the new interval will contain the true value.  We do not think that the new interval will be the same as the original interval.)

---

67.  Let's say that the null hypothesis (e.g., p=0.47) is TRUE.   My level of significance is 0.03 (reject if p-value < 0.03). How often will I reject the null hypothesis?[^67]
     (a) 1 % of the time
     (b) 3% of the time
     (c) 5 % of the time
     (d) 95% of the time
     (e) 97% of the time

[^67]: b. 3% of the time

---

What does "of the time" mean???

It means in repeated samples.  That is, in 3% of all datasets we'd take from that exact same population, we would mistakenly reject the actually true hypothesis that p=0.47.

---

68. Let's say that the null hypothesis (e.g., p=0.47) is TRUE.   My level of significance is 0.03.  
How often will p be in a 97% confidence interval?[^68]
    (a) 1 % of the time
    (b) 3% of the time
    (c) 5 % of the time
    (d) 95% of the time
    (e) 97% of the time

[^68]: e. 97% of the time

---

What does "of the time" mean???

It means in repeated samples.  That is, in 97% of all datasets we'd take from that exact same population, we would capture the true population proportion of 0.47.

---

69.  Suppose the sample is 10 times larger.  The SE of the statistic:[^69]
     a. increases
     b. stays the same
     c. decrease

[^69]: c. decreases

---

70.  Suppose the population is 10 times larger.  The SE of the statistic:[^70]
     a. increases
     b. stays the same
     c. decrease

[^70]: b. stays the same (the population size has no effect on the sampling distribution of the statistic)

---

71. Suppose the sample is 10 times larger.  The variability of the data:[^71]
     a. increases
     b. stays the same
     c. decrease

[^71]: b. stays the same (the variability of the data should be the same as the variability of the population, regardless of the sample size)


---

72. How many hits out of 20 at bats would make you believe him?[^72]
    (a)	5
    (b)	6
    (c)	7
    (d)	8
    (e)	9

[^72]: e. 9

---

73. Type I error is[^73]
    (a)	We give him a raise when he deserves it.
    (b)	We don't give him a raise when he deserves it.
    (c)	We give him a raise when he doesn't deserve it.
    (d)	We don't give him a raise when he doesn't deserve it.
 
[^73]: c. We give him a raise when he doesn't deserve it.

---

74. Type II error is[^74]
    (a)	We give him a raise when he deserves it.
    (b)	We don't give him a raise when he deserves it.
    (c)	We give him a raise when he doesn't deserve it.
    (d)	We don't give him a raise when he doesn't deserve it.

[^74]: c. We don't give him a raise when he deserves it.

---

75. Power is the probability that:[^75]
    (a)	We give him a raise when he deserves it.
    (b)	We don't give him a raise when he deserves it.
    (c)	We give him a raise when he doesn't deserve it.
    (d)	We don't give him a raise when he doesn't deserve it.

[^75]: a. We give him a raise when he deserves it.

---

76. The player is more worried about[^76]
    (a)	A type I error
    (b)	A type II error

[^76]: b. A type I error

---

77. The manager is more worried about[^77]
    (a)	A type I error
    (b)	A type II error

[^77]: a. A type I error

---

78. Increasing your sample size[^78]
    (a)	Increases your power
    (b)	Decreases your power

[^78]: a. Increases your power

---

79. Making your significance level more stringent ($\alpha$ smaller)[^79]
    (a)	Increases your power
    (b)	Decreases your power

[^79]: b. Decreases your power

---

80. A more extreme alternative[^80]:
    (a)	Increases your power
    (b)	Decreases your power

[^80]: a. Increases your power

---

81. Is the Alien's interval for the true proportion of all humans who self-identify as female consistent with your lived experience?[^81]
    (a) Yes
    (b) No
    (c) I don't understand what the confidence interval represents.

[^81]: b. No.  My experience is that close to 50% of humans self-identify as female.

---

82. As we've seen with the applet, about 5% of all 95% intervals fail to capture the actual value of the population parameter.  Do you think the alien just got a "red" interval?[^82]
    (a)	Yes
    (b)	No

[^82]: b. No.  They didn't just "get unlucky".  Instead, the reason the interval failed to capture the true parameter is because the sample was not representative of the population.  

---

83.	 Would it be reasonable for the alien to conclude, with 95% confidence, that between 16.5% and 33.5% of US Senators in the year 2023 self-identify as female?[^83]
     (a)	Yes
     (b)	No

[^83]: b. No. We know (for sure, with 100% confidence) that exactly 25% of U.S. senators in 2019 self identify as female. If that's the entire population of interest, there's no reason to calculate a confidence interval.

---

84. The "random" part in clinical trials typically comes from:[^84]
    (a) random samples
    (b) random allocation of treatment

[^84]: b. random allocation of treatment

---

85. The "random" part in polling typically comes from:[^85]
    (a) random samples
    (b) random allocation of treatment

[^85]: random samples

---

86. You want to collect data to investigate whether teenagers in the United States have read fewer Harry Potter books than teenagers in the United Kingdom.  Would you make use of random sampling, random assignment, both, or neither?[^86]
    (a)	Random sampling
    (b)	Random assignment
    (c)	Both
    (d)	Neither

[^86]: b. Random sampling, although it would be pretty hard to do a *true* random sample from either country.

---

87.	An instructor wants to investigate whether using a red pen to grade assignments leads to lower scores on exams than using a blue pen to grade assignments.  Would you advise the professor to make use of random sampling, random assignment, both, or neither?[^87]
    (a)	Random sampling
    (b)	Random assignment
    (c)	Both
    (d)	Neither

[^87]: b. Random assignment.  Randomly decide which exams to grade with which pen, and then record the scores.

---

88.	A student decides to investigate whether NFL football games played in indoor stadiums tend to have more points scored than games played outdoors.  The student examines points scored in every NFL game of the 2022 season. Has the student used random sampling, random assignment, both, or neither?[^88]
    (a)	Random sampling
    (b)	Random assignment
    (c)	Both
    (d)	Neither

[^88]: d. Neither.  The student has the entire population of teams and was not able to randomly assign stadium type.


---

89. Relative Risk is[^89]
    (a) the difference of two proportions
    (b) the ratio of two proportions
    (c) the log of the ratio of two proportions
    (d) the log of the difference of two proportions

[^89]: b. the ratio of two proportions

---

90. In order to find a CI for the true RR, our steps are:[^90]  
Step 1. ln(RR-hat)  
Step 2. add Â± z* sqrt( 1/A - 1/(A+C) + 1/B - 1/(B+D) )  
Step 3. find exp of the endpoints   
    (a) because the sampling distribution of RR is normal
    (b) because RR is typically greater than 1
    (c) because the ln transformation makes the sampling distribution almost normal
    (d) because RR is invariant to the choice of explanatory or response variable

[^90]: c. because the ln transformation makes the sampling distribution almost normal

---

91. In finding a CI for $p_1$/$p_2$, why is it okay to exponentiate the end points of the interval for ln($p_1$/$p_2$)?[^91] 
    (a) if ln($p_1$/$p_2$) is in the natural log-interval, $p_1$/$p_2$ will be in the exponentiated interval.
    (b) the natural log of the RR makes the distribution approximately normal.
    (c) the natural log compresses values that are > 1 and spreads values < 1.

[^91]: a. if ln($p_1$/$p_2$) is in the natural log-interval, $p_1$/$p_2$ will be in the exponentiated interval.  (Where "okay" means you have 95% coverage in repeated samples.)

---

92.   Usually, the CI for  $p_1$/$p_2$ is considered to be "significant" if[^92] 
      (a) $p_1$/$p_2$ is not in the interval
      (b) $\hat{p}_1 / \hat{p}_2$ is not in the interval
      (c) 0 is not in the interval
      (d) 1 is not in the interval

[^92]: d. 1 is not in the interval

---

93. In order to find a CI for the true OR, our steps are:[^93]  
Step 1. ln(OR-hat)  
Step 2. add Â± z* sqrt( 1/A + 1/B + 1/C + 1/D )  
Step 3. find exp of the endpoints   
     (a) because the sampling distribution of OR is normal
     (b) because OR is typically greater than 1
     (c) because the ln transformation makes the sampling distribution almost normal
     (d) because OR is invariant to the choice of explanatory or response variable

[^93]: c. because the ln transformation makes the sampling distribution almost normal

---

94. Sample 1,000,000 people who are over 6' tall and 1,000,000 people who are under 6' tall.  Record if the person is in the NBA.  
What is measurable?[^94]
    (a) P(NBA if 6' tall)
    (b) P(6' tall if in the NBA)
    (c) both
    (d) neither  

[^94]: a. P(NBA if 6' tall) (cohort: cannot measure the probability of the explanatory variable given the response)

---

95.  Sample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6' tall. What is measurable?[^95]
     (a) P(NBA if 6' tall)
     (b) P(6' tall if in the NBA)
     (c) both
     (d) neither  

[^95]: b. P(6' tall if in the NBA) (case-control:  cannot measure the probability of the response variable given a level of the explanatory variable)

---

96.  Sample 10,000,000 people.  Record their height and whether or not they are in the NBA.  
What is measurable?[^96]
     (a) P(NBA if 6' tall)
     (b) P(6' tall if in the NBA)
     (c) both
     (d) neither  

[^96]: c. both (cross-classification: can measure all the probabilities)

---

From the NYT, March 21, 2023, https://www.nytimes.com/2023/03/21/sports/basketball/tall-basketball-march-madness.html

> The average W.N.B.A. player, at a shade taller than 6 feet, towers over the average American woman (5 feet 3.5 inches). American men who are between 6 feet and 6-2 â€” significantly taller than the 5-9 average â€” have about a five in a million chance of making the N.B.A., according to "The Sports Gene," a 2013 book by David Epstein about the science of athletic performance. But if you hit the genetic lottery and happen to be 7 feet tall, your chances of landing in the N.B.A. are roughly one in six. (There are 38 players on active rosters who are 7 feet or taller, according to N.B.A. Advanced Stats; the average height of an N.B.A. player is 6 feet 6.5 inches.)


https://davidepstein.com/david-epstein-the-sports-gene/

---

97.  When we randomly select individuals based on the explanatory variable, we cannot accurately measure[^97]
     (a) the proportion of people in the population in each explanatory category
     (b) the proportion of people in the population in each response group
     (c) anything about the population
     (d) confounding variables
 
[^97]: a. the proportion of people in the population in each explanatory category

---

98. The odds ratio is invariant to which variable is explanatory and which is response means:[^98]
    (a) we always put the bigger odds in the numerator
    (b) we must collect data so that we can estimate the response in the population
    (c) which variable is called the explanatory changes the value of the OR
    (d) which variable is called the explanatory does not change the value of the OR

[^98]: d. which variable is called the explanatory does not change the value of the OR 

---

99.  One reason we should be careful interpreting relative risks is if:[^99]
     (a) we don't know the difference in proportions
     (b) we don't know the SE of the relative risk
     (c) we might be dividing by zero
     (d) we don't know the baseline risk

[^99]: d. we don't know the baseline risk

---

100. If the null hypothesis is true, the observed counts will equal the expected counts.[^100]
     (a) True
     (b) False

[^100]: b. False

---

101. To reject the null hypothesis we want to see[^101]
     (a) a small $X^2$ value
     (b) a big $X^2$ value

[^101]: b. a big $X^2$ value


---

102. A chi-square test has a[^102]
     (a) one-sided alt hypothesis, and we only consider the upper end of the sampling distribution
     (b) one-sided alt hypothesis, and we consider both ends of the sampling distribution
     (c) two-sided alt hypothesis, and we only consider the upper end of the sampling distribution
     (d) two-sided alt hypothesis, and we consider both ends of the sampling distribution 

[^102]: c. two-sided alt hypothesis, and we only consider the upper end of the sampling distribution

---

103. For the lighting study, which variable is the explanatory variable?[^103]
     (a) sleeping light
     (b) eye sightedness
     (c) child
     (d) parent

[^103]: a. sleeping light

---

104. If we sample randomly from a population, the conclusions we can make are about:[^104]
     (a) causation 
     (b) population characteristics

[^104]: b. population characteristics

---

105. Based on the night light / myopia example, the correct conclusion is:[^105]
     (a) the p-value is small, so sleeping in a lit room makes it more likely that you are near-sighted. 
     (b) the p-value is small, so sleeping in a dark room makes it more likely that you are near-sighted. 
     (c) the p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms. 
     (d) $\hat{p}_{\mbox{near}}$ if lit room = 41/75 = 0.547 and    $\hat{p}_{\mbox{near}}$ if dark = 18/172 = 0.105, therefore sleeping with the light on is bad for you. 

[^105]: c. the p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms.

---

106. A possible confounding variable for the night light study is:[^106]
     (a) low birth weight
     (b) race (70% of the children were white)
     (c) region of the country where the clinic was located

[^106]: a. low birth weight (the argument needs to be made that the confounding variable is associated with both the explanatory and the response variable)

---

107. Which dataset has the smallest standard deviation?[^107]
     (a)	A: left
     (b)	B: center
     (c)	C: right

```{r fig.height=4}
#| echo: false
#| warning: false
#| message: false

set.seed(47)
par(mfrow = c(1,3))
hist(runif(50,0,10), xlab="A", xlim = c(0,10), main = "")
hist(10 - rchisq(50,1), xlab = "B", xlim = c(0,10), main = "")
hist(rnorm(50,5,2), xlab = "C", xlim = c(0,10), main = "")
```
[^107]: b. B: center (the typical distance from the mean is smallest)

---

108. Which of the two dotplots displays the dataset with the smaller IQR?[^108]
     (a)	A
     (b)	B

```{r fig.height=4.2}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
set.seed(4704)
data <- data.frame(x = c(sample(25:35, 16, replace = TRUE), 11, 13, 50, 60,
                         sample(15:50, 20, replace = TRUE)),
                   group = c(rep("A", 20), rep("B", 20)))

#create stacked dot plot
ggplot(data, aes(y = x, x = group)) +
  geom_dotplot(binaxis = "y", stackdir = "up") + 
  xlab("") + 
  coord_flip()
```

[^108]: a. A (the IQR measures the middle 10 points in each group, the middle 10 points in group A are closer together than the middle 10 points in group B)

---

109. The standard deviation of weights (mean = 167 lbs) is approximately[^109]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100

[^109]: d. 35 (none of the other answers are reasonable)

---

110. The standard deviation of average weights (mean = 167 lbs) in repeated samples of size 10 is approximately[^110]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100

[^110]: c. 10 ($\approx 35/\sqrt{10}$)

---

111. The standard deviation of average weights (mean = 167 lbs) in repeated samples of size 50 is approximately[^111]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100

[^111]: b. 5 ($\approx 35/\sqrt{100}$)
---

112. The standard deviation of average weights (mean = 167 lbs) in repeated samples of size 1000 is approximately[^112]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100
     
[^112]: a. 1 ( $\approx 35/\sqrt{1000}$)
     
---

Q: what is the most confusing part of understanding the difference between the variability of the weights and the variability of the average of the weights?

---

113. The sampling distribution of the mean will be[^113]
     (a) centered below the data distribution
     (b) centered at the same place as the data distribution
     (c) centered above the data distribution
     (d) unrelated to the center of the data distribution

[^113]: b. centered at the same place as the data distribution

---

114. The sampling distribution of the mean will be[^114]
     (a) less variable than the data distribution
     (b) the same variability as the data distribution
     (c) more variable than the data distribution
     (d) unrelated to the variability of the data distribution

[^114]: a. less variable than the data distribution

---

115. Why did we switch from talking about total weight to talking about average weight?[^115]
     (a) So that it is easier to infer from the sample to the population.
     (b) Because the Coast Guard certifies vessels according to average weight.
     (c) Because the average is less variable than the sum.
     (d) Because the average has a normal distribution and the sum doesn't.

[^115]: a. So that it is easier to infer from the sample to the population

---

116. When the population is skewed right, the sampling distribution for the sample mean will be[^116]
     (a) always skewed right
     (b) skewed right if n is big enough
     (c) always normal
     (d) normal if n is big enough

[^116]: d. normal if n is big enough

---

117. What does the CLT say?[^117]


[^117]: Describing random samples (of size n) from the population, the sampling distribution of the sample mean is normal if the sample size (n) is large enough.

---

118. What type of variable is "healthy body temp"?[^118]
     a. explanatory
     b. response

[^118]: b. response (we don't have an explanatory variable in this setting)

---

119.  We use $s$ instead of $\sigma$ because[^119]
      (a) we know $s$ and we don't know $\sigma$ 
      (b) $s$ is a better estimate of the st dev
      (c) $s$ is less variable than $\sigma$
      (d) we want our test statistic to vary as much as possible
      (e) we like the letter t better than the letter z

[^119]: a. we know $s$ and we don't know $\sigma$ 

---

120.  The variability associated with $\overline{X}$ is[^120]
      (a) less than the variability of X
      (b) more than the variability of X
      (c) the same as the variability of X
      (d) unrelated to the variability of X
      (e) some other function of X

[^120]: a. less than the variability of X

---

121. If asked to "determine how many standard errors the sample mean (98.249) falls from the hypothesized value of 98.6", which formula should you use?[^121]
      (a) $\frac{(98.249-98.6)}{s}$
      (b) $\frac{(98.249-98.6)}{s/\sqrt{n}}$
      (c) $\frac{(98.249-98.6)}{\sigma}$
      (d) $\frac{(98.249-98.6)}{\sigma/\sqrt{n}}$

[^121]: b. (98.249-98.6)/($s/\sqrt{n}$) (because $s/\sqrt{n}$ is the SE of $\overline{X})

---

122.  When we use $s$ instead of $\sigma$ in the CI for $\mu$, but still keep z* (instead of using a t* multiplier), the resulting CI has coverage[^122]
      (a) **LESS** than the stated confidence level
      (b) **MORE** than the stated confidence level
      (c) **OF** the stated confidence level

[^122]: a. **LESS** than the stated confidence level

---

123. What is the difference between t* and a t score?[^123]
     a. t score comes from the data, t* and is a pre-defined unit of measurement.
     b. t* comes from the data, and t score is a pre-defined unit of measurement
     c. t score assumes the null hypothesis is true and t* doesn't.
     d. t* assumes the null hypothesis is true, and t score doesn't

[^123]: a. t score comes from the data, t* and is a pre-defined unit of measurement.

---

124.  What is the correct interpretation of the 92% CI for $\mu$ which is given as (98.2, 98.3)?[^124]
      (a) 92% of intervals will be (98.2, 98.3).
      (b) 92% of individual temperatures will be between (98.2, 98.3).
      (c) There is a 0.92 probability that the true temperature is between (98.2, 98.3).
      (d) There is a 0.92 probability that the true average temperature is between (98.2, 98.3).
      (e) In repeated samples, 92% of the intervals will contain $\mu$.

[^124]: e. In repeated samples, 92% of the intervals will contain $\mu$. Also good interpretation for a CI: f. We are 92% confident that the interval (98.2, 98.3) captures the true average temperature, $\mu$.

---

125. Let's say you *truly* believe that the true average body temp is between (98.2, 98.3). (Your CI is green.) You record a temp of 98.6 F.  Do you think you are sick?[^125]
     a. Yes, it is outside the range above.
     b. No, I still believe $\mu$ is 98.6.
     c. No, 98.6 isn't too far above the upper bound.
     d. No, the interval isn't for individual people.

[^125]: d. No, the interval isn't for individual people.

---

126.  The variability associated with $\overline{X}$ is[^126]
      (a) less than the variability of X
      (b) more than the variability of X
      (c) the same as the variability of X
      (d) unrelated to the variability of X
      (e) some other function of X

[^126]: a. less than the variability of X

---

127.  The variability associated with predicting a new value, $X_{n+1}$,[^127]
      (a) is less than the variability of $\overline{X}$
      (b) is more than the variability of $\overline{X}$
      (c) is the same as variability of $\overline{X}$
      (d) is less than the variability of X
      (e) is more than the variability of X

[^127]: b. is more than the variability of $\overline{X}$

---

128.  Prediction intervals are[^128]
      (a) smaller than confidence intervals
      (b) about the same width as confidence intervals
      (c) larger than confidence intervals
      (d) unrelated to confidence intervals

[^128]: c. larger than confidence intervals

---

129. Where should a prediction interval for a new value, $X_{n+1}$, be centered?[^129]
      (a) $\overline{X}$
      (b) $\mu$
      (c) 98.6
      (d) $X_1$ (the first person in the dataset)
      (e) $X_n$ (the last person in the dataset)
  
[^129]: a. $\overline{X}$

---

130. What is the correct interpretation of the 95% PI for $X_{n+1}$ which is given as (96.79, 99.70)?[^130]
      (a) 95% of intervals will be (96.79, 99.70).
      (b) 95% of individual temperatures will be between (96.79, 99.70).
      (c) There is a 0.95 probability that the true temperature is between (96.79, 99.70).
      (d) There is a 0.95 probability that the true average temperature is between (96.79, 99.70).
      (e) In repeated samples, 95% of the intervals will contain $\mu$.

[^130]: b. 95% of individual temperatures will be between (96.79, 99.70).

---

131. Prediction intervals have[^131]
      (a) the same technical conditions as CIs
      (b) stricter technical conditions than CIs
      (c) more lenient technical conditions than CIs
      (d) technical conditions which are unrelated to CIs

[^131]: b. stricter technical conditions than CIs

---

132. When the population is skewed right, the sampling distribution for the sample mean will be[^132]
      (a) always skewed right
      (b) skewed right if n is big enough
      (c) always normal
      (d) normal if n is big enough

[^132]: d. normal if n is big enough

---

133.  When the population is skewed right, the distribution for the data will be[^133]
      (a) always skewed right
      (b) skewed right if n is big enough
      (c) always normal
      (d) normal if n is big enough

[^133]: a. always skewed right

---

134. A newspaper article claims that the average age for people who receive food stamps is 40 years. You believe that the average age is lower. You take a random sample of 100 people who receive food stamps and find their average age to be 39.2 years. You find that 39.2 is significantly lower than the age of 40 stated in the article (p < 0.05). What would be an appropriate interpretation of the result?[^134]
      (a) The statistically significant result indicates that the majority of people who receive food stamps is younger than 40.
      (b) Although the result is statistically significant, the difference in age is not of practical importance.
      (c) An error must have been made. This difference is too small to be statistically significant.

[^134]: b. Although the result is statistically significant, the difference in age is not of practical importance.

---

135. In order to investigate a claim that the average time required for the county fire department to respond to a reported fire is greater than 5 minutes, county staff determined the response times for 40 randomly selected fire reports.  The data were used to test $H_0: \mu = 5$ versus $H_a:  \mu > 5$, and the computed p-value was 0.12.  If a 0.05 level of significance is used, what conclusions can be drawn?[^135]
      (a) There is convincing evidence that the mean response time is 5 minutes (or less).
      (b) There is convincing evidence that the mean response time is greater than 5 minutes.
      (c) There is not convincing evidence that the mean response time is greater than 5 minutes.

[^135]: c. There is not convincing evidence that the mean response time is greater than 5 minutes.

---

136. You have two samples of size n = 50.  You sample with replacement 1000 times (to get 1000 bootstrap resamples). What is the sample size of each bootstrap resample?[^136]
     (a) 50
     (b) 1000

[^136]: a. 50 (bootstrap 50 resamples separately from each group.)

---

137. You have two samples of size n = 50.  You sample with replacement 1000 times (to get 1000 bootstrap resamples). How many bootstrap statistics will you have?[^137]
     (a) 50
     (b) 1000

[^137]: b. 1000 statistics (here the statistic is $\overline{X}_1 - \overline{X}_2)$

---

138. You have two samples of size n = 50.  You shuffle the explanatory and response variables (i.e., sample without replacement) 1000 times. What is the sample size of each group after shuffling?[^138]
     (a) 50
     (b) 1000

[^138]: a. 50 (shuffling maintains the exact same number in each group)

---

139. You have two samples of size n = 50.  You shuffle the explanatory and response variables (i.e., sample without replacement) 1000 times. How many randomization statistics will you have?[^139]
     (a) 50
     (b) 1000

[^139]: b. 1000 statistics (here the statistic is $\overline{X}_1 - \overline{X}_2)$

---

140. We typically compare means instead of medians because[^140]
     (a) we don't know the SE of the difference of medians
     (b) means are inherently more interesting than medians
     (c) the randomization applet (or R code) doesn't work with medians
     (d) the Central Limit Theorem doesn't apply for medians

[^140]: a. we don't know the SE of the difference of medians and d. the Central Limit Theorem doesn't apply for medians

---

141. $SE(\overline{X}_1 - \overline{X}_2)$ is:[^141]
      a. $\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$
      b. $\sqrt{\frac{\sigma_1^2}{n_1} - \frac{\sigma_2^2}{n_2}}$
      c. $\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$
      d. $\sqrt{\frac{s_1^2}{n_1} - \frac{s_2^2}{n_2}}$
      e. $\sqrt{s_1^2 - s_2^2}$

[^141]: c. $\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$

---

142. The distribution of age of death is:[^142]
     a. right skewed
     b. left skewed
     c. symmetric
     d. can't tell with this information

[^142]: b. left skewed

---

143. The standard deviation for the age of death is likely around:[^143]
     (a) 1 year
     (b) 10 years
     (c) 20 years
     (d) 50 years
     (e) 100 years

[^143]: I don't know.  10 or 20 both seem like reasonable values.  I don't think 1, 50, or 100 are reasonable.

---

144. The number of left handed people is likely:[^144]
     (a) 10
     (b) 100
     (c) 300
     (d) 500
     (e) 900

[^144]: b. 100, which is roughly 10% of the sample

---

145. Are the two samples (lefties and righties) independent?[^145]
     (a) yes
     (b) no
     (c) we can't tell

[^145]: a. yes.  They are not a pure random sample.  however, there is no reason to think that knowledge about one person in the sample tells you anything about about the lifetime of other people in the sample.

---

146.  For the handedness example, which has a lower p-value?[^146]
      a. Scenario 2
      b. Scenario 3

[^146]: a. Scenario 2 (because the sample SDs are smaller)

---

147. For the handedness example, which has a lower p-value?[^147]
      a. Scenario 3
      b. Scenario 4

[^147]: a. Scenario 3 (because the samples are more balanced)

---

148. How does each affect the power?[^148]
(i) increasing the sample sizes of both groups
     (a) increases the power
     (b) doesn't change the power
     (c) decreases the power
     
--- 

(ii) larger variability within the groups 
     (a) increases the power
     (b) doesn't change the power
     (c) decreases the power

---

(iii) larger difference in actual (population) group means
      (a) increases the power
      (b) doesn't change the power
      (c) decreases the power

[^148]: i. a. increases power; ii. c. decreases power; iii. a. increases power

---

149. We use the t-distribution (instead of the z-distribution) because:[^149]
     (a) the CLT makes the test statistic normal
     (b) the CLT makes the numerator of the test statistic normal
     (c) the variability in the denominator makes the test statistic more variable
     (d) the variability in the denominator makes the test statistic less variable

[^149]: c. the variability in the denominator makes the test statistic more variable

---

150.  If we use the SE and the z-curve (instead of t-curve) to find the p-value (assuming $\overline{X}$ values are reasonably different):[^150]
      (a) the p-value will be too small
      (b) the p-value will be too big
      (c) the p-value will be just right
      (d) the p-value is unrelated to the curve
      (e) we should use the SD instead

[^150]: a. the p-value will be too small

---
 
151. If we use the SE and the z-curve (instead of t-curve) to find a 95% CI:[^151]
      a. The capture rate will be at 95% over the long run.
      b. The capture rate will be higher than 95% over the long run.
      c. The capture rate will be lower than 95% over the long run.

[^151]: c. The capture rate will be lower than 95% over the long run.

---

152. In the ANOVA setting, the null hypothesis is always: $$H_0: \mu_1 = \mu_2 = \mu_3 = \ldots = \mu_I$$
What is the alternative hypothesis?[^152]
     (a) $H_a: \mu_1 \ne \mu_2 \ne \mu_3 \ne \ldots \ne \mu_I$
     (b) $H_a: \mu_1 = \mu_2 = \mu_3 = \ldots = \mu_I = \mu$ (for some $\mu$ value)
     (c) $H_a$: at least one $\mu_i$ is different
     (d) $H_a$: at least one $\mu_i$ is significantly different
     (e) $H_a$: at least one $\mu_i$ is a lot different

[^152]: c. $H_a$: at least one $\mu_i$ is different

---

153. In order to tell whether the differences in sample means are significant, we need to ALSO know:[^153]
     (a) how variable the observations are
     (b) the distribution of the observations
     (c) the sample sizes
     (d) all of the above
     (e) some of the above

[^153]: a. how variable the observations are and c. the sample sizes for sure. We also need b. the distribution of the observations if the sample sizes are small.

---

154. Which is more significant?[^154]
     (a) A
     (b) B
     (c) They are the same
     (d) We can't tell
    
```{r fig.cap = "",  out.width="95%", fig.align='center', echo=FALSE}
knitr::include_graphics("images/anova_boxplots.png")
```

[^154]: b. B

---

155. We reject the null hypothesis if:[^155]
     (a) the between group variability is much bigger than the within group variability
     (b) the within group variability is much bigger than the between group variability
     (c) the within group variability and the between group variability are both quite large
     (d) the within group variability and the between group variability are both quite small

[^155]: a. the between group variability is much bigger than the within group variability

---

156. What types of values will the F-ratio have when the null hypothesis is false, that is, when the population means are not all equal?[^156]
     (a) large, positive
     (b) large, negative
     (c) small, positive
     (d) small, negative

[^156]: a. large, positive

---


157.  With ANOVA, if the null hypothesis is true, then[^157]
$$H_0: \overline{X}_1 = \overline{X}_2 = \overline{X}_3 = \ldots = \overline{X}_I$$
      a. TRUE
      b. FALSE

[^157]: b. FALSE
---

158. How can I figure out which $\mu_i$ is different?[^158]
      a. The ANOVA reports it
      b. Do repeated one sample mean tests, but worry about type I errors
      c. Do repeated one sample mean tests, but worry about type II errors
      d. Do repeated two sample mean tests, but worry about type I errors
      e. Do repeated two sample mean tests, but worry about type II errors
      
[^158]: d. Do repeated two sample mean tests, but worry about type I errors

---

159. Consider a categorical variable with 4 levels. In addition to the intercept how many variables show up in the linear model output?[^159]
      a. 1
      b. 3
      c. 4
      d. n-4
      e. n

[^159]: b. 3, there will always be one fewer variables in the linear model because the baseline group will be part of the intercept.

---


160. If there is no relationship in the population (true correlation $\rho = 0$), then $r=0.$[^160]
     (a) TRUE
     (b) FALSE

[^160]: FALSE, we never think that the statistic will be the same as the parameter, regardless of the value of the parameter.

---

161. If there is no relationship in the population (true slope $\beta_1 = 0$), then $b_1=0.$[^161]
     (a) TRUE
     (b) FALSE

[^161]: FALSE, we never think that the statistic will be the same as the parameter, regardless of the value of the parameter.

---

If we set a parameter equal to XXXX, should we expect the statistic to be XXXX?

No.  Because statistics vary from sample to sample.

---

162. A smaller variability around the regression line (can be thought of as: $\sigma$ or MSE or variability of the $e_i$):[^162]
     (a) increases the variability of $b_1.$
     (b) decreases the variability of $b_1.$
     (c) doesn't necessarily change the variability of $b_1.$

[^162]: b. decreases the variability of $b_1.$

---

163. A smaller variability in the explanatory variable (SD(X) = $s_x$):[^163]
     (a) increases the variability of $b_1.$
     (b) decreases the variability of $b_1.$
     (c) doesn't necessarily change the variability of $b_1.$

[^163]: a. increases the variability of $b_1.$

---

164. A smaller sample size ($n$):[^164]
     (a) increases the variability of $b_1.$
     (b) decreases the variability of $b_1.$
     (c) doesn't necessarily change the variability of $b_1.$

[^164]: a. increases the variability of $b_1.$

---

165. The regression technical assumptions include:[^165]
     (a) The Y variable is normally distributed at each X
     (b) The X variable is normally distributed
     (c) The residuals are normally distributed
     (d) The slope coefficient is normally distributed
     (e) The intercept coefficient is normally distributed

[^165]: a. The Y variable is normally distributed at each X  or c. The residuals are normally distributed.  (b. is not true as there are no technical conditions on X.  d. and e. are both a result of either a. or c. being true.) 

---

166. The technical conditions do not include:[^166]
     (a) normally distributed residuals
     (b) normally distributed response
     (c) normally distributed explanatory variable
     (d) constant variance
     (e) independence of observations

[^166]: c. normally distributed explanatory variable (there are no technical conditions on X)

---

167. What happens if the technical conditions are not met?[^167]
     a. The line does not minimize the sum of squared residuals.
     b. $R^2$ does not measure the proportion of variability explained by the line.
     c. The null sampling distribution of $b_1$ is wrong (therefore incorrect p-values and CI).
     d. The computer (R) will produce an error when running the linear model.

[^167]: c. The null sampling distribution of $b_1$ is wrong (therefore incorrect p-values and CI).

---

168. Which linear regression condition is violated?[^168]
     (a) linearity 
     (b) equal variance of errors
     (c) independent errors
     (d) normal errors
     (e) outliers

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-height: 4

library(openintro)

mammals %>%
  ggplot(aes(x = gestation, y = life_span)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

[^168]: b. equal variance of errors

---

169. Which linear regression condition is violated?[^169]
     (a) linearity
     (b) constant errors
     (c) independent errors
     (d) normal errors
     (e) outliers

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-height: 4


mammals %>%
  ggplot(aes(y = gestation, x = body_wt)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

[^169]: a. linearity (or maybe e. outliers???)


---

170. Which linear regression condition is violated?[^170]
     (a) linearity
     (b) constant errors
     (c) independent errors
     (d) normal errors
     (e) outliers

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-height: 4

set.seed(470)
ex <- runif(100, 35, 60)
why <- 2 + .7*ex + rexp(100, .1)

data.frame(ex, why) %>%
  ggplot(aes(x = ex, y = why)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```


[^170]: d. normal errors

---

171. Which of the below correctly describes the roles of variables in this regression model?[^171]
     (a) response: weight; explanatory: volume, paperback cover
     (b) response: weight; explanatory: volume, hardcover cover
     (c) response: volume; explanatory: weight, cover type
     (d) response: weight; explanatory: volume, cover type

```{r}
#| echo: false
#| message: false
#| warning: false

library(DAAG)
library(broom)
library(tidyverse)

allbacks %>%
  lm(weight ~ volume + cover, data = .) %>%
  tidy()
```

[^171]: d. response: weight 
explanatory: volume, cover type

---

172. An econometrician is interested in evaluating the relation of demand for building materials to mortgage rates in LA and SF.[^172] 
$$\hat{Y} =  10 + 5X_1 + 8X_2$$
     (a) predicted $500 more per capita
     (b) predicted $500 less per capita
     (c) predicted $5 more per capita
     (d) predicted $5 less per capita

$X_1$ = mortgage rate in %   
$X_2$ = 1 if SF, 0 if LA  
$Y$ = demand in $100 per capita  
Holding constant the effect of city, each additional increase of 1%  in the mortgage rate would lead to an estimated average ________ in the mean demand.

[^172]: c. predicted $5 more per capita

---

173. Referring to 
$$\hat{Y} =  10 + 5X_1 + 8X_2$$
the effect of living in LA rather than SF is a ________ demand by an estimated ________ holding the effect of mortgage rate constant.[^173]
     (a) larger; $800 per capita
     (b) smaller; $800 per capita
     (c) larger, $8 per capita
     (d) smaller, $8 per capita

[^173]: b. smaller; $800 per capita

---

174. Consider the housing model, (Y = ln price)
$$\hat{Y} = 12.2 + 0.000468 \cdot \mbox{sqft} âˆ’ 0.0603\cdot \mbox{\# bedrooms}$$
the coefficient on bedrooms (-0.0603) can be interpreted as the change in predicted ln(price) ...[^174]
      a. for a one unit increase in bedrooms
      b. for a home that adds a bedroom to the existing structure (without adding square feet)
      c. for a one unit increase in bedrooms when comparing homes that have identical square feet
      d. for a one unit increase in square feet
      e. for a one unit increase in square feet when comparing homes that have identical number of bedrooms

[^174]: c. for a one unit increase in bedrooms when comparing homes that have identical square feet

---

175. To test if there is convincing evidence that the slope of the regression line between ln(price) and square feet (only, no bedroom here) is different from zero, what are the appropriate hypotheses?[^175]
     (a)	$H_0: b_0 = 0$
      	  $H_a: b_0 \ne 0$
     (b) 	$H_0: b_1 = 0$
      	  $H_a: b_1 \ne 0$
     (c) 	$H_0: \beta_0 = 0$
      	  $H_a: \beta_0 \ne 0$
     (d) 	$H_0: \beta_1 = 0$
 	        $H_a: \beta_1 \ne 0$

[^175]: d. $H_0: \beta_1 = 0$ and $H_a: \beta_1 \ne 0$

---

p-value = probability of observed data ($b_1$) or more extreme if $H_0$ is true ($\beta_1 = 0).$

---

176. With # bedrooms in the model, in words, the test $(H_0: \beta_1 = 0 \ \ \ \ \ \ H_a: \beta_1 \ne 0)$ asks:[^176]
$$\hat{Y} = 12.2 + 0.000468 \cdot \mbox{sqft} âˆ’ 0.0603\cdot \mbox{\# bedrooms}$$
      a. the slope of the regression line between ln(price) and square feet is different from zero
      b. the slope of the regression line between ln(price) and square feet is different from zero when # bedrooms is included in the model
      c. adding square footage to your house causes the value to increase
      d. the slope of the regression line between ln(price) and bedrooms is different from zero
      e. the slope of the regression line between ln(price) and bedrooms is different from zero when square feet is included in the model

[^176]: b. the slope of the regression line between ln(price) and square feet is different from zero when # bedrooms is included in the model

---

177. We created a 95% confidence interval for the mean GPA given 10 absences to be (3.20, 3.42).  What is the correct interpretation?[^177]
     (a) There is a 95% chance that the mean GPA of students with 10 absences is between 3.20 and 3.42.
     (b) 95% of GPA averages (for students with 10 absences) are between 3.20 and 3.42.
     (c) 95% of GPAs (for students with 10 absences) are between 3.20 and 3.42.
     (d) We are 95% confident that the true mean GPA (for students with 10 absences) is between 3.20 and 3.42.
     (e) 95% of our intervals will have a mean GPA between 3.20 and 3.42.

[^177]: d. We are 95% confident that the true mean GPA (for students with 10 absences) is between 3.20 and 3.42.

---

178. We created a 95% prediction interval for an individual GPA given 10 absences to be (3, 3.62).  What is the correct interpretation?[^178]
     (a) There is a 95% chance that the mean GPA of students with 10 absences is between 3 and 3.62.
     (b) 95% of GPA averages (for students with 10 absences) are between 3 and 3.62.
     (c) 95% of GPAs (for students with 10 absences) are between 3 and 3.62.
     (d) We are 95% confident that the true mean GPA (for students with 10 absences) is between 3 and 3.62.
     (e) 95% of our intervals will have a mean GPA between 3 and 3.62.

[^178]: b. 95% of GPA averages (for students with 10 absences) are between 3 and 3.62.  It is also totally okay to interpret the interval as: there is a 95% chance that if I randomly select someone with 10 absences, their GPA will be between 3 and 3.62. 

---


179. Prediction intervals and confidence intervals have the same technical conditions:[^179]
     (a) TRUE
     (b) FALSE
     (c) sort of

[^179]: c. sort of.  Both methods require the data to be normally distributed (for one sample mean, the variable is normal; for linear regression, the residuals are normal).  However, if you have a large enough sample size, the CLT kicks in when building the mean interval  The CLT does not ever help for prediction intervals, so you always need normal data to have reasonable prediction intervals.

---

180. It is often a good idea to transform the variable(s)...[^180]
     (a) ... to find the highest R2 value.
     (b) ... when the X variable is not normally distributed.
     (c) ... to make the model easier to interpret.
     (d) ... so that the technical conditions are met.

[^180]: d. ... so that the technical conditions are met. (And often transforming makes the results harder to interpret.)

---

181. A large value of $R^2$ says...[^181]
      a. the technical conditions hold.
      b. the variability in the response is well explained by the explanatory variable(s).
      c. the explanatory variable(s) determine the response variable.  
      d. the explanatory variable(s) are significant.

[^181]: b. the variability in the response is well explained by the explanatory variable(s).


