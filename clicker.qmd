---
---
<!-- the two formats are html and revealjs -->

# Clicker Q

to go with **Introduction to Modern Statistics** by Çentinkaya-Rundel & Hardin.  Math 58B - Introduction to Biostatistics.

```{=html}
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
```


---

1. If 16 infants with **no genuine preference** choose 16 toys, what is the most likely number of "helping" toys that will be chosen?[^1]
   (a)	4
   (b)	7
   (c)	8
   (d)	9
   (e)	10


[^1]:  c.	8

---

2. How likely is it that exactly 8 helpers will be chosen (if there is no preference)?[^2]
   (a)	0-15% 
   (b)	16-30% 
   (c)	31-49%
   (d)	50%
   (e)	51-100%


[^2]:  b. 0.196 (19.6% of the time)

---

3. What if we flipped a coin 160 times?  What percent of the time will the simulation flip exactly 80 heads?[^3]
   (a)	0-15% 
   (b)	16-30% 
   (c)	31-49%
   (d)	50%
   (e)	51-100%

[^3]: a. 0.063 (6.3% of the time)

---

4. Is our actual result of 14 (under the coin model)...[^4]
   (a) very surprising?
   (b) somewhat surprising?
   (c) not very surprising?
   
[^4]: a. very surprising (prob of 14 or more is 0.0021)

---

5.	Based on the first handwriting study, can we conclude that cursive causes higher scores (on average)?[^5]
    (a)	Yes
    (b)	No
    (c)	It depends

[^5]: b. No, we can't establish causation from an observational study.

---

6.	Based on the second handwriting study, can we conclude that cursive causes higher scores (on average)?[^6]
    (a)	Yes
    (b)	No
    (c)	It depends

[^6]: a. Yes.  For the exam(s?) under study, cursive caused higher scores on average.

---

7. A possible confounding variable for the handwriting study is:[^7]
   (a) grade of the student (age)
   (b) region of country where the SAT was taken
   (c) academic ability of the student
   (d) gender identity of the student
   (e) number of siblings of the student.

[^7]: You must connect the variable to both the explanatory and response variable.  For me, that is easiest to do with c. academic ability of the student.

---

8. The main reason we randomly assign the explanatory variable is:[^8]
   (a) To get the smallest p-value possible
   (b) To balance the expected causal mechanism across the two groups
   (c) To balance every possible variable except the causal mechanism across the two groups
   (d) So that our sample is representative of the population
   (e) So that the sampling process is unbiased 

[^8]: c. To balance every possible variable except the causal mechanism across the two groups

---

9. The main reason we take random samples from the population is:[^9]
   (a) To get the smallest p-value possible
   (b) To balance the expected causal mechanism across the two groups
   (c) To balance every possible variable except the expected causal mechanism across the two groups
   (d) So that our sample is representative of the population
   (e) So that the sampling process is unbiased

[^9]: d. So that our sample is representative of the population

---

10. Are there effects of second-hand smoke on the health of children?[^10]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^10]: c. unhappily obs study (becuase we want to establish causation)

---

11. Do people tend to spend more money in stores located next to food outlets with pleasing smells?[^11]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^11]: a. definitely obs study (do we care about causation?  maybe.  maybe not.)
---

12. Does cell phone use increase the rate of automobile accidents?[^12]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^12]: c. unhappily obs study

---

13. Do people consume different amounts of ice cream depending on the size of bowl used?[^13]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^13]: b. definitely experiment

---

14.	Which is more effective: diet A or diet B?[^14]
    (a) definitely obs study
    (b) definitely experiment
    (c) unhappily obs study
    (d) unhappily experiment

[^14]: b. definitely experiment

---

15. Suppose that we record the midterm exam score and the final exam score for every student in a class.  What would the value of the correlation coefficient be if every student in the class scored ten points higher on the final than on the midterm:[^15]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^15]: e. r = 1

---

16. Suppose that we record the midterm exam score and the final exam score for every student in a class.  What would the value of the correlation coefficient be if every student in the class scored five points lower on the final than on the midterm:[^16]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1
    
[^16]: e. r = 1

---

17. Suppose that we record the midterm exam score and the final exam score for every student in a class.  What would the value of the correlation coefficient be if every student in the class scored twice as many points on the final than on the midterm:[^17]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^17]: e. r = 1

---

18. Suppose you guessed every value correctly (guess the correlation applet), what would be the value of the correlation coefficient between your guesses and the actual correlations?[^18]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^18]: e. r = 1

---

19. Suppose each of your guesses was too high by 0.2 from the actual value of the correlation coefficient, what would be the value of the correlation coefficient between your guesses and the actual correlations?[^19]
    (a) r = -1
    (b) -1 < r < 0
    (c) r = 0
    (d) 0 < r < 1
    (e) r = 1

[^19]: e. r = 1

---

20. A correlation coefficient equal to 1 indicates that you are a good guesser.[^20]
    (a) TRUE
    (b) FALSE

[^20]: b. FALSE.  You could get every single value wrong and still have a correlation of one.

---

21. Perfect Correlation... if not for a single outlier  
n = 101 observations:  1 observation in top left, 25 observations in each in of the points near the bottom right.    
The value of the correlation, r, is:[^21]
    (a)	-1   < r < -0.9
    (b)	-0.9 < r < -0.5
    (c)	-0.5 < r <  0.5
    (d)	 0.5 < r <  0.9
    (e)	 0.9 < r <  1


[^21]: c. r = -0.416    
---

```{r}
#| echo: false
#| warning: false
#| message: false
#| 
x_value <- c(rep(17, 25), rep(18, 25), rep(19, 25), rep(20, 25), 1)
y_value <- c(rep(1, 25), rep(2, 25), rep(3, 25), rep(4, 25), 20)

#cor(x_value, y_value)
library(ggplot2)
ggplot(data.frame(x_value, y_value)) + 
  geom_point(aes(x = x_value, y = y_value)) +
  theme(aspect.ratio = 1)
```



---

22. The sum of residuals from the sample mean (no X):[^22] 
$$\sum_{i=1}^n(Y_i - \overline{Y})$$
    (a) is positive
    (b) is negative
    (c) is zero
    (d) is different for every dataset

[^22]: c. always zero

---

23. A good measure of how well the prediction (of the sample mean) fits the data is:[^23]
    (a) $\sum_{i=1}^n(Y_i - \overline{Y})$
    (b) $\sum_{i=1}^n(Y_i - \overline{Y})^2$
    (c) $\sum_{i=1}^n|Y_i - \overline{Y}|$
    (d) $\mbox{median}(Y_i - \overline{Y})$
    (e) $\mbox{median}|Y_i - \overline{Y}|$

[^23]: we usually use b. $\sum_{i=1}^n(Y_i - \overline{Y})^2$ (for calculus and historical reasons), but c. and e. are also totally reasonably answers.

---


24. A good measure of how well the prediction (of the regression line) fits the data is:[^24]
    (a) $\sum_{i=1}^n(Y_i - \hat{Y}_i)$
    (b) $\sum_{i=1}^n(Y_i - \hat{Y}_i)^2$
    (c) $\sum_{i=1}^n|Y_i - \hat{Y}_i|$
    (d) $\mbox{median}(Y_i -\hat{Y}_i)$
    (e) $\mbox{median}|Y_i -\hat{Y}_i|$

[^24]: we usually use b. $\sum_{i=1}^n(Y_i - \hat{Y}_i)^2$ (for calculus and historical reasons), but c. and e. are also totally reasonably answers.

---

25. What math is used to find the value of $m$ that minimizes:[^25]
$$\sum_{i=1}^n(Y_i - m)^2$$
    (a) combinatorics
    (b) derivative
    (c) integral
    (d) linear algebra

[^25]: b. derivative

---

26. $\sum_i(Y_i - \overline{Y})^2$ is sometimes $\geq \sum_i(Y_i - \hat{Y}_i)^2$[^26]
    a. TRUE
    b. FALSE, $\sum_i(Y_i - \overline{Y})^2$ is always $\geq \sum_i(Y_i - \hat{Y}_i)^2$
    c. FALSE, $\sum_i(Y_i - \overline{Y})^2$ is never $\geq \sum_i(Y_i - \hat{Y}_i)^2$

[^26]: b. FALSE, $\sum_i(Y_i - \overline{Y})^2$ is always $\geq \sum_i(Y_i - \hat{Y}_i)^2$

---

27. When writing the regression equation, why is there a hat ( ^) on the response variable?[^27]
    (a) because the prediction is an estimate
    (b) because the prediction is an average
    (c) because the prediction may be due to extrapolation
    (d) a & b
    (e) all of the above

[^27]: d. due to estimation and average

---

28. "Observed data or more extreme" is:[^28]
    (a) fewer than 9
    (b) 9 or fewer
    (c) 9 or more
    (d) more than 9

[^28]: c. 9 or more

---

29. What is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?[^29]
    (a) 0
    (b) 9
    (c) 5.3
    (d) 11
    (e) 15

[^29]: c. 5.3 because (15/31)*11 = 5.3

---

30. In the Botox and Pain Relief example, the p-value is calculated.  What does "probability" refer to?[^30]
    (a) random allocation
    (b) random sample

[^30]: a. random allocation

---

p-value = probability of the observed data or more extreme given the null hypothesis is true.

---

31. What conclusion would you draw from the Back Pain and Botox study?[^31]
    (a) Not enough evidence to conclude that Botox is more effective than the placebo.
    (b) Strong evidence that Botox is equally as effective as the placebo.
    (c) Strong evidence that Botox is more effective than the placebo.
    
[^31]: c. Strong evidence that Botox is more effective than the placebo. p-value was roughly 0.005.

---

32. If we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?[^32]
    (a) Substantially greater than 50%
    (b) Substantially less than 50%
    (c) Very close to 50%

[^32]: c. Close to 50%  (the point estimate is 0.6)

---

33. Material check-in
    (a) So far, so good
    (b) Concepts are good, R is confusing
    (c) R is good, concepts are confusing
    (d) Everything is confusing
    
---

34. People check-in
    (a) So far, so good
    (b) I can go to office hours / mentor sessions / learning community check-ins, but I didn't go this week.
    (c) I can't make the scheduled office hours / mentor sessions / learning community check-ins
    (d) I'm looking for someone to study with
    
---

See Canvas front page for **anonymous** survey / feedback for the class.  Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.

---

35. If communication medium and cheating are independent variables, how many of the email senders (out of 26) would you expect to cheat?[^35]
    (a)	10 (ish)
    (b)	13 (ish)
    (c)	16 (ish)
    (d)	20 (ish)
    (e)	24 (ish)

[^35]: d. 20 (ish), 26*(38/48) = 20.58

---

36. When looking at the null differences, is the observed result of 28.7%:[^36]
    (a)	Very surprising
    (b)	Somewhat surprising
    (c)	Not very surprising

[^36]: b. Somewhat surprising, p-value was 0.04

---

37. Hypothesis: the number of hours that grade-school children spend doing homework predicts their future success on standardized tests.[^37]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^37]: c. alternative, one sided (because probably we are studying that it **increases** their success rate)

---

38. Hypothesis: king cheetahs on average run the same speed as standard spotted cheetahs.[^38]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^38]: a. null, two sided (because I have no idea which cheetah might run faster)

---

39. Hypothesis: the mean length of African elephant tusks has changed over the last 100 years.[^39]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^39]: d. alternative, two sided (because I have no idea whether they've increased or decreased)

---

40. Hypothesis: the risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not.[^40]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^40]: a. null, one sided (because I happen to know that folic acid is thought to prevent facial clefts)

---

41. Hypothesis: caffeine intake during pregnancy affects mean birth weight.[^41]
    (a) null, one sided
    (b) null, two sided
    (c) alternative, one sided
    (d) alternative, two sided

[^41]: c. alternative, one sided (because I happen to know that caffeine is thought to decrease baby's birth weight)

---

42. In this class, the word parameter means:[^42]
    (a) The values in a model
    (b) Numbers that need to be tuned
    (c) A number which is calculated from a sample of data.
    (d) A number which (is almost always unknown and) describes a population.

[^42]: d. A number which (is almost always unknown and) describes a population.

---

43. I know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW/Lab solutions[^43]
     (a) TRUE
     (b) FALSE
     
[^43]: The warm-up solutions and clicker questions are on the main course website.  The HW & Lab solutions are on Canvas under Files.
    
---

44. You have a sample of size n = 50.  You sample with replacement 1000 times (to get 1000 bootstrap resamples). What is the sample size of each bootstrap resample?[^44]
    (a) 50
    (b) 1000

[^44]: a. 50

---

45. You have a sample of size n = 50.  You sample with replacement 1000 times (to get 1000 bootstrap resamples). How many bootstrap statistics will you have?[^45] 
     (a) 50
     (b) 1000

[^45]: b. 1000
---

46. Let's say you take a random sample and compute $\hat{p}=0.3.$  After bootstrapping, you see that the bootstrapped resamples produce almost all the $\hat{p}_{boot}$ within plus or minus 0.01 of your original statistic. It seems that the parameter $p$ is probably:[^46]
	   a. 0.3
	   b. between (0.2, 0.4)
	   c. between (0.29, 0.31)
	   d. between (0.28, 0.32)
	   e. huh?  how can we get $p$ from $\hat{p}?$

[^46]: c. between (0.29, 0.31)

---

47. In a second analysis, I create a 90% CI for the true proportion $p.$ What is the impact (of switching from 95% to 90%) on the CI?[^47]
    (a) narrower
    (b) less likely (long-run) to capture the parameter
    (c) neither
    (d) both

[^47]: d. both.  the intervals will be less likely (long-run) to capture the parameter *and* they will be narrower.

---

48. In a second study, I set out to obtain twice as much data (as in the first study) in order to create a 95% CI for the true proportion $p.$ What is the impact (of the larger sample) on the CI?[^48]
    (a) narrower
    (b) more likely (long-run) to capture the parameter
    (c) neither
    (d) both

[^48]: a. narrower (the sample size will not change the capture rate)

---

49. What is one main reason to use bootstrapping to find a confidence interval?[^49]
    (a) larger coverage probabilities
    (b) narrower intervals
    (c) more resistant to outliers
    (d) can be done for any statistic
    
[^49]: d. can be done for any statistic

---
 
50. 95% CI for the true median mercury:[^50]
    (a) (0.025 mg/kg, 0.975 mg/kg)
    (b) (0.469 mg/kg, 0.053 mg/kg)
    (c) (0.053 mg/kg, 0.469 mg/kg)
    (d) (0.34 mg/kg, 0.56 mg/kg)

[^50]: d. (0.34 mg/kg, 0.56 mg/kg)
---

```{r fig.cap = "From StatKey applet: https://www.lock5stat.com/StatKey/",  out.width="95%", fig.align='center', echo=FALSE}
knitr::include_graphics("images/merc_boot.jpg")
```


---

51. What are the observational units for your individual candy study?[^51]
     a. Color of the candy
     b. Piece of candy
     c. Cup of candy
     d. The Hershey Company
     e. Proportion that are orange

[^51]: b. Piece of candy

---

52. What are the observational units for the class compilation (dotplot)?[^52]
     a. Color of the candy
     b. Piece of candy
     c. Cup of candy
     d. The Hershey Company
     e. Proportion that are orange

[^52]: c. Cup of candy

---

53. How does the sampling distribution for the sample proportion change as n changes (for a fixed p)?[^53]
     a. The spread changes
     b. The symmetry changes
     c. The center changes
     d. The shape changes

[^53]: a. The spread changes

---

54. How does the sampling distribution change as p changes (for a fixed n)?[^54]
     a. The spread changes
     b. The symmetry changes
     c. The center changes
     d. The shape changes
     
[^54]: c. The center changes (the spread also changes a little bit, but mostly the center)

---

55. The Central Limit Theorem says that the distribution of $\hat{p}$ will be approximately normal with what center:[^55]
    (a) $\hat{p}$
    (b) $p$
    (c) 0.5
    (d) 1
    (e) $\sqrt{p(1-p) / n}$

[^55]: b. p

---

56. Would you rather have an extra 20 points on the SAT or an extra 10 points on the ACT?[^56]
     a. +20 on the SAT
     b. +10 on the ACT

[^56]: b. +10 on the ACT

---

57. The standardized score (z-score) counts:[^57]
    (a) the number of standard deviations from the mean
    (b) the number of standard deviations above the mean
    (c) the number of standard deviations below the mean
    (d) the distance from the mean
    (e) the distance from the standard deviation

[^57]: a. the number of standard deviations from the mean

---

58. If the normal distribution is a good model, we would expect the large majority of our z scores to be:[^58]
    (a) within $\pm$ 1 of the mean
    (b) within $\pm$ 2 of the mean
    (c) within $\pm$ 1 
    (d) within $\pm$ 2 

[^58]: d. within $\pm$ 2

---

59. With your cup of candy, you personally got a Z score of:[^59]
    (a) between (-1, 1) (not including 1)
    (b) between (-2, -1] or [1, 2)
    (c) between (-3, -2] or [2, 3)
    (d) -3 or smaller or 3 or above

[^59]: a. or b.  you most likely got between -2 and 2

---

60. Assume n = 100 and p= 0.8  (note: $\sqrt{(0.8 \cdot 0.2)/100} = 0.4/10 = 0.04$)  
What is the largest reasonable distance between $\hat{p}$ and $p$?  
That is, we would expect $\hat{p}$ and $p$ to be no more than _____ apart[^60]
    (a) 0.04
    (b) 0.08
    (c) 0.12
    (d) 0.16
    (e) 0.24

[^60]: b. 0.08 (we usually consider two standard deviations)

---


61. Assume n = 100 and p= 0.8  (note: $\sqrt{(0.8 \cdot 0.2)/100} = 0.4/10 = 0.04$) Which statement is true?[^61]
    (a) 95% of $\hat{p}$ are between (0.76, 0.84)
    (b) 95% of $\hat{p}$ are between (0.72, 0.88)
    (c) 95% of $\hat{p}$ are between (0.68, 0.92)
    (d) 95% of $p$ are between (0.76, 0.84)
    (e) 95% of $p$ are between (0.72, 0.88)

[^61]: b. 95% of $\hat{p}$ are between (0.72, 0.88)

---

62.  If you want a 90% confidence interval for p, your z* multiplier should be[^62]
     (a) less than 1
     (b) less than 2 (but greater than 1)
     (c) equal to 2
     (d) greater than 2 (but less than 3)
     (e) greater than 3

[^62]: b. less than 2 (but greater than 1)

---

63. What is the difference between Z* and a Z score?[^63]
     a. Z score comes from the data, Z* and is a pre-defined unit of measurement.
     b. Z* comes from the data, and Z score is a pre-defined unit of measurement
     c. Z score assumes the null hypothesis is true and Z* doesn't.
     d. Z* assumes the null hypothesis is true, and Z score doesn't

[^63]: a. Z score comes from the data, Z* and is a pre-defined unit of measurement.

---

64. Let's say we are making confidence intervals (not doing a hypothesis test), what is your best guess for $SE(\hat{p})$?[^64]
    (a) $\sqrt{0.5 \cdot (1 - 0.5) / n}$
    (b) $\sqrt{p \cdot (1 - p) / n}$
    (c) $\sqrt{\hat{p} \cdot (1 - \hat{p}) / n}$
    (d) $\sqrt{X \cdot (1 - X) / n}$
    (e) $\sqrt{0.95 \cdot (1 - 0.95) / n}$

[^64]: c. $\sqrt{\hat{p} \cdot (1 - \hat{p}) / n}$

---

65. The following is a correct interpretation of the CI:[^65]  

> 95% confident that the interval includes the sample proportion who believe that the global poverty rate has doubled. 

   a. TRUE
   b. FALSE

[^65]: a. TRUE

---

66.  The following is a correct interpretation of the CI:[^66]  

> If researchers were to select a new sample of 1005 adult Americans, then we're 95% confident that between 56% and 62% of those people would answer "doubled" to the question. 

   a. TRUE
   b. FALSE

[^66]: b. FALSE (we are 95% confident that the new interval will contain the true value.  We do not think that the new interval will be the same as the original interval.)

---

67.  Let's say that the null hypothesis (e.g., p=0.47) is TRUE.   My level of significance is 0.03 (reject if p-value < 0.03). How often will I reject the null hypothesis?[^67]
     (a) 1 % of the time
     (b) 3% of the time
     (c) 5 % of the time
     (d) 95% of the time
     (e) 97% of the time

[^67]: b. 3% of the time

---

What does "of the time" mean???

It means in repeated samples.  That is, in 3% of all datasets we'd take from that exact same population, we would mistakenly reject the actually true hypothesis that p=0.47.

---

68. Let's say that the null hypothesis (e.g., p=0.47) is TRUE.   My level of significance is 0.03.  
How often will p be in a 97% confidence interval?[^68]
    (a) 1 % of the time
    (b) 3% of the time
    (c) 5 % of the time
    (d) 95% of the time
    (e) 97% of the time

[^68]: e. 97% of the time

---

What does "of the time" mean???

It means in repeated samples.  That is, in 97% of all datasets we'd take from that exact same population, we would capture the true population proportion of 0.47.

---

69.  Suppose the sample is 10 times larger.  The SE of the statistic:[^69]
     a. increases
     b. stays the same
     c. decrease

[^69]: c. decreases

---

70.  Suppose the population is 10 times larger.  The SE of the statistic:[^70]
     a. increases
     b. stays the same
     c. decrease

[^70]: b. stays the same (the population size has no effect on the sampling distribution of the statistic)

---

71. Suppose the sample is 10 times larger.  The variability of the data:[^71]
     a. increases
     b. stays the same
     c. decrease

[^71]: b. stays the same (the variability of the data should be the same as the variability of the population, regardless of the sample size)


---

72. How many hits out of 20 at bats would make you believe him?[^72]
    (a)	5
    (b)	6
    (c)	7
    (d)	8
    (e)	9

[^72]: e. 9

---

73. Type I error is[^73]
    (a)	We give him a raise when he deserves it.
    (b)	We don't give him a raise when he deserves it.
    (c)	We give him a raise when he doesn't deserve it.
    (d)	We don't give him a raise when he doesn't deserve it.
 
[^73]: c. We give him a raise when he doesn't deserve it.

---

74. Type II error is[^74]
    (a)	We give him a raise when he deserves it.
    (b)	We don't give him a raise when he deserves it.
    (c)	We give him a raise when he doesn't deserve it.
    (d)	We don't give him a raise when he doesn't deserve it.

[^74]: c. We don't give him a raise when he deserves it.

---

75. Power is the probability that:[^75]
    (a)	We give him a raise when he deserves it.
    (b)	We don't give him a raise when he deserves it.
    (c)	We give him a raise when he doesn't deserve it.
    (d)	We don't give him a raise when he doesn't deserve it.

[^75]: a. We give him a raise when he deserves it.

---

76. The player is more worried about[^76]
    (a)	A type I error
    (b)	A type II error

[^76]: b. A type I error

---

77. The manager is more worried about[^77]
    (a)	A type I error
    (b)	A type II error

[^77]: a. A type I error

---

78. Increasing your sample size[^78]
    (a)	Increases your power
    (b)	Decreases your power

[^78]: a. Increases your power

---

79. Making your significance level more stringent ($\alpha$ smaller)[^79]
    (a)	Increases your power
    (b)	Decreases your power

[^79]: b. Decreases your power

---

80. A more extreme alternative[^80]:
    (a)	Increases your power
    (b)	Decreases your power

[^80]: a. Increases your power

---

81. Is the Alien’s interval for the true proportion of all humans who self-identify as female consistent with your lived experience?[^81]
    (a) Yes
    (b) No
    (c) I don’t understand what the confidence interval represents.

[^81]: b. No.  My experience is that close to 50% of humans self-identify as female.

---

82. As we’ve seen with the applet, about 5% of all 95% intervals fail to capture the actual value of the population parameter.  Do you think the alien just got a "red" interval?[^82]
    (a)	Yes
    (b)	No

[^82]: b. No.  They didn't just "get unlucky".  Instead, the reason the interval failed to capture the true parameter is because the sample was not representative of the population.  

---

83.	 Would it be reasonable for the alien to conclude, with 95% confidence, that between 16.5% and 33.5% of US Senators in the year 2023 self-identify as female?[^83]
     (a)	Yes
     (b)	No

[^83]: b. No. We know (for sure, with 100% confidence) that exactly 25% of U.S. senators in 2019 self identify as female. If that’s the entire population of interest, there’s no reason to calculate a confidence interval.

---

84. The "random" part in clinical trials typically comes from:[^84]
    (a) random samples
    (b) random allocation of treatment

[^84]: b. random allocation of treatment

---

85. The "random" part in polling typically comes from:[^85]
    (a) random samples
    (b) random allocation of treatment

[^85]: random samples

---

86. You want to collect data to investigate whether teenagers in the United States have read fewer Harry Potter books than teenagers in the United Kingdom.  Would you make use of random sampling, random assignment, both, or neither?[^86]
    (a)	Random sampling
    (b)	Random assignment
    (c)	Both
    (d)	Neither

[^86]: b. Random sampling, although it would be pretty hard to do a *true* random sample from either country.

---

87.	An instructor wants to investigate whether using a red pen to grade assignments leads to lower scores on exams than using a blue pen to grade assignments.  Would you advise the professor to make use of random sampling, random assignment, both, or neither?[^87]
    (a)	Random sampling
    (b)	Random assignment
    (c)	Both
    (d)	Neither

[^87]: b. Random assignment.  Randomly decide which exams to grade with which pen, and then record the scores.

---

88.	A student decides to investigate whether NFL football games played in indoor stadiums tend to have more points scored than games played outdoors.  The student examines points scored in every NFL game of the 2022 season. Has the student used random sampling, random assignment, both, or neither?[^88]
    (a)	Random sampling
    (b)	Random assignment
    (c)	Both
    (d)	Neither

[^88]: d. Neither.  The student has the entire population of teams and was not able to randomly assign stadium type.


---

89. Relative Risk is[^89]
    (a) the difference of two proportions
    (b) the ratio of two proportions
    (c) the log of the ratio of two proportions
    (d) the log of the difference of two proportions

[^89]: b. the ratio of two proportions

---

90. In order to find a CI for the true RR, our steps are:[^90]  
Step 1. ln(RR-hat)  
Step 2. add ± z* sqrt( 1/A - 1/(A+C) + 1/B - 1/(B+D) )  
Step 3. find exp of the endpoints   
    (a) because the sampling distribution of RR is normal
    (b) because RR is typically greater than 1
    (c) because the ln transformation makes the sampling distribution almost normal
    (d) because RR is invariant to the choice of explanatory or response variable

[^90]: c. because the ln transformation makes the sampling distribution almost normal

---

91. In finding a CI for $p_1$/$p_2$, why is it okay to exponentiate the end points of the interval for ln($p_1$/$p_2$)?[^91] 
    (a) if ln($p_1$/$p_2$) is in the natural log-interval, $p_1$/$p_2$ will be in the exponentiated interval.
    (b) the natural log of the RR makes the distribution approximately normal.
    (c) the natural log compresses values that are > 1 and spreads values < 1.

[^91]: a. if ln($p_1$/$p_2$) is in the natural log-interval, $p_1$/$p_2$ will be in the exponentiated interval.  (Where "okay" means you have 95% coverage in repeated samples.)

---

92.   Usually, the CI for  $p_1$/$p_2$ is considered to be "significant" if[^92] 
      (a) $p_1$/$p_2$ is not in the interval
      (b) $\hat{p}_1 / \hat{p}_2$ is not in the interval
      (c) 0 is not in the interval
      (d) 1 is not in the interval

[^92]: d. 1 is not in the interval

---

93. In order to find a CI for the true OR, our steps are:[^93]  
Step 1. ln(OR-hat)  
Step 2. add ± z* sqrt( 1/A + 1/B + 1/C + 1/D )  
Step 3. find exp of the endpoints   
     (a) because the sampling distribution of OR is normal
     (b) because OR is typically greater than 1
     (c) because the ln transformation makes the sampling distribution almost normal
     (d) because OR is invariant to the choice of explanatory or response variable

[^93]: c. because the ln transformation makes the sampling distribution almost normal

---

94. Sample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall.  Record if the person is in the NBA.  
What is measurable?[^94]
    (a) P(NBA if 6’ tall)
    (b) P(6’ tall if in the NBA)
    (c) both
    (d) neither  

[^94]: a. P(NBA if 6' tall) (cohort: cannot measure the probability of the explanatory variable given the response)

---

95.  Sample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?[^95]
     (a) P(NBA if 6’ tall)
     (b) P(6’ tall if in the NBA)
     (c) both
     (d) neither  

[^95]: b. P(6' tall if in the NBA) (case-control:  cannot measure the probability of the response variable given a level of the explanatory variable)

---

96.  Sample 10,000,000 people.  Record their height and whether or not they are in the NBA.  
What is measurable?[^96]
     (a) P(NBA if 6’ tall)
     (b) P(6’ tall if in the NBA)
     (c) both
     (d) neither  

[^96]: c. both (cross-classification: can measure all the probabilities)

---

97.  When we randomly select individuals based on the explanatory variable, we cannot accurately measure[^97]
     (a) the proportion of people in the population in each explanatory category
     (b) the proportion of people in the population in each response group
     (c) anything about the population
     (d) confounding variables
 
[^97]: a. the proportion of people in the population in each explanatory category

---

98. The odds ratio is invariant to which variable is explanatory and which is response means:[^98]
    (a) we always put the bigger odds in the numerator
    (b) we must collect data so that we can estimate the response in the population
    (c) which variable is called the explanatory changes the value of the OR
    (d) which variable is called the explanatory does not change the value of the OR

[^98]: d. which variable is called the explanatory does not change the value of the OR 

---

99.  One reason we should be careful interpreting relative risks is if:[^99]
     (a) we don’t know the difference in proportions
     (b) we don’t know the SE of the relative risk
     (c) we might be dividing by zero
     (d) we don’t know the baseline risk

[^99]: d. we don't know the baseline risk

---

100. If the null hypothesis is true, the observed counts will equal the expected counts.[^100]
     (a) True
     (b) False

[^100]: b. False

---

101. To reject the null hypothesis we want to see[^101]
     (a) a small $X^2$ value
     (b) a big $X^2$ value

[^101]: b. a big $X^2$ value


---

102. A chi-square test has a[^102]
     (a) one-sided alt hypothesis, and we only consider the upper end of the sampling distribution
     (b) one-sided alt hypothesis, and we consider both ends of the sampling distribution
     (c) two-sided alt hypothesis, and we only consider the upper end of the sampling distribution
     (d) two-sided alt hypothesis, and we consider both ends of the sampling distribution 

[^102]: c. two-sided alt hypothesis, and we only consider the upper end of the sampling distribution

---

103. For the lighting study, which variable is the explanatory variable?[^103]
     (a) sleeping light
     (b) eye sightedness
     (c) child
     (d) parent

[^103]: a. sleeping light

---

104. If we sample randomly from a population, the conclusions we can make are about:[^104]
     (a) causation 
     (b) population characteristics

[^104]: b. population characteristics

---

105. Based on the night light / myopia example, the correct conclusion is:[^105]
     (a) the p-value is small, so sleeping in a lit room makes it more likely that you are near-sighted. 
     (b) the p-value is small, so sleeping in a dark room makes it more likely that you are near-sighted. 
     (c) the p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms. 
     (d) $\hat{p}_{\mbox{near}}$ if lit room = 41/75 = 0.547 and    $\hat{p}_{\mbox{near}}$ if dark = 18/172 = 0.105, therefore sleeping with the light on is bad for you. 

[^105]: c. the p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms.

---

106. A possible confounding variable for the night light study is:[^106]
     (a) low birth weight
     (b) race (70% of the children were white)
     (c) region of the country where the clinic was located

[^106]: a. low birth weight (the argument needs to be made that the confounding variable is associated with both the explanatory and the response variable)

---

107. Which dataset has the smallest standard deviation?[^107]
     (a)	A: left
     (b)	B: center
     (c)	C: right

```{r fig.height=4}
#| echo: false
#| warning: false
#| message: false

set.seed(47)
par(mfrow = c(1,3))
hist(runif(50,0,10), xlab="A", xlim = c(0,10), main = "")
hist(10 - rchisq(50,1), xlab = "B", xlim = c(0,10), main = "")
hist(rnorm(50,5,2), xlab = "C", xlim = c(0,10), main = "")
```
[^107]: b. B: center (the typical distance from the mean is smallest)

---

108. Which of the two dotplots displays the dataset with the smaller IQR?[^108]
     (a)	A
     (b)	B

```{r fig.height=4.2}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
set.seed(4704)
data <- data.frame(x = c(sample(25:35, 16, replace = TRUE), 11, 13, 50, 60,
                         sample(15:50, 20, replace = TRUE)),
                   group = c(rep("A", 20), rep("B", 20)))

#create stacked dot plot
ggplot(data, aes(y = x, x = group)) +
  geom_dotplot(binaxis = "y", stackdir = "up") + 
  xlab("") + 
  coord_flip()
```

[^108]: a. A (the IQR measures the middle 10 points in each group, the middle 10 points in group A are closer together than the middle 10 points in group B)

---

109. The standard deviation of weights (mean = 167 lbs) is approximately[^109]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100

[^109]: d. 35 (none of the other answers are reasonable)

---

110. The standard deviation of average weights (mean = 167 lbs) in a sample of size 10 is approximately[^110]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100

[^110]: c. 10 ($\approx 35/\sqrt{10}$)

---

111. The standard deviation of average weights (mean = 167 lbs) in a sample of size 50 is approximately[^111]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100

[^111]: b. 5 ($\approx 35/\sqrt{100}$)
---

112. The standard deviation of average weights (mean = 167 lbs) in a sample of size 1000 is approximately[^112]
     (a) 1
     (b) 5
     (c) 10
     (d) 35
     (e) 100
     
[^112]: a. 1 ( $\approx 35/\sqrt{1000}$)
     
---

Q: what is the most confusing part of understanding the difference between the variability of the weights and the variability of the average of the weights?

---

113. The sampling distribution of the mean will be[^113]
     (a) centered below the data distribution
     (b) centered at the same place as the data distribution
     (c) centered above the data distribution
     (d) unrelated to the center of the data distribution

[^113]: b. centered at the same place as the data distribution

---

114. The sampling distribution of the mean will be[^114]
     (a) less variable than the data distribution
     (b) the same variability as the data distribution
     (c) more variable than the data distribution
     (d) unrelated to the variability of the data distribution

[^114]: a. less variable than the data distribution

---

115. Why did we switch from talking about total weight to talking about average weight?[^115]
     (a) So that it is easier to infer from the sample to the population.
     (b) Because the Coast Guard certifies vessels according to average weight.
     (c) Because the average is less variable than the sum.
     (d) Because the average has a normal distribution and the sum doesn’t.

[^115]: a. So that it is easier to infer from the sample to the population

---

116. When the population is skewed right, the sampling distribution for the sample mean will be[^116]
     (a) always skewed right
     (b) skewed right if n is big enough
     (c) always normal
     (d) normal if n is big enough

[^116]: d. normal if n is big enough

---

117. What does the CLT say?[^117]


[^117]: Describing random samples (of size n) from the population, the sampling distribution of the sample mean is normal if the sample size (n) is large enough.

---

```{r eval = FALSE, echo = FALSE}

63.	A newspaper article claims that the average age for people who receive food stamps is 40 years. You believe that the average age is less than that. You take a random sample of 100 people who receive food stamps and find their average age to be 39.2 years. You find that this is significantly lower than the age of 40 stated in the article (p < 0.05). What would be an appropriate interpretation of this result?
(a) The statistically significant result indicates that the majority of people who receive food stamps is younger than 40.
(b) Although the result is statistically significant, the difference in age is not of practical importance.
(c) An error must have been made. This difference is too small to be statistically significant.
64.	In order to investigate a claim that the average time required for the county fire department to respond to a reported fire is greater than 5 minutes, county staff determined the response times for 40 randomly selected fire reports.  The data was used to test H0:  μ = 5 versus Ha:  μ > 5 and the computed p-value was 0.12.  If a 0.05 level of significance is used, what conclusions can be drawn?

(a) There is convincing evidence that the mean response time is 5 minutes (or less).
(b) There is convincing evidence that the mean response time is greater than 5 minutes.
(c) There is not convincing evidence that the mean response time is greater than 5 minutes. 
```

