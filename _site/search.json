[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester! Each week, follow the general process outlined below:\n\nBefore class on Tuesday, read the suggested article \nEnjoy the notes / readings \nAttend class, review the warm-up  if you have any questions after completing it during class.\nComplete the Lab  and HW  assignments\n\nLab due every Tuesday by 11:59pm\nHW due every Thursday by 11:59pm\n\nDiscuss the reflection questions  and ethics considerations  (see class notes) with your classmates, mentor, and professor\nIMS: Introduction to Modern Statistics by Çetinkaya-Rundel and Hardin.\nISCAM: Investigating Statistics, Concepts, Applications, and Methods by Chance and Rossman. Many of the examples come from ISCAM, a pdf of which can be purchased for $5.\n\nISCAM applets\nStatKey (boostrapping) applets\n\n\n\n\n\n\n\n\n\n \n  \n    date \n    topic \n    agenda \n    readings \n    article (Tues) \n    assignments \n  \n \n\n  \n    Week 1  1.17.23 \n    Intro +  variables &   studies  SLR \n    • course info  • tests  • studies  • causation \n     Introduction   Studies    ISCAM 1.1   IMS 1 + 2 \n     none   Felicity Enders \n     WU1   WU2   HW0 pdf   HW0 Rmd \n  \n  \n    Week 2  1.24.23 \n    Correlation + Leastsquares \n    • correlation • linear model  • R^2 \n     Cor   Least Sq    ISCAM 5.6, 5.7, 5.8   IMS 7   guess corr   least sq \n     AstraZeneca   W.E.B. Du Bois \n     WU3   WU4   HW1 pdf   HW1 Rmd   Lab1 pdf   Lab1 Rmd \n  \n  \n    Week 3  1.31.23 \n    Hyp Test + RandomizationTest \n    •  structure of hypothesis testing \n     Tests    2x2 Rand   IMS 11 \n     chocolate   Regina Nuzzo \n     WU5   WU6   HW2 pdf   HW2 Rmd   Lab2 pdf   Lab2 Rmd \n  \n  \n    Week 4  2.7.23 \n    Bootstrapping \n    • boot samp dist  • boot CI \n     Bootstrapping   Boot CIs    StatKey    CIs   IMS 12 \n     confounding   David Blackwell \n     WU7   WU8   HW3 pdf   HW3 Rmd   Lab3 pdf   Lab3 Rmd \n  \n  \n    Week 5  2.14.23 \n    normality +CLT \n    • normal dist • CLT • Z-score  • empirical rule  • conf int  • norm prob  • hyp test \n    CLT    Norm dist    Samp Dist    IMS  \n     efficacy   vaccines   Florence Nightingale \n     WU9   WU10   HW4 pdf   HW4 Rmd   Lab4 pdf   Lab4 Rmd \n  \n  \n    Week 6  2.21.23 \n    Errors + Power +Sampling \n    • Type I error  • Type II error  • Power  • biased samples \n     Errors   Sampling    Power   IMS 14 + 2 \n     type II   type I   Tommy Wright \n     WU11   WU12   HW5 pdf   HW5 Rmd   Lab5 pdf   Lab5 Rmd \n  \n  \n    Week 7  2.28.23 \n    Rel Risk +Odds Ratios \n    •  RR  • OR • Case-control  • Cohort \n     diff props   RR    OR    ISCAM 3.9-3.11   IMS 17 \n     interaction   Monica Jackson \n     WU13   WU14   HW6 pdf   HW6 Rmd   Lab6 pdf   Lab6 Rmd \n  \n  \n    Week 8  3.7.23 \n    catch-up \n     \n     study materials  (see Canvas)  advice for exam 1 \n    no lab on 3.10.23 \n     HW pdf - not due ever   HW Rmd - not due ever   Lab7 pdf   Lab7 Rmd \n  \n  \n    3.9.23 \n    Exam 1 \n     \n     \n     \n     \n  \n  \n    3.14.23 \n    spring break \n     \n     \n     \n     \n  \n  \n    Week 9  3.21.23 \n    χ^2 Test \n    • expected counts  • X^2 statistic  • χ^2 test \n     χ^2 Test    ISCAM 3.2   IMS 18 \n     masks    Maricela Cruz \n     WU15   WU16 \n  \n  \n    Week 10  3.28.23 \n    1 mean +pred interval \n    • dist of X-bar  • SE(X-bar)  • t-test of μ • t-CI for μ  • PI for X_{n+1} \n     1 mean   pred int    IMS 19 \n      Data sovereignty  no lab on 3.31.23   Edna Paisano \n     HW 8 pdf   HW 8 Rmd   Pilot Study Due    Lab 8 doc \n  \n  \n    3.28.23 \n    Proj 1: Pilot \n     \n     \n     \n     Proj 1: Pilot \n  \n  \n    Week 11  4.4.23 \n    2 means +t-proced \n    • dist of X-bar_1 - X-bar_2  • SE(X-bar_1 - X-bar_2)  • t-test of μ_1 - μ_2  • t-CI for μ_1 - μ_2 \n     2 means    2 samples   IMS 20 \n     clinical trials    Omayra Ortega \n     \n  \n  \n    4.4.23 \n    Proj 2: Power Analysis \n     \n     \n     \n     Proj 2: Power Analysis \n  \n  \n    Week 12  4.11.23 \n    ANOVA \n    • MSG  • MSE • F-test \n     ANOVA    Inf on β_1    slopes   IMS 22   IMS 24 \n     p-hacking  \n     \n  \n  \n    Week 13  4.18.23 \n    Inference β_1 + Tech cond + MLR \n    •  SE(β_1)  • residual plots  • multiple X \n     Tech Cond    MLR   IMS 25 \n     1918 flu  \n     \n  \n  \n    4.18.23 \n    Proj 3: Analysis \n     \n     \n     \n     Proj 3: Preliminary Analysis \n  \n  \n    Week 14  4.25.23 \n    catch-up \n     \n     study materials  (see Canvas)   when to use what \n     \n     \n  \n  \n    4.27.23 \n    Exam 2 \n     \n     \n     \n     when to use what \n  \n  \n    5.2.23 \n    Proj 4: Full draft \n     \n     \n     \n     Proj 4: Full draft \n  \n  \n    5.4 + 12.23 \n    Proj Presentation \n     \n     \n     \n     Proj Presentation \n  \n  \n    5.12.23 \n    Proj 5: Final Project \n     \n     \n     \n     Proj 5: Final Project \n  \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Semester Project",
    "section": "",
    "text": "Artwork by @allison_horst."
  },
  {
    "objectID": "project.html#pilot",
    "href": "project.html#pilot",
    "title": "Semester Project",
    "section": "Pilot",
    "text": "Pilot\nThe pilot project will be based on a preliminary data collection. Details about the structure of the data are given in the assignment, note that the response variable must be numerical. You will have an opportunity to collect more data as we go, but the better the original data are, the better the follow-up data collection will be. The goal of the pilot study assignment is to develop a preliminary research question and to collect some initial data, both in preparation for your group Island project."
  },
  {
    "objectID": "project.html#power",
    "href": "project.html#power",
    "title": "Semester Project",
    "section": "Power",
    "text": "Power\nThe power analysis will use the pilot data to estimate the sample size which will be needed to see the effect of interest. Most of the code will be written by the professor, but you will need to understand both why the power analysis is important and how the power analysis is implemented. The goal of the power analysis assignment is to estimate how many observations are needed for an Islands project to reach significance."
  },
  {
    "objectID": "project.html#preliminary-analysis",
    "href": "project.html#preliminary-analysis",
    "title": "Semester Project",
    "section": "Preliminary Analysis",
    "text": "Preliminary Analysis\nThe preliminary analysis will contain numerical and graphical summaries. Additionally, one regression model should be run. The goal of the preliminary data analysis assignment is to start working with the variables including visualizations, numerical summaries, and modeling."
  },
  {
    "objectID": "project.html#full-draft",
    "href": "project.html#full-draft",
    "title": "Semester Project",
    "section": "Full Draft",
    "text": "Full Draft\nThe complete draft of the project will contain a full set of analyses designed to answer the research question (and practice with multiple regression). The paper will continue to have sections which communicate the study background and methods to the reader. All plots and tables should be well labelled. The goal of the assignment is to provide a complete analysis of the data collected on the Islands as a research project."
  },
  {
    "objectID": "project.html#presentation",
    "href": "project.html#presentation",
    "title": "Semester Project",
    "section": "Presentation",
    "text": "Presentation\nThe presentation will bring together your work and showcase your ideas. You will have 5 minutes to report on your study, using slides (visualizations from your analyses). Each group member will be expected to talk. The goal of this assignment is to synthesize your study and your results into a clear and succinct presentation to your peers.\nYour group may choose whether to present during reading days (May 6, 2022) or during the time allocated to our course for the final exam (May 13, 2022)."
  },
  {
    "objectID": "project.html#final-project",
    "href": "project.html#final-project",
    "title": "Semester Project",
    "section": "Final Project",
    "text": "Final Project\nThe final project will compile all of the project pieces together into a beautiful and clear report. The paper will have sections which communicate the study background and methods to the reader as well a conclusions of the experiment. All plots and tables should be well labelled. The goal of the assignment is to provide a complete analysis of the data collected on the Islands as a research project.\nAlong with the final project report, each student will fill out an individual reflection piece and a group dynamic report."
  },
  {
    "objectID": "roles.html",
    "href": "roles.html",
    "title": "Group Dynamic Roles",
    "section": "",
    "text": "Artwork by @allison_horst."
  },
  {
    "objectID": "roles.html#roles-for-the-projects",
    "href": "roles.html#roles-for-the-projects",
    "title": "Group Dynamic Roles",
    "section": "Roles for the Projects",
    "text": "Roles for the Projects\nEach individual will have (at least) 2 roles; one role to help foster the group dynamics and the other role to divvy up the responsibilities involved in completing the project.\n\nGroup Dynamic Roles:\n\nProject Manager: Makes sure that the group is organized and has a clear plan for completing the project. This includes scheduling meetings and having a plan for what needs to be done before the next meeting.\n\nTask Manager: Makes sure that everyone knows what they are expected to do before the next meeting and makes sure that they do it. This might involve calling or emailing each person between meetings to discuss what they have/haven’t done. If someone can’t do the work that needs to be done, the task manager is responsible for calling another meeting if needed.\n\nFacilitator: Makes sure that every member of the group is participating and being listened to and heard. This might involve asking questions of a member that’s been silent and stopping others when someone’s comment is being overlooked.\n\n\n\nProject Roles\nIn each case, the person assuming that role is responsible for that aspect of the project. It doesn’t mean that they will do all that part of the project by themselves; it means that they are responsible for dividing that work up among the members of the group and ensuring that it is done and recorded correctly.\n\nDirector of Research: Is responsible for the literature searches. The Director of Research identifies what needs to be searched for in the literature, divvies up the literature searches to be performed among the group members, and coordinates changes in the searches based on information gathered and changes in direction. They are also responsible for making sure that the citations in the project are complete and accurate.\n\nDirector of Computation: Is responsible for the computer programs involved in the project. The Director of Computation is responsible for designing the code so that different people can write different parts of the code. The programmer is responsible for making sure that any code written by different people can be integrated.\n\nReporter: Is responsible for the written report. This involves taking notes during the complete process in order to keep a record of what has been done. The reporter may also gather everyone’s individual notes and put them together. The reporter is also responsible for editing the final report and making sure that the various pieces (that may have been written by different people) fit well together.\n\n\n\nThings to think about:\n\nIf all members of a group think that one member isn’t pulling their weight then you can come and talk to me about it. I can fire that member and have them do the project on their own.\n\n(Generally) All members of the group get the same grade for the project.\n\nGrade on projects (a rubric for grading will be posted soon, but generally the grade will be based on the following):\n\nGrade for technical depth and sophistication.\n\nGrade for quality of write-up (organization, clarity, grammatical correctness, appropriate use of graphs, tables, formulas).\n\nGrade for quality of oral presentation (organization, clarity, appropriate use of graphs, tables formula, ability to answer questions).\n\nGrade for quality of group work and distribution of labor.\n\n\nAttendance: you should keep track of who is or isn’t showing up to group project meetings\n\nStudents should keep a record of all the times that they worked on the project and the work that they did. Every time that you spend more than 15 minutes on the project you should write down the start and stop time and what you did. Group members should be spending roughly the same amount of time on the project (I will ask you to report on time spent on the project so far at the check-in in November.)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "syllabus",
    "section": "",
    "text": "Class: Tuesdays & Thursdays, 9:35 - 10:50 am\nLab: Fridays, 11-11:50am\nJo Hardin\n2351 Estella\njo.hardin@pomona.edu\n\n\nMonday 1:30-3pm\nTuesday 2:30-3:30pm\nWednesday 9-11am\nThursday 3-4pm\nor by appointment\n\n\nMentors: Sara Colando, Anu Krishnan, Naomi Meurice, and Kyle Torres\nMonday 8-10pm\nWednesday 6-8pm\nEstella 2131\n\n\n\n\n\nArtwork by @allison_horst.\n\n\n\n\n\n\n\n\n\nIntroduction to Biostatistics is an introduction to statistical ideas using R. We will cover the majority of statistical methods which are used in standard analyses (e.g., t-tests, chi-squared analysis, regression, confidence intervals, binomial tests, etc.). The main inferential techniques will be covered using both theoretical approximations (e.g., the central limit theorem) as well as computational methods (e.g., randomization tests and bootstrapping). Focus will be on understanding the methods and interpreting results.\n\n\n\n\n\n\nAnonymous Feedback\n\n\n\nAs someone who is, myself, constantly learning and growing in many ways, I welcome your feedback about the course, the classroom dynamics, or anything else you’d like me to know. There is a link to an anonymous feedback form on the landing page of our Canvas webpage. Please provide me with feedback at any time!\n\n\n\n\n\nBy the end of the semester, students will be able to do the following:\n\nGiven a study, identify population, sample, parameter, statistic, observational unit, and variable.\nDescribe the differences between, benefits of each, and conclusions which can be drawn in observational studies versus experiments.\nGiven a dataset and research query, create an appropriate figure in R.\nGiven a dataset and research query, compute appropriate statistics in R.\nDescribe the difference between the distribution of a sample of data and a sampling distribution of a particular statistic.\nFor a particular research question, identify whether the task requires descriptive analysis / model, graphic, confidence interval, or hypothesis test,\nApply the empirical rule to as an approximation to confidence intervals and hypothesis testing in settings of means and proportions.\nBe able to describe in words what a p-value is and what it is not.\nWrite down appropriate null and alternative hypotheses, and choose the correct analysis technique.\nRun the hypothesis test / confidence interval analysis in R.\nIdentify when it is and when it is not appropriate to summarize the relationship between two variables using a least squares line. Describe the optimization procedure the leads to a least squares fit (although not necessarily to do the calculations).\nProvide the settings in which a causal claim is warranted, and when a strong correlation is possibly due to spurious relationships.\nUse a regression line to make predictions and distinguish between a prediction interval for an independent response as compared to a confidence interval for the slope parameter.\nFor each descriptive analysis, visualization, confidence interval, or hypothesis test, in words communicate the conclusion of the analysis in the original context of the data.\nUse R Markdown to run reproducible analyses that include all aspects of the data analysis.\n\n\n\n\nIn an ideal world, science would be objective. However, much of science is subjective and is historically built on a small subset of privileged voices. In this class, we will make an effort to recognize how science (and statistics!) has played a role in both understanding diversity as well as in promoting systems of power and privilege. I acknowledge that there may be both overt and covert biases in the material due to the lens with which it was written, even though the material is primarily of a scientific nature. Integrating a diverse set of experiences is important for a more comprehensive understanding of science. I would like to discuss issues of diversity in statistics as part of the course from time to time.\nPlease contact me if you have any suggestions to improve the quality of the course materials.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official records, please let me know!\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. You can also relay information to me via your mentors. I want to be a resource for you. If you prefer to speak with someone outside of the course, the math liaisons, Dean of Students, or QSC staff are all excellent resources.\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. As a participant in course discussions, you should also strive to honor the diversity of your classmates.\n\n\n\n\n\n\nIMS: Introduction to Modern Statistics by Çetinkaya-Rundel and Hardin.\nISCAM: Investigating Statistics, Concepts, Applications, and Methods by Chance and Rossman. Many of the examples come from ISCAM, a pdf of which can be purchased for $5.\n\nISCAM applets\nStatKey (boostrapping) applets\n\n\n\n\n\n\n\n\nImportant dates\n\n\n\n\n3/9/23 (Thursday) Exam 1\n3/28/23 (Tuesday) Project 1: Pilot\n4/4/23 (Tuesday) Project 2: Power\n4/18/23 (Tuesday) Project 3: Analysis\n4/27/23 (Thursday) Exam 2\n5/2/23 (Tuesday) Project 4: Full draft\n5/4 or 12/23 (Friday) Project presentation\n5/12/23 (Friday) Project 5: Final (edited) Project\n\n\n\n\n\n\n\nEnough R\nR tutorial\nGreat tutorials through the Coding Club\nA true beginner’s introduction to the tidyverse, the introverse.\nfor a good start to R in general\nA fantastic ggplot2 tutorial\nGreat tutorials through the Coding Club\nGoogle for R\nsome R ideas that I wrote up\nIncredibly helpful cheatsheets from RStudio.\n\ndata wrangling\nggplot2\nR Markdown\nRStudio IDE\n\n\n\n\n\nR will be used for all homework assignments. You can use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to get Pomona login information.)\nAlternatively, feel free to download R onto your own computer. R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, you are required to install RStudio and turn in all R assignments using RMarkdown. http://rstudio.org/. (You can use the LaTeX compiler at: https://yihui.name/tinytex/)\n\n\n\nThis course uses Canvas as the main learning management system. The Canvas login is http://canvas.pomona.edu/. If you haven’t used Canvas before, I recommend bookmarking Canvas Student Guides and Canvas Student Videos for easy reference to tips and tutorials. If you run into an issue with Canvas, help is available.\n\nFrom anywhere in Canvas, select the Help button, located in the blue Global Navigation menu on the left.\n\nClick on Pomona Service Desk - Canvas Support to report a problem by submitting a service request ticket. Be sure to include “Canvas Issue” in your subject line.\nFor additional assistance, you can click on Ask Your Instructor or simply send me an email.\n\n\nPlease be proactive and reach out for help as soon as possible to resolve the issue you are experiencing.\n\n\n\n\n\n\nThere are no statistics prerequisites for this class; nor is there an R prerequisite. Instead, the prerequisite is that you should feel comfortable with mathematical notation and computational thinking. Although we do not actually do any calculus, having had at least a semester of calculus indicates that you are at the right level mathematically.\n\n\n\nHomework will be assigned from the text and due every Wednesday at 11:59pm. One homework grade will be automatically dropped, so there are no late assignments. Homework will be turned in via Gradescope on Canvas.\n\n\nHomework assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n[5] All problems completed with detailed solutions provided and 75% or more of the problems are fully correct. Additionally, there are no extraneous messages, warnings, or printed lists of numbers.\n[4] All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. Or all problems are completed and there are extraneous messages, warnings, or printed lists of numbers.\n[3] Close to all problems completed with less than 75% correct.\n[2] More than half but fewer than all problems completed and > 75% correct.\n[1] More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed.\n[0] No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\n\n\nThere will be a semester long group project. Your task is to use data to tell us something interesting. This project is deliberately open-ended to allow you to fully explore your creativity. There are three main rules that must be followed: (1) data centered, (2) tell us something, (3) do something new. The project information is available here: Math 58B Semester Project\n\n\n\n\nThis class will be interactive, and your participation is expected (every day in class). Although notes will be posted, your participation is an integral part of the in-class learning process.\nIn class: after answering one question, wait until 5 other people have spoken before answering another question. [Feel free to ask as many questions as often as you like!]\n\n\n\n\nThroughout the semester, you will be challenged, and you may find yourself stuck. Every single one of us has been there, I promise. Below, I’ve provided Pomona’s academic honesty policy. But before the policy, I’ve given some thoughts on cheating which I have taken from Nick Ball’s CHEM 147 Collective (thank you, Prof Ball!). Prof Ball gives us all something to think about when we are learning in a classroom as well as on our journey to become scientists and professionals:\n\n\n\n\n\n\nWhy Cheat?\n\n\n\nThere are many known reasons why we may feel the need to “cheat” on problem sets or exams:\n\nAn academic environment that values grades above learning.\nFinancial aid is critical for remaining in school that places undue pressure on maintaining a high GPA.\nNavigating school, work, and/or family obligations that have diverted focus from class.\nChallenges balancing coursework and mental health.\nBalancing academic, family, peer, or personal issues.\n\nBeing accused of cheating – whether it has occurred or not – can be devastating for students. The college requires me to respond to potential academic dishonesty with a process that is very long and damaging. As your instructor, I care about you and want to offer alternatives to prevent us from having to go through this process.\n\n\nIf you find yourself in a situation where “cheating” seems like the only option, please come talk to me. We will figure this out together.\nPomona College is an academic community, all of whose members are expected to abide by ethical standards both in their conduct and in their exercise of responsibilities toward other members of the community. The college expects students to understand and adhere to basic standards of honesty and academic integrity. These standards include, but are not limited to, the following:\n\nIn projects and assignments prepared independently, students never represent the ideas or the language of others as their own.\nStudents do not destroy or alter either the work of other students or the educational resources and materials of the College.\nStudents neither give nor receive assistance in examinations.\nStudents do not take unfair advantage of fellow students by representing work completed for one course as original work for another or by deliberately disregarding course rules and regulations.\nIn laboratory or research projects involving the collection of data, students accurately report data observed and do not alter these data for any reason.\n\n\n\n\nPlease email and / or set up a time to talk if you have any questions about or difficulty with the material, the computing, or the course. Talk to me as soon as possible if you find yourself struggling. The material will build on itself, so it will be much easier to catch up if the concepts get clarified earlier rather than later. This semester is going to be fun. Let’s do it.\n\n\n\n\n\n\nGrading\n\n\n\n\n20% Homework\n50% Midterms\n25% Group Project & Presentation\n5% Class Participation"
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Rubric for Semester Project",
    "section": "",
    "text": "The grading rubric tries to lay out the main aspects of the project inluding the analysis parts and the communication parts. Please ask if you have any questions.\n\n\n\n\nThe introduction should include:\n\nBrief background (Has an explanation been given for why the research was done? Why is the work important? What is its relevance?)\nObjective or Research question (Is the brief description of the hypothesis/goals and findings of the paper clearly stated for the reader?)\n\n\n\n\nMethods should include description of:\n\nStudy design (enough info for someone to replicate)\nSampling method and eligibility criteria\nTreatment (i.e., explanatory variable)\nOutcome definition(s) (i.e., response variable)\nSample size justification\nStatistical methods which were applied\n\n\n\n\nResults should communicate / describe / summarize:\n\nBaseline data (including numbers analyzed – explain the explanatory variables, summarize the explanatory variables, plot the explanatory variables, etc.)\nOutcomes (explain the response variable, summarize the response variable, plot the response variable). Is your treatment liable to do harm? Are you worried about type I vs type II errors when considering the outcome?\nEach model is completely described\nEach figure / table is completely described\nConnections between analyses; e.g., between figures and model results\nDiscussion\nConclusion (i.e., answer to the research question), interpretation\nLimitations\nIdeas for future studies\n\n\n\n\nFull analysis should be clear, concise, well-organized, reproducible, and engaging:\n\nDataset is complete, available, tidy (rows are observations, columns are variables, no exceptions)\nAll information was provided\nPresentation was organized\nPresentation was clear, understandable\nTables and figures were used well (axes labeled, etc.)\nWork is completely reproducible (the instructor must be able to run the .Rmd file on their own computer)\n\n\n\n\n\nEach of you will evaluate your own work and that of your peers keeping the following topics in mind.\n\nMastery of the material (understanding of the material is strong and well communicated)\nBackground, completeness (thoroughly explains all key points)\nPresentation evaluation (organized, communicates effectively with the audience)"
  },
  {
    "objectID": "clicker_study.html",
    "href": "clicker_study.html",
    "title": "Introduction to Biostatistics",
    "section": "",
    "text": "Clicker Q\nto go with Introduction to Modern Statistics by Çentinkaya-Rundel & Hardin. Math 58B - Introduction to Biostatistics.\n\n\n\nIf 16 infants with no genuine preference choose 16 toys, what is the most likely number of “helping” toys that will be chosen?1\n\n4\n7\n8\n9\n10\n\n\n\n\nHow likely is it that exactly 8 helpers will be chosen (if there is no preference)?2\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nWhat if we flipped a coin 160 times? What percent of the time will the simulation flip exactly 80 heads?3\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nIs our actual result of 14 (under the coin model)…4\n\nvery surprising?\nsomewhat surprising?\nnot very surprising?\n\n\n\n\nBased on the first handwriting study, can we conclude that cursive causes higher scores (on average)?5\n\nYes\nNo\nIt depends\n\n\n\n\nBased on the second handwriting study, can we conclude that cursive causes higher scores (on average)?6\n\nYes\nNo\nIt depends\n\n\n\n\nA possible confounding variable for the handwriting study is:7\n\ngrade of the student (age)\nregion of country where the SAT was taken\nacademic ability of the student\ngender identity of the student\nnumber of siblings of the student.\n\n\n\n\nThe main reason we randomly assign the explanatory variable is:8\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nThe main reason we take random samples from the population is:9\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the expected causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nAre there effects of second-hand smoke on the health of children?10\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people tend to spend more money in stores located next to food outlets with pleasing smells?11\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDoes cell phone use increase the rate of automobile accidents?12\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people consume different amounts of ice cream depending on the size of bowl used?13\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nWhich is more effective: diet A or diet B?14\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored ten points higher on the final than on the midterm:15\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored five points lower on the final than on the midterm:16\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored twice as many points on the final than on the midterm:17\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose you guessed every value correctly (guess the correlation applet), what would be the value of the correlation coefficient between your guesses and the actual correlations?18\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose each of your guesses was too high by 0.2 from the actual value of the correlation coefficient, what would be the value of the correlation coefficient between your guesses and the actual correlations?19\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nA correlation coefficient equal to 1 indicates that you are a good guesser.20\n\nTRUE\nFALSE\n\n\n\n\nPerfect Correlation… if not for a single outlier\nn = 101 observations: 1 observation in top left, 25 observations in each in of the points near the bottom right.\nThe value of the correlation, r, is:21\n\n-1 < r < -0.9\n-0.9 < r < -0.5\n-0.5 < r < 0.5\n0.5 < r < 0.9\n0.9 < r < 1\n\n\n\n\n\n\n\n\n\n\nThe sum of residuals from the sample mean (no X):22 \\[\\sum_{i=1}^n(Y_i - \\overline{Y})\\]\n\nis positive\nis negative\nis zero\nis different for every dataset\n\n\n\n\nA good measure of how well the prediction (of the sample mean) fits the data is:23\n\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})\\)\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\overline{Y}|\\)\n\\(\\mbox{median}(Y_i - \\overline{Y})\\)\n\\(\\mbox{median}|Y_i - \\overline{Y}|\\)\n\n\n\n\nA good measure of how well the prediction (of the regression line) fits the data is:24\n\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)\\)\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\hat{Y}_i|\\)\n\\(\\mbox{median}(Y_i -\\hat{Y}_i)\\)\n\\(\\mbox{median}|Y_i -\\hat{Y}_i|\\)\n\n\n\n\nWhat math is used to find the value of \\(m\\) that minimizes:25 \\[\\sum_{i=1}^n(Y_i - m)^2\\]\n\ncombinatorics\nderivative\nintegral\nlinear algebra\n\n\n\n\n\\(\\sum_i(Y_i - \\overline{Y})^2\\) is sometimes \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)26\n\nTRUE\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is never \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?27\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\n“Observed data or more extreme” is:28\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?29\n\n0\n9\n5.3\n11\n15\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?30\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?31\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?32\n\nSubstantially greater than 50%\nSubstantially less than 50%\nVery close to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions / learning community check-ins, but I didn’t go this week.\nI can’t make the scheduled office hours / mentor sessions / learning community check-ins\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nIf communication medium and cheating are independent variables, how many of the email senders (out of 26) would you expect to cheat?33\n\n10 (ish)\n13 (ish)\n16 (ish)\n20 (ish)\n24 (ish)\n\n\n\n\nWhen looking at the null differences, is the observed result of 28.7%:34\n\nVery surprising\nSomewhat surprising\nNot very surprising\n\n\n\n\nHypothesis: the number of hours that grade-school children spend doing homework predicts their future success on standardized tests.35\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: king cheetahs on average run the same speed as standard spotted cheetahs.36\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: the mean length of African elephant tusks has changed over the last 100 years.37\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: the risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not.38\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: caffeine intake during pregnancy affects mean birth weight.39\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nIn this class, the word parameter means:40\n\nThe values in a model\nNumbers that need to be tuned\nA number which is calculated from a sample of data.\nA number which (is almost always unknown and) describes a population.\n\n\n\n\nI know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW/Lab solutions41\n\nTRUE\nFALSE\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times (to get 1000 bootstrap resamples). What is the sample size of each bootstrap resample?42\n\n50\n1000\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times (to get 1000 bootstrap resamples). How many bootstrap statistics will you have?43\n\n50\n1000\n\n\n\n\nLet’s say you take a random sample and compute \\(\\hat{p}=0.3.\\) After bootstrapping, you see that the bootstrapped resamples produce almost all the \\(\\hat{p}_{boot}\\) within plus or minus 0.01 of your original statistic. It seems that the parameter \\(p\\) is probably:44\n\n0.3\nbetween (0.2, 0.4)\nbetween (0.29, 0.31)\nbetween (0.28, 0.32)\nhuh? how can we get \\(p\\) from \\(\\hat{p}?\\)\n\n\n\n\nIn a second analysis, I create a 90% CI for the true proportion \\(p.\\) What is the impact (of switching from 95% to 90%) on the CI?45\n\nnarrower\nless likely (long-run) to capture the parameter\nneither\nboth\n\n\n\n\nIn a second study, I set out to obtain twice as much data (as in the first study) in order to create a 95% CI for the true proportion \\(p.\\) What is the impact (of the larger sample) on the CI?46\n\nnarrower\nmore likely (long-run) to capture the parameter\nneither\nboth\n\n\n\n\nWhat is one main reason to use bootstrapping to find a confidence interval?47\n\nlarger coverage probabilities\nnarrower intervals\nmore resistant to outliers\ncan be done for any statistic\n\n\n\n\n95% CI for the true median mercury:48\n\n(0.025 mg/kg, 0.975 mg/kg)\n(0.469 mg/kg, 0.053 mg/kg)\n(0.053 mg/kg, 0.469 mg/kg)\n(0.34 mg/kg, 0.56 mg/kg)\n\n\n\n\n\n\n\n\nFrom StatKey applet: https://www.lock5stat.com/StatKey/\n\n\n\n\n\n\nWhat are the observational units for your individual candy study?49\n\nColor of the candy\nPiece of candy\nCup of candy\nThe Hershey Company\nProportion that are orange\n\n\n\n\nWhat are the observational units for the class compilation (dotplot)?50\n\nColor of the candy\nPiece of candy\nCup of candy\nThe Hershey Company\nProportion that are orange\n\n\n\n\nHow does the sampling distribution for the sample proportion change as n changes (for a fixed p)?51\n\nThe spread changes\nThe symmetry changes\nThe center changes\nThe shape changes\n\n\n\n\nHow does the sampling distribution change as p changes (for a fixed n)?52\n\nThe spread changes\nThe symmetry changes\nThe center changes\nThe shape changes\n\n\n\n\nThe Central Limit Theorem says that the distribution of \\(\\hat{p}\\) will be approximately normal with what center:53\n\n\\(\\hat{p}\\)\n\\(p\\)\n0.5\n1\n\\(\\sqrt{p(1-p) / n}\\)\n\n\n\n\nWould you rather have an extra 20 points on the SAT or an extra 10 points on the ACT?54\n\n+20 on the SAT\n+10 on the ACT\n\n\n\n\nThe standardized score (z-score) counts:55\n\nthe number of standard deviations from the mean\nthe number of standard deviations above the mean\nthe number of standard deviations below the mean\nthe distance from the mean\nthe distance from the standard deviation\n\n\n\n\nIf the normal distribution is a good model, we would expect the large majority of our z scores to be:56\n\nwithin \\(\\pm\\) 1 of the mean\nwithin \\(\\pm\\) 2 of the mean\nwithin \\(\\pm\\) 1\nwithin \\(\\pm\\) 2\n\n\n\n\nWith your cup of candy, you personally got a Z score of:57\n\nbetween (-1, 1) (not including 1)\nbetween (-2, -1] or [1, 2)\nbetween (-3, -2] or [2, 3)\n-3 or smaller or 3 or above\n\n\n\n\nAssume n = 100 and p= 0.8 (note: \\(\\sqrt{(0.8 \\cdot 0.2)/100} = 0.4/10 = 0.04\\))\nWhat is the largest reasonable distance between \\(\\hat{p}\\) and \\(p\\)?\nThat is, we would expect \\(\\hat{p}\\) and \\(p\\) to be no more than _____ apart58\n\n0.04\n0.08\n0.12\n0.16\n0.24\n\n\n\n\nAssume n = 100 and p= 0.8 (note: \\(\\sqrt{(0.8 \\cdot 0.2)/100} = 0.4/10 = 0.04\\)) Which statement is true?59\n\n95% of \\(\\hat{p}\\) are between (0.76, 0.84)\n95% of \\(\\hat{p}\\) are between (0.72, 0.88)\n95% of \\(\\hat{p}\\) are between (0.68, 0.92)\n95% of \\(p\\) are between (0.76, 0.84)\n95% of \\(p\\) are between (0.72, 0.88)\n\n\n\n\nIf you want a 90% confidence interval for p, your z* multiplier should be60\n\nless than 1\nless than 2 (but greater than 1)\nequal to 2\ngreater than 2 (but less than 3)\ngreater than 3\n\n\n\n\nWhat is the difference between Z* and a Z score?61\n\nZ score comes from the data, Z* and is a pre-defined unit of measurement.\nZ* comes from the data, and Z score is a pre-defined unit of measurement\nZ score assumes the null hypothesis is true and Z* doesn’t.\nZ* assumes the null hypothesis is true, and Z score doesn’t\n\n\n\n\nLet’s say we are making confidence intervals (not doing a hypothesis test), what is your best guess for \\(SE(\\hat{p})\\)?62\n\n\\(\\sqrt{0.5 \\cdot (1 - 0.5) / n}\\)\n\\(\\sqrt{p \\cdot (1 - p) / n}\\)\n\\(\\sqrt{\\hat{p} \\cdot (1 - \\hat{p}) / n}\\)\n\\(\\sqrt{X \\cdot (1 - X) / n}\\)\n\\(\\sqrt{0.95 \\cdot (1 - 0.95) / n}\\)\n\n\n\n\nThe following is a correct interpretation of the CI:63\n\n\n95% confident that the interval includes the sample proportion who believe that the global poverty rate has doubled.\n\n\nTRUE\nFALSE\n\n\n\nThe following is a correct interpretation of the CI:64\n\n\nIf researchers were to select a new sample of 1005 adult Americans, then we’re 95% confident that between 56% and 62% of those people would answer “doubled” to the question.\n\n\nTRUE\nFALSE\n\n\n\nLet’s say that the null hypothesis (e.g., p=0.47) is TRUE. My level of significance is 0.03 (reject if p-value < 0.03). How often will I reject the null hypothesis?65\n\n1 % of the time\n3% of the time\n5 % of the time\n95% of the time\n97% of the time\n\n\n\nWhat does “of the time” mean???\nIt means in repeated samples. That is, in 3% of all datasets we’d take from that exact same population, we would mistakenly reject the actually true hypothesis that p=0.47.\n\n\nLet’s say that the null hypothesis (e.g., p=0.47) is TRUE. My level of significance is 0.03.\nHow often will p be in a 97% confidence interval?66\n\n1 % of the time\n3% of the time\n5 % of the time\n95% of the time\n97% of the time\n\n\n\nWhat does “of the time” mean???\nIt means in repeated samples. That is, in 97% of all datasets we’d take from that exact same population, we would capture the true population proportion of 0.47.\n\n\nSuppose the sample is 10 times larger. The SE of the statistic:67\n\nincreases\nstays the same\ndecrease\n\n\n\n\nSuppose the population is 10 times larger. The SE of the statistic:68\n\nincreases\nstays the same\ndecrease\n\n\n\n\nSuppose the sample is 10 times larger. The variability of the data:69\n\nincreases\nstays the same\ndecrease\n\n\n\n\nHow many hits out of 20 at bats would make you believe him?70\n\n5\n6\n7\n8\n9\n\n\n\n\nType I error is71\n\nWe give him a raise when he deserves it.\nWe don’t give him a raise when he deserves it.\nWe give him a raise when he doesn’t deserve it.\nWe don’t give him a raise when he doesn’t deserve it.\n\n\n\n\nType II error is72\n\nWe give him a raise when he deserves it.\nWe don’t give him a raise when he deserves it.\nWe give him a raise when he doesn’t deserve it.\nWe don’t give him a raise when he doesn’t deserve it.\n\n\n\n\nPower is the probability that:73\n\nWe give him a raise when he deserves it.\nWe don’t give him a raise when he deserves it.\nWe give him a raise when he doesn’t deserve it.\nWe don’t give him a raise when he doesn’t deserve it.\n\n\n\n\nThe player is more worried about74\n\nA type I error\nA type II error\n\n\n\n\nThe manager is more worried about75\n\nA type I error\nA type II error\n\n\n\n\nIncreasing your sample size76\n\nIncreases your power\nDecreases your power\n\n\n\n\nMaking your significance level more stringent (\\(\\alpha\\) smaller)77\n\nIncreases your power\nDecreases your power\n\n\n\n\nA more extreme alternative78:\n\nIncreases your power\nDecreases your power\n\n\n\n\nIs the Alien’s interval for the true proportion of all humans who self-identify as female consistent with your lived experience?79\n\nYes\nNo\nI don’t understand what the confidence interval represents.\n\n\n\n\nAs we’ve seen with the applet, about 5% of all 95% intervals fail to capture the actual value of the population parameter. Do you think the alien just got a “red” interval?80\n\nYes\nNo\n\n\n\n\nWould it be reasonable for the alien to conclude, with 95% confidence, that between 16.5% and 33.5% of US Senators in the year 2023 self-identify as female?81\n\nYes\nNo\n\n\n\n\nThe “random” part in clinical trials typically comes from:82\n\nrandom samples\nrandom allocation of treatment\n\n\n\n\nThe “random” part in polling typically comes from:83\n\nrandom samples\nrandom allocation of treatment\n\n\n\n\nYou want to collect data to investigate whether teenagers in the United States have read fewer Harry Potter books than teenagers in the United Kingdom. Would you make use of random sampling, random assignment, both, or neither?84\n\nRandom sampling\nRandom assignment\nBoth\nNeither\n\n\n\n\nAn instructor wants to investigate whether using a red pen to grade assignments leads to lower scores on exams than using a blue pen to grade assignments. Would you advise the professor to make use of random sampling, random assignment, both, or neither?85\n\nRandom sampling\nRandom assignment\nBoth\nNeither\n\n\n\n\nA student decides to investigate whether NFL football games played in indoor stadiums tend to have more points scored than games played outdoors. The student examines points scored in every NFL game of the 2022 season. Has the student used random sampling, random assignment, both, or neither?86\n\nRandom sampling\nRandom assignment\nBoth\nNeither\n\n\n\n\nRelative Risk is87\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nIn order to find a CI for the true RR, our steps are:88\nStep 1. ln(RR-hat)\nStep 2. add ± z* sqrt( 1/A - 1/(A+C) + 1/B - 1/(B+D) )\nStep 3. find exp of the endpoints\n\nbecause the sampling distribution of RR is normal\nbecause RR is typically greater than 1\nbecause the ln transformation makes the sampling distribution almost normal\nbecause RR is invariant to the choice of explanatory or response variable\n\n\n\n\nIn finding a CI for \\(p_1\\)/\\(p_2\\), why is it okay to exponentiate the end points of the interval for ln(\\(p_1\\)/\\(p_2\\))?89\n\nif ln(\\(p_1\\)/\\(p_2\\)) is in the natural log-interval, \\(p_1\\)/\\(p_2\\) will be in the exponentiated interval.\nthe natural log of the RR makes the distribution approximately normal.\nthe natural log compresses values that are > 1 and spreads values < 1.\n\n\n\n\nUsually, the CI for \\(p_1\\)/\\(p_2\\) is considered to be “significant” if90\n\n\\(p_1\\)/\\(p_2\\) is not in the interval\n\\(\\hat{p}_1 / \\hat{p}_2\\) is not in the interval\n0 is not in the interval\n1 is not in the interval\n\n\n\n\nIn order to find a CI for the true OR, our steps are:91\nStep 1. ln(OR-hat)\nStep 2. add ± z* sqrt( 1/A + 1/B + 1/C + 1/D )\nStep 3. find exp of the endpoints\n\nbecause the sampling distribution of OR is normal\nbecause OR is typically greater than 1\nbecause the ln transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA.\nWhat is measurable?92\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?93\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA.\nWhat is measurable?94\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nWhen we randomly select individuals based on the explanatory variable, we cannot accurately measure95\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nThe odds ratio is invariant to which variable is explanatory and which is response means:96\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nOne reason we should be careful interpreting relative risks is if:97\n\nwe don’t know the difference in proportions\nwe don’t know the SE of the relative risk\nwe might be dividing by zero\nwe don’t know the baseline risk\n\n\n\n\nIf the null hypothesis is true, the observed counts will equal the expected counts.98\n\nTrue\nFalse\n\n\n\n\nTo reject the null hypothesis we want to see99\n\na small \\(X^2\\) value\na big \\(X^2\\) value\n\n\n\n\nA chi-square test has a100\n\none-sided alt hypothesis, and we only consider the upper end of the sampling distribution\none-sided alt hypothesis, and we consider both ends of the sampling distribution\ntwo-sided alt hypothesis, and we only consider the upper end of the sampling distribution\ntwo-sided alt hypothesis, and we consider both ends of the sampling distribution\n\n\n\n\nFor the lighting study, which variable is the explanatory variable?101\n\nsleeping light\neye sightedness\nchild\nparent\n\n\n\n\nIf we sample randomly from a population, the conclusions we can make are about:102\n\ncausation\npopulation characteristics\n\n\n\n\nBased on the night light / myopia example, the correct conclusion is:103\n\nthe p-value is small, so sleeping in a lit room makes it more likely that you are near-sighted.\nthe p-value is small, so sleeping in a dark room makes it more likely that you are near-sighted.\nthe p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms.\n\\(\\hat{p}_{\\mbox{near}}\\) if lit room = 41/75 = 0.547 and \\(\\hat{p}_{\\mbox{near}}\\) if dark = 18/172 = 0.105, therefore sleeping with the light on is bad for you.\n\n\n\n\nA possible confounding variable for the night light study is:104\n\nlow birth weight\nrace (70% of the children were white)\nregion of the country where the clinic was located\n\n\n\n\nWhich dataset has the smallest standard deviation?105\n\nA: left\nB: center\nC: right\n\n\n\n\n\n\n\n\n\nWhich of the two dotplots displays the dataset with the smaller IQR?106\n\nA\nB\n\n\n\n\n\n\n\n\n\nThe standard deviation of weights (mean = 167 lbs) is approximately107\n\n1\n5\n10\n35\n100\n\n\n\n\nThe standard deviation of average weights (mean = 167 lbs) in repeated samples of size 10 is approximately108\n\n1\n5\n10\n35\n100\n\n\n\n\nThe standard deviation of average weights (mean = 167 lbs) in repeated samples of size 50 is approximately109\n\n1\n5\n10\n35\n100\n\n\n\n\nThe standard deviation of average weights (mean = 167 lbs) in repeated samples of size 1000 is approximately110\n\n1\n5\n10\n35\n100\n\n\n\nQ: what is the most confusing part of understanding the difference between the variability of the weights and the variability of the average of the weights?\n\n\nThe sampling distribution of the mean will be111\n\ncentered below the data distribution\ncentered at the same place as the data distribution\ncentered above the data distribution\nunrelated to the center of the data distribution\n\n\n\n\nThe sampling distribution of the mean will be112\n\nless variable than the data distribution\nthe same variability as the data distribution\nmore variable than the data distribution\nunrelated to the variability of the data distribution\n\n\n\n\nWhy did we switch from talking about total weight to talking about average weight?113\n\nSo that it is easier to infer from the sample to the population.\nBecause the Coast Guard certifies vessels according to average weight.\nBecause the average is less variable than the sum.\nBecause the average has a normal distribution and the sum doesn’t.\n\n\n\n\nWhen the population is skewed right, the sampling distribution for the sample mean will be114\n\nalways skewed right\nskewed right if n is big enough\nalways normal\nnormal if n is big enough\n\n\n\n\nWhat does the CLT say?115\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\n\n8\n\n↩︎\n\n0.196 (19.6% of the time)\n\n↩︎\n\n0.063 (6.3% of the time)\n\n↩︎\n\nvery surprising (prob of 14 or more is 0.0021)\n\n↩︎\n\nNo, we can’t establish causation from an observational study.\n\n↩︎\n\nYes. For the exam(s?) under study, cursive caused higher scores on average.\n\n↩︎\nYou must connect the variable to both the explanatory and response variable. For me, that is easiest to do with c. academic ability of the student.↩︎\n\nTo balance every possible variable except the causal mechanism across the two groups\n\n↩︎\n\nSo that our sample is representative of the population\n\n↩︎\n\nunhappily obs study (becuase we want to establish causation)\n\n↩︎\n\ndefinitely obs study (do we care about causation? maybe. maybe not.)\n\n↩︎\n\nunhappily obs study\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nFALSE. You could get every single value wrong and still have a correlation of one.\n\n↩︎\n\nr = -0.416\n\n↩︎\n\nalways zero\n\n↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\n\nderivative\n\n↩︎\n\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\n9 or more\n\n↩︎\n\n5.3 because (15/31)*11 = 5.3\n\n↩︎\n\nrandom allocation\n\n↩︎\n\nStrong evidence that Botox is more effective than the placebo. p-value was roughly 0.005.\n\n↩︎\n\nClose to 50% (the point estimate is 0.6)\n\n↩︎\n\n20 (ish), 26*(38/48) = 20.58\n\n↩︎\n\nSomewhat surprising, p-value was 0.04\n\n↩︎\n\nalternative, one sided (because probably we are studying that it increases their success rate)\n\n↩︎\n\nnull, two sided (because I have no idea which cheetah might run faster)\n\n↩︎\n\nalternative, two sided (because I have no idea whether they’ve increased or decreased)\n\n↩︎\n\nnull, one sided (because I happen to know that folic acid is thought to prevent facial clefts)\n\n↩︎\n\nalternative, one sided (because I happen to know that caffeine is thought to decrease baby’s birth weight)\n\n↩︎\n\nA number which (is almost always unknown and) describes a population.\n\n↩︎\nThe warm-up solutions and clicker questions are on the main course website. The HW & Lab solutions are on Canvas under Files.↩︎\n\n50\n\n↩︎\n\n1000\n\n↩︎\n\nbetween (0.29, 0.31)\n\n↩︎\n\nboth. the intervals will be less likely (long-run) to capture the parameter and they will be narrower.\n\n↩︎\n\nnarrower (the sample size will not change the capture rate)\n\n↩︎\n\ncan be done for any statistic\n\n↩︎\n\n(0.34 mg/kg, 0.56 mg/kg)\n\n↩︎\n\nPiece of candy\n\n↩︎\n\nCup of candy\n\n↩︎\n\nThe spread changes\n\n↩︎\n\nThe center changes (the spread also changes a little bit, but mostly the center)\n\n↩︎\n\np\n\n↩︎\n\n+10 on the ACT\n\n↩︎\n\nthe number of standard deviations from the mean\n\n↩︎\n\nwithin \\(\\pm\\) 2\n\n↩︎\n\nor b. you most likely got between -2 and 2\n\n↩︎\n\n0.08 (we usually consider two standard deviations)\n\n↩︎\n\n95% of \\(\\hat{p}\\) are between (0.72, 0.88)\n\n↩︎\n\nless than 2 (but greater than 1)\n\n↩︎\n\nZ score comes from the data, Z* and is a pre-defined unit of measurement.\n\n↩︎\n\n\\(\\sqrt{\\hat{p} \\cdot (1 - \\hat{p}) / n}\\)\n\n↩︎\n\nTRUE\n\n↩︎\n\nFALSE (we are 95% confident that the new interval will contain the true value. We do not think that the new interval will be the same as the original interval.)\n\n↩︎\n\n3% of the time\n\n↩︎\n\n97% of the time\n\n↩︎\n\ndecreases\n\n↩︎\n\nstays the same (the population size has no effect on the sampling distribution of the statistic)\n\n↩︎\n\nstays the same (the variability of the data should be the same as the variability of the population, regardless of the sample size)\n\n↩︎\n\n9\n\n↩︎\n\nWe give him a raise when he doesn’t deserve it.\n\n↩︎\n\nWe don’t give him a raise when he deserves it.\n\n↩︎\n\nWe give him a raise when he deserves it.\n\n↩︎\n\nA type I error\n\n↩︎\n\nA type I error\n\n↩︎\n\nIncreases your power\n\n↩︎\n\nDecreases your power\n\n↩︎\n\nIncreases your power\n\n↩︎\n\nNo. My experience is that close to 50% of humans self-identify as female.\n\n↩︎\n\nNo. They didn’t just “get unlucky”. Instead, the reason the interval failed to capture the true parameter is because the sample was not representative of the population.\n\n↩︎\n\nNo. We know (for sure, with 100% confidence) that exactly 25% of U.S. senators in 2019 self identify as female. If that’s the entire population of interest, there’s no reason to calculate a confidence interval.\n\n↩︎\n\nrandom allocation of treatment\n\n↩︎\nrandom samples↩︎\n\nRandom sampling, although it would be pretty hard to do a true random sample from either country.\n\n↩︎\n\nRandom assignment. Randomly decide which exams to grade with which pen, and then record the scores.\n\n↩︎\n\nNeither. The student has the entire population of teams and was not able to randomly assign stadium type.\n\n↩︎\n\nthe ratio of two proportions\n\n↩︎\n\nbecause the ln transformation makes the sampling distribution almost normal\n\n↩︎\n\nif ln(\\(p_1\\)/\\(p_2\\)) is in the natural log-interval, \\(p_1\\)/\\(p_2\\) will be in the exponentiated interval. (Where “okay” means you have 95% coverage in repeated samples.)\n\n↩︎\n\n1 is not in the interval\n\n↩︎\n\nbecause the ln transformation makes the sampling distribution almost normal\n\n↩︎\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n↩︎\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n↩︎\n\nboth (cross-classification: can measure all the probabilities)\n\n↩︎\n\nthe proportion of people in the population in each explanatory category\n\n↩︎\n\nwhich variable is called the explanatory does not change the value of the OR\n\n↩︎\n\nwe don’t know the baseline risk\n\n↩︎\n\nFalse\n\n↩︎\n\na big \\(X^2\\) value\n\n↩︎\n\ntwo-sided alt hypothesis, and we only consider the upper end of the sampling distribution\n\n↩︎\n\nsleeping light\n\n↩︎\n\npopulation characteristics\n\n↩︎\n\nthe p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms.\n\n↩︎\n\nlow birth weight (the argument needs to be made that the confounding variable is associated with both the explanatory and the response variable)\n\n↩︎\n\nB: center (the typical distance from the mean is smallest)\n\n↩︎\n\nA (the IQR measures the middle 10 points in each group, the middle 10 points in group A are closer together than the middle 10 points in group B)\n\n↩︎\n\n35 (none of the other answers are reasonable)\n\n↩︎\n\n10 (\\(\\approx 35/\\sqrt{10}\\))\n\n↩︎\n\n5 (\\(\\approx 35/\\sqrt{100}\\))\n\n↩︎\n\n1 ( \\(\\approx 35/\\sqrt{1000}\\))\n\n↩︎\n\ncentered at the same place as the data distribution\n\n↩︎\n\nless variable than the data distribution\n\n↩︎\n\nSo that it is easier to infer from the sample to the population\n\n↩︎\n\nnormal if n is big enough\n\n↩︎\nDescribing random samples (of size n) from the population, the sampling distribution of the sample mean is normal if the sample size (n) is large enough.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "clicker.html",
    "href": "clicker.html",
    "title": "Introduction to Biostatistics",
    "section": "",
    "text": "Clicker Q\nto go with Introduction to Modern Statistics by Çentinkaya-Rundel & Hardin. Math 58B - Introduction to Biostatistics.\n\n\n\nIf 16 infants with no genuine preference choose 16 toys, what is the most likely number of “helping” toys that will be chosen?1\n\n4\n7\n8\n9\n10\n\n\n\n\nHow likely is it that exactly 8 helpers will be chosen (if there is no preference)?2\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nWhat if we flipped a coin 160 times? What percent of the time will the simulation flip exactly 80 heads?3\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nIs our actual result of 14 (under the coin model)…4\n\nvery surprising?\nsomewhat surprising?\nnot very surprising?\n\n\n\n\nBased on the first handwriting study, can we conclude that cursive causes higher scores (on average)?5\n\nYes\nNo\nIt depends\n\n\n\n\nBased on the second handwriting study, can we conclude that cursive causes higher scores (on average)?6\n\nYes\nNo\nIt depends\n\n\n\n\nA possible confounding variable for the handwriting study is:7\n\ngrade of the student (age)\nregion of country where the SAT was taken\nacademic ability of the student\ngender identity of the student\nnumber of siblings of the student.\n\n\n\n\nThe main reason we randomly assign the explanatory variable is:8\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nThe main reason we take random samples from the population is:9\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the expected causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nAre there effects of second-hand smoke on the health of children?10\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people tend to spend more money in stores located next to food outlets with pleasing smells?11\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDoes cell phone use increase the rate of automobile accidents?12\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people consume different amounts of ice cream depending on the size of bowl used?13\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nWhich is more effective: diet A or diet B?14\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored ten points higher on the final than on the midterm:15\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored five points lower on the final than on the midterm:16\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored twice as many points on the final than on the midterm:17\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose you guessed every value correctly (guess the correlation applet), what would be the value of the correlation coefficient between your guesses and the actual correlations?18\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose each of your guesses was too high by 0.2 from the actual value of the correlation coefficient, what would be the value of the correlation coefficient between your guesses and the actual correlations?19\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nA correlation coefficient equal to 1 indicates that you are a good guesser.20\n\nTRUE\nFALSE\n\n\n\n\nPerfect Correlation… if not for a single outlier\nn = 101 observations: 1 observation in top left, 25 observations in each in of the points near the bottom right.\nThe value of the correlation, r, is:21\n\n-1 < r < -0.9\n-0.9 < r < -0.5\n-0.5 < r < 0.5\n0.5 < r < 0.9\n0.9 < r < 1\n\n\n\n\n\n\n\n\n\n\nThe sum of residuals from the sample mean (no X):22 \\[\\sum_{i=1}^n(Y_i - \\overline{Y})\\]\n\nis positive\nis negative\nis zero\nis different for every dataset\n\n\n\n\nA good measure of how well the prediction (of the sample mean) fits the data is:23\n\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})\\)\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\overline{Y}|\\)\n\\(\\mbox{median}(Y_i - \\overline{Y})\\)\n\\(\\mbox{median}|Y_i - \\overline{Y}|\\)\n\n\n\n\nA good measure of how well the prediction (of the regression line) fits the data is:24\n\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)\\)\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\hat{Y}_i|\\)\n\\(\\mbox{median}(Y_i -\\hat{Y}_i)\\)\n\\(\\mbox{median}|Y_i -\\hat{Y}_i|\\)\n\n\n\n\nWhat math is used to find the value of \\(m\\) that minimizes:25 \\[\\sum_{i=1}^n(Y_i - m)^2\\]\n\ncombinatorics\nderivative\nintegral\nlinear algebra\n\n\n\n\n\\(\\sum_i(Y_i - \\overline{Y})^2\\) is sometimes \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)26\n\nTRUE\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is never \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?27\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\n“Observed data or more extreme” is:28\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?29\n\n0\n9\n5.3\n11\n15\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?30\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?31\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?32\n\nSubstantially greater than 50%\nSubstantially less than 50%\nVery close to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions / learning community check-ins, but I didn’t go this week.\nI can’t make the scheduled office hours / mentor sessions / learning community check-ins\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nIf communication medium and cheating are independent variables, how many of the email senders (out of 26) would you expect to cheat?33\n\n10 (ish)\n13 (ish)\n16 (ish)\n20 (ish)\n24 (ish)\n\n\n\n\nWhen looking at the null differences, is the observed result of 28.7%:34\n\nVery surprising\nSomewhat surprising\nNot very surprising\n\n\n\n\nHypothesis: the number of hours that grade-school children spend doing homework predicts their future success on standardized tests.35\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: king cheetahs on average run the same speed as standard spotted cheetahs.36\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: the mean length of African elephant tusks has changed over the last 100 years.37\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: the risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not.38\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nHypothesis: caffeine intake during pregnancy affects mean birth weight.39\n\nnull, one sided\nnull, two sided\nalternative, one sided\nalternative, two sided\n\n\n\n\nIn this class, the word parameter means:40\n\nThe values in a model\nNumbers that need to be tuned\nA number which is calculated from a sample of data.\nA number which (is almost always unknown and) describes a population.\n\n\n\n\nI know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW/Lab solutions41\n\nTRUE\nFALSE\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times (to get 1000 bootstrap resamples). What is the sample size of each bootstrap resample?42\n\n50\n1000\n\n\n\n\nYou have a sample of size n = 50. You sample with replacement 1000 times (to get 1000 bootstrap resamples). How many bootstrap statistics will you have?43\n\n50\n1000\n\n\n\n\nLet’s say you take a random sample and compute \\(\\hat{p}=0.3.\\) After bootstrapping, you see that the bootstrapped resamples produce almost all the \\(\\hat{p}_{boot}\\) within plus or minus 0.01 of your original statistic. It seems that the parameter \\(p\\) is probably:44\n\n0.3\nbetween (0.2, 0.4)\nbetween (0.29, 0.31)\nbetween (0.28, 0.32)\nhuh? how can we get \\(p\\) from \\(\\hat{p}?\\)\n\n\n\n\nIn a second analysis, I create a 90% CI for the true proportion \\(p.\\) What is the impact (of switching from 95% to 90%) on the CI?45\n\nnarrower\nless likely (long-run) to capture the parameter\nneither\nboth\n\n\n\n\nIn a second study, I set out to obtain twice as much data (as in the first study) in order to create a 95% CI for the true proportion \\(p.\\) What is the impact (of the larger sample) on the CI?46\n\nnarrower\nmore likely (long-run) to capture the parameter\nneither\nboth\n\n\n\n\nWhat is one main reason to use bootstrapping to find a confidence interval?47\n\nlarger coverage probabilities\nnarrower intervals\nmore resistant to outliers\ncan be done for any statistic\n\n\n\n\n95% CI for the true median mercury:48\n\n(0.025 mg/kg, 0.975 mg/kg)\n(0.469 mg/kg, 0.053 mg/kg)\n(0.053 mg/kg, 0.469 mg/kg)\n(0.34 mg/kg, 0.56 mg/kg)\n\n\n\n\n\n\n\n\nFrom StatKey applet: https://www.lock5stat.com/StatKey/\n\n\n\n\n\n\nWhat are the observational units for your individual candy study?49\n\nColor of the candy\nPiece of candy\nCup of candy\nThe Hershey Company\nProportion that are orange\n\n\n\n\nWhat are the observational units for the class compilation (dotplot)?50\n\nColor of the candy\nPiece of candy\nCup of candy\nThe Hershey Company\nProportion that are orange\n\n\n\n\nHow does the sampling distribution for the sample proportion change as n changes (for a fixed p)?51\n\nThe spread changes\nThe symmetry changes\nThe center changes\nThe shape changes\n\n\n\n\nHow does the sampling distribution change as p changes (for a fixed n)?52\n\nThe spread changes\nThe symmetry changes\nThe center changes\nThe shape changes\n\n\n\n\nThe Central Limit Theorem says that the distribution of \\(\\hat{p}\\) will be approximately normal with what center:53\n\n\\(\\hat{p}\\)\n\\(p\\)\n0.5\n1\n\\(\\sqrt{p(1-p) / n}\\)\n\n\n\n\nWould you rather have an extra 20 points on the SAT or an extra 10 points on the ACT?54\n\n+20 on the SAT\n+10 on the ACT\n\n\n\n\nThe standardized score (z-score) counts:55\n\nthe number of standard deviations from the mean\nthe number of standard deviations above the mean\nthe number of standard deviations below the mean\nthe distance from the mean\nthe distance from the standard deviation\n\n\n\n\nIf the normal distribution is a good model, we would expect the large majority of our z scores to be:56\n\nwithin \\(\\pm\\) 1 of the mean\nwithin \\(\\pm\\) 2 of the mean\nwithin \\(\\pm\\) 1\nwithin \\(\\pm\\) 2\n\n\n\n\nWith your cup of candy, you personally got a Z score of:57\n\nbetween (-1, 1) (not including 1)\nbetween (-2, -1] or [1, 2)\nbetween (-3, -2] or [2, 3)\n-3 or smaller or 3 or above\n\n\n\n\nAssume n = 100 and p= 0.8 (note: \\(\\sqrt{(0.8 \\cdot 0.2)/100} = 0.4/10 = 0.04\\))\nWhat is the largest reasonable distance between \\(\\hat{p}\\) and \\(p\\)?\nThat is, we would expect \\(\\hat{p}\\) and \\(p\\) to be no more than _____ apart58\n\n0.04\n0.08\n0.12\n0.16\n0.24\n\n\n\n\nAssume n = 100 and p= 0.8 (note: \\(\\sqrt{(0.8 \\cdot 0.2)/100} = 0.4/10 = 0.04\\)) Which statement is true?59\n\n95% of \\(\\hat{p}\\) are between (0.76, 0.84)\n95% of \\(\\hat{p}\\) are between (0.72, 0.88)\n95% of \\(\\hat{p}\\) are between (0.68, 0.92)\n95% of \\(p\\) are between (0.76, 0.84)\n95% of \\(p\\) are between (0.72, 0.88)\n\n\n\n\nIf you want a 90% confidence interval for p, your z* multiplier should be60\n\nless than 1\nless than 2 (but greater than 1)\nequal to 2\ngreater than 2 (but less than 3)\ngreater than 3\n\n\n\n\nWhat is the difference between Z* and a Z score?61\n\nZ score comes from the data, Z* and is a pre-defined unit of measurement.\nZ* comes from the data, and Z score is a pre-defined unit of measurement\nZ score assumes the null hypothesis is true and Z* doesn’t.\nZ* assumes the null hypothesis is true, and Z score doesn’t\n\n\n\n\nLet’s say we are making confidence intervals (not doing a hypothesis test), what is your best guess for \\(SE(\\hat{p})\\)?62\n\n\\(\\sqrt{0.5 \\cdot (1 - 0.5) / n}\\)\n\\(\\sqrt{p \\cdot (1 - p) / n}\\)\n\\(\\sqrt{\\hat{p} \\cdot (1 - \\hat{p}) / n}\\)\n\\(\\sqrt{X \\cdot (1 - X) / n}\\)\n\\(\\sqrt{0.95 \\cdot (1 - 0.95) / n}\\)\n\n\n\n\nThe following is a correct interpretation of the CI:63\n\n\n95% confident that the interval includes the sample proportion who believe that the global poverty rate has doubled.\n\n\nTRUE\nFALSE\n\n\n\nThe following is a correct interpretation of the CI:64\n\n\nIf researchers were to select a new sample of 1005 adult Americans, then we’re 95% confident that between 56% and 62% of those people would answer “doubled” to the question.\n\n\nTRUE\nFALSE\n\n\n\nLet’s say that the null hypothesis (e.g., p=0.47) is TRUE. My level of significance is 0.03 (reject if p-value < 0.03). How often will I reject the null hypothesis?65\n\n1 % of the time\n3% of the time\n5 % of the time\n95% of the time\n97% of the time\n\n\n\nWhat does “of the time” mean???\nIt means in repeated samples. That is, in 3% of all datasets we’d take from that exact same population, we would mistakenly reject the actually true hypothesis that p=0.47.\n\n\nLet’s say that the null hypothesis (e.g., p=0.47) is TRUE. My level of significance is 0.03.\nHow often will p be in a 97% confidence interval?66\n\n1 % of the time\n3% of the time\n5 % of the time\n95% of the time\n97% of the time\n\n\n\nWhat does “of the time” mean???\nIt means in repeated samples. That is, in 97% of all datasets we’d take from that exact same population, we would capture the true population proportion of 0.47.\n\n\nSuppose the sample is 10 times larger. The SE of the statistic:67\n\nincreases\nstays the same\ndecrease\n\n\n\n\nSuppose the population is 10 times larger. The SE of the statistic:68\n\nincreases\nstays the same\ndecrease\n\n\n\n\nSuppose the sample is 10 times larger. The variability of the data:69\n\nincreases\nstays the same\ndecrease\n\n\n\n\nHow many hits out of 20 at bats would make you believe him?70\n\n5\n6\n7\n8\n9\n\n\n\n\nType I error is71\n\nWe give him a raise when he deserves it.\nWe don’t give him a raise when he deserves it.\nWe give him a raise when he doesn’t deserve it.\nWe don’t give him a raise when he doesn’t deserve it.\n\n\n\n\nType II error is72\n\nWe give him a raise when he deserves it.\nWe don’t give him a raise when he deserves it.\nWe give him a raise when he doesn’t deserve it.\nWe don’t give him a raise when he doesn’t deserve it.\n\n\n\n\nPower is the probability that:73\n\nWe give him a raise when he deserves it.\nWe don’t give him a raise when he deserves it.\nWe give him a raise when he doesn’t deserve it.\nWe don’t give him a raise when he doesn’t deserve it.\n\n\n\n\nThe player is more worried about74\n\nA type I error\nA type II error\n\n\n\n\nThe manager is more worried about75\n\nA type I error\nA type II error\n\n\n\n\nIncreasing your sample size76\n\nIncreases your power\nDecreases your power\n\n\n\n\nMaking your significance level more stringent (\\(\\alpha\\) smaller)77\n\nIncreases your power\nDecreases your power\n\n\n\n\nA more extreme alternative78:\n\nIncreases your power\nDecreases your power\n\n\n\n\nIs the Alien’s interval for the true proportion of all humans who self-identify as female consistent with your lived experience?79\n\nYes\nNo\nI don’t understand what the confidence interval represents.\n\n\n\n\nAs we’ve seen with the applet, about 5% of all 95% intervals fail to capture the actual value of the population parameter. Do you think the alien just got a “red” interval?80\n\nYes\nNo\n\n\n\n\nWould it be reasonable for the alien to conclude, with 95% confidence, that between 16.5% and 33.5% of US Senators in the year 2023 self-identify as female?81\n\nYes\nNo\n\n\n\n\nThe “random” part in clinical trials typically comes from:82\n\nrandom samples\nrandom allocation of treatment\n\n\n\n\nThe “random” part in polling typically comes from:83\n\nrandom samples\nrandom allocation of treatment\n\n\n\n\nYou want to collect data to investigate whether teenagers in the United States have read fewer Harry Potter books than teenagers in the United Kingdom. Would you make use of random sampling, random assignment, both, or neither?84\n\nRandom sampling\nRandom assignment\nBoth\nNeither\n\n\n\n\nAn instructor wants to investigate whether using a red pen to grade assignments leads to lower scores on exams than using a blue pen to grade assignments. Would you advise the professor to make use of random sampling, random assignment, both, or neither?85\n\nRandom sampling\nRandom assignment\nBoth\nNeither\n\n\n\n\nA student decides to investigate whether NFL football games played in indoor stadiums tend to have more points scored than games played outdoors. The student examines points scored in every NFL game of the 2022 season. Has the student used random sampling, random assignment, both, or neither?86\n\nRandom sampling\nRandom assignment\nBoth\nNeither\n\n\n\n\nRelative Risk is87\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nIn order to find a CI for the true RR, our steps are:88\nStep 1. ln(RR-hat)\nStep 2. add ± z* sqrt( 1/A - 1/(A+C) + 1/B - 1/(B+D) )\nStep 3. find exp of the endpoints\n\nbecause the sampling distribution of RR is normal\nbecause RR is typically greater than 1\nbecause the ln transformation makes the sampling distribution almost normal\nbecause RR is invariant to the choice of explanatory or response variable\n\n\n\n\nIn finding a CI for \\(p_1\\)/\\(p_2\\), why is it okay to exponentiate the end points of the interval for ln(\\(p_1\\)/\\(p_2\\))?89\n\nif ln(\\(p_1\\)/\\(p_2\\)) is in the natural log-interval, \\(p_1\\)/\\(p_2\\) will be in the exponentiated interval.\nthe natural log of the RR makes the distribution approximately normal.\nthe natural log compresses values that are > 1 and spreads values < 1.\n\n\n\n\nUsually, the CI for \\(p_1\\)/\\(p_2\\) is considered to be “significant” if90\n\n\\(p_1\\)/\\(p_2\\) is not in the interval\n\\(\\hat{p}_1 / \\hat{p}_2\\) is not in the interval\n0 is not in the interval\n1 is not in the interval\n\n\n\n\nIn order to find a CI for the true OR, our steps are:91\nStep 1. ln(OR-hat)\nStep 2. add ± z* sqrt( 1/A + 1/B + 1/C + 1/D )\nStep 3. find exp of the endpoints\n\nbecause the sampling distribution of OR is normal\nbecause OR is typically greater than 1\nbecause the ln transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA.\nWhat is measurable?92\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?93\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA.\nWhat is measurable?94\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nWhen we randomly select individuals based on the explanatory variable, we cannot accurately measure95\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nThe odds ratio is invariant to which variable is explanatory and which is response means:96\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nOne reason we should be careful interpreting relative risks is if:97\n\nwe don’t know the difference in proportions\nwe don’t know the SE of the relative risk\nwe might be dividing by zero\nwe don’t know the baseline risk\n\n\n\n\nIf the null hypothesis is true, the observed counts will equal the expected counts.98\n\nTrue\nFalse\n\n\n\n\nTo reject the null hypothesis we want to see99\n\na small \\(X^2\\) value\na big \\(X^2\\) value\n\n\n\n\nA chi-square test has a100\n\none-sided alt hypothesis, and we only consider the upper end of the sampling distribution\none-sided alt hypothesis, and we consider both ends of the sampling distribution\ntwo-sided alt hypothesis, and we only consider the upper end of the sampling distribution\ntwo-sided alt hypothesis, and we consider both ends of the sampling distribution\n\n\n\n\nFor the lighting study, which variable is the explanatory variable?101\n\nsleeping light\neye sightedness\nchild\nparent\n\n\n\n\nIf we sample randomly from a population, the conclusions we can make are about:102\n\ncausation\npopulation characteristics\n\n\n\n\nBased on the night light / myopia example, the correct conclusion is:103\n\nthe p-value is small, so sleeping in a lit room makes it more likely that you are near-sighted.\nthe p-value is small, so sleeping in a dark room makes it more likely that you are near-sighted.\nthe p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms.\n\\(\\hat{p}_{\\mbox{near}}\\) if lit room = 41/75 = 0.547 and \\(\\hat{p}_{\\mbox{near}}\\) if dark = 18/172 = 0.105, therefore sleeping with the light on is bad for you.\n\n\n\n\nA possible confounding variable for the night light study is:104\n\nlow birth weight\nrace (70% of the children were white)\nregion of the country where the clinic was located\n\n\n\n\nWhich dataset has the smallest standard deviation?105\n\nA: left\nB: center\nC: right\n\n\n\n\n\n\n\n\n\nWhich of the two dotplots displays the dataset with the smaller IQR?106\n\nA\nB\n\n\n\n\n\n\n\n\n\nThe standard deviation of weights (mean = 167 lbs) is approximately107\n\n1\n5\n10\n35\n100\n\n\n\n\nThe standard deviation of average weights (mean = 167 lbs) in repeated samples of size 10 is approximately108\n\n1\n5\n10\n35\n100\n\n\n\n\nThe standard deviation of average weights (mean = 167 lbs) in repeated samples of size 50 is approximately109\n\n1\n5\n10\n35\n100\n\n\n\n\nThe standard deviation of average weights (mean = 167 lbs) in repeated samples of size 1000 is approximately110\n\n1\n5\n10\n35\n100\n\n\n\nQ: what is the most confusing part of understanding the difference between the variability of the weights and the variability of the average of the weights?\n\n\nThe sampling distribution of the mean will be111\n\ncentered below the data distribution\ncentered at the same place as the data distribution\ncentered above the data distribution\nunrelated to the center of the data distribution\n\n\n\n\nThe sampling distribution of the mean will be112\n\nless variable than the data distribution\nthe same variability as the data distribution\nmore variable than the data distribution\nunrelated to the variability of the data distribution\n\n\n\n\nWhy did we switch from talking about total weight to talking about average weight?113\n\nSo that it is easier to infer from the sample to the population.\nBecause the Coast Guard certifies vessels according to average weight.\nBecause the average is less variable than the sum.\nBecause the average has a normal distribution and the sum doesn’t.\n\n\n\n\nWhen the population is skewed right, the sampling distribution for the sample mean will be114\n\nalways skewed right\nskewed right if n is big enough\nalways normal\nnormal if n is big enough\n\n\n\n\nWhat does the CLT say?115\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\n\n8\n\n↩︎\n\n0.196 (19.6% of the time)\n\n↩︎\n\n0.063 (6.3% of the time)\n\n↩︎\n\nvery surprising (prob of 14 or more is 0.0021)\n\n↩︎\n\nNo, we can’t establish causation from an observational study.\n\n↩︎\n\nYes. For the exam(s?) under study, cursive caused higher scores on average.\n\n↩︎\nYou must connect the variable to both the explanatory and response variable. For me, that is easiest to do with c. academic ability of the student.↩︎\n\nTo balance every possible variable except the causal mechanism across the two groups\n\n↩︎\n\nSo that our sample is representative of the population\n\n↩︎\n\nunhappily obs study (becuase we want to establish causation)\n\n↩︎\n\ndefinitely obs study (do we care about causation? maybe. maybe not.)\n\n↩︎\n\nunhappily obs study\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nFALSE. You could get every single value wrong and still have a correlation of one.\n\n↩︎\n\nr = -0.416\n\n↩︎\n\nalways zero\n\n↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\n\nderivative\n\n↩︎\n\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\n9 or more\n\n↩︎\n\n5.3 because (15/31)*11 = 5.3\n\n↩︎\n\nrandom allocation\n\n↩︎\n\nStrong evidence that Botox is more effective than the placebo. p-value was roughly 0.005.\n\n↩︎\n\nClose to 50% (the point estimate is 0.6)\n\n↩︎\n\n20 (ish), 26*(38/48) = 20.58\n\n↩︎\n\nSomewhat surprising, p-value was 0.04\n\n↩︎\n\nalternative, one sided (because probably we are studying that it increases their success rate)\n\n↩︎\n\nnull, two sided (because I have no idea which cheetah might run faster)\n\n↩︎\n\nalternative, two sided (because I have no idea whether they’ve increased or decreased)\n\n↩︎\n\nnull, one sided (because I happen to know that folic acid is thought to prevent facial clefts)\n\n↩︎\n\nalternative, one sided (because I happen to know that caffeine is thought to decrease baby’s birth weight)\n\n↩︎\n\nA number which (is almost always unknown and) describes a population.\n\n↩︎\nThe warm-up solutions and clicker questions are on the main course website. The HW & Lab solutions are on Canvas under Files.↩︎\n\n50\n\n↩︎\n\n1000\n\n↩︎\n\nbetween (0.29, 0.31)\n\n↩︎\n\nboth. the intervals will be less likely (long-run) to capture the parameter and they will be narrower.\n\n↩︎\n\nnarrower (the sample size will not change the capture rate)\n\n↩︎\n\ncan be done for any statistic\n\n↩︎\n\n(0.34 mg/kg, 0.56 mg/kg)\n\n↩︎\n\nPiece of candy\n\n↩︎\n\nCup of candy\n\n↩︎\n\nThe spread changes\n\n↩︎\n\nThe center changes (the spread also changes a little bit, but mostly the center)\n\n↩︎\n\np\n\n↩︎\n\n+10 on the ACT\n\n↩︎\n\nthe number of standard deviations from the mean\n\n↩︎\n\nwithin \\(\\pm\\) 2\n\n↩︎\n\nor b. you most likely got between -2 and 2\n\n↩︎\n\n0.08 (we usually consider two standard deviations)\n\n↩︎\n\n95% of \\(\\hat{p}\\) are between (0.72, 0.88)\n\n↩︎\n\nless than 2 (but greater than 1)\n\n↩︎\n\nZ score comes from the data, Z* and is a pre-defined unit of measurement.\n\n↩︎\n\n\\(\\sqrt{\\hat{p} \\cdot (1 - \\hat{p}) / n}\\)\n\n↩︎\n\nTRUE\n\n↩︎\n\nFALSE (we are 95% confident that the new interval will contain the true value. We do not think that the new interval will be the same as the original interval.)\n\n↩︎\n\n3% of the time\n\n↩︎\n\n97% of the time\n\n↩︎\n\ndecreases\n\n↩︎\n\nstays the same (the population size has no effect on the sampling distribution of the statistic)\n\n↩︎\n\nstays the same (the variability of the data should be the same as the variability of the population, regardless of the sample size)\n\n↩︎\n\n9\n\n↩︎\n\nWe give him a raise when he doesn’t deserve it.\n\n↩︎\n\nWe don’t give him a raise when he deserves it.\n\n↩︎\n\nWe give him a raise when he deserves it.\n\n↩︎\n\nA type I error\n\n↩︎\n\nA type I error\n\n↩︎\n\nIncreases your power\n\n↩︎\n\nDecreases your power\n\n↩︎\n\nIncreases your power\n\n↩︎\n\nNo. My experience is that close to 50% of humans self-identify as female.\n\n↩︎\n\nNo. They didn’t just “get unlucky”. Instead, the reason the interval failed to capture the true parameter is because the sample was not representative of the population.\n\n↩︎\n\nNo. We know (for sure, with 100% confidence) that exactly 25% of U.S. senators in 2019 self identify as female. If that’s the entire population of interest, there’s no reason to calculate a confidence interval.\n\n↩︎\n\nrandom allocation of treatment\n\n↩︎\nrandom samples↩︎\n\nRandom sampling, although it would be pretty hard to do a true random sample from either country.\n\n↩︎\n\nRandom assignment. Randomly decide which exams to grade with which pen, and then record the scores.\n\n↩︎\n\nNeither. The student has the entire population of teams and was not able to randomly assign stadium type.\n\n↩︎\n\nthe ratio of two proportions\n\n↩︎\n\nbecause the ln transformation makes the sampling distribution almost normal\n\n↩︎\n\nif ln(\\(p_1\\)/\\(p_2\\)) is in the natural log-interval, \\(p_1\\)/\\(p_2\\) will be in the exponentiated interval. (Where “okay” means you have 95% coverage in repeated samples.)\n\n↩︎\n\n1 is not in the interval\n\n↩︎\n\nbecause the ln transformation makes the sampling distribution almost normal\n\n↩︎\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n↩︎\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n↩︎\n\nboth (cross-classification: can measure all the probabilities)\n\n↩︎\n\nthe proportion of people in the population in each explanatory category\n\n↩︎\n\nwhich variable is called the explanatory does not change the value of the OR\n\n↩︎\n\nwe don’t know the baseline risk\n\n↩︎\n\nFalse\n\n↩︎\n\na big \\(X^2\\) value\n\n↩︎\n\ntwo-sided alt hypothesis, and we only consider the upper end of the sampling distribution\n\n↩︎\n\nsleeping light\n\n↩︎\n\npopulation characteristics\n\n↩︎\n\nthe p-value is small, so a higher proportion of children who sleep in light rooms are near-sighted than who sleep in dark rooms.\n\n↩︎\n\nlow birth weight (the argument needs to be made that the confounding variable is associated with both the explanatory and the response variable)\n\n↩︎\n\nB: center (the typical distance from the mean is smallest)\n\n↩︎\n\nA (the IQR measures the middle 10 points in each group, the middle 10 points in group A are closer together than the middle 10 points in group B)\n\n↩︎\n\n35 (none of the other answers are reasonable)\n\n↩︎\n\n10 (\\(\\approx 35/\\sqrt{10}\\))\n\n↩︎\n\n5 (\\(\\approx 35/\\sqrt{100}\\))\n\n↩︎\n\n1 ( \\(\\approx 35/\\sqrt{1000}\\))\n\n↩︎\n\ncentered at the same place as the data distribution\n\n↩︎\n\nless variable than the data distribution\n\n↩︎\n\nSo that it is easier to infer from the sample to the population\n\n↩︎\n\nnormal if n is big enough\n\n↩︎\nDescribing random samples (of size n) from the population, the sampling distribution of the sample mean is normal if the sample size (n) is large enough.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Class notes can be found at http://st47s.com/Math58/Notes/.\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/MA58_HW0s23.html",
    "href": "slides/MA58_HW0s23.html",
    "title": "HW0 – Math 58B",
    "section": "",
    "text": "The syllabus is an agreement between the professor and students. It explains your responsibilities, lays out the structure, and gives you information on how best to achieve the course goals. Like any agreement, it must be read carefully and referenced frequently to answer questions. This fun activity will highlight some important parts of the syllabus and give you a chance to try out R!1\n\nInstructions\nAnswer each question below by typing a response. Knit the file to a compiled pdf, and submit it to Gradescope (via Canvas).\nHint: To compile to pdf, you might need to use the tinytex package (you’ll know you need it if you can knit to html but not to pdf). If that is the case, then in the console (below, do you see where it says “Console”?) only one time ever, type the following. Ask me if it doesn’t work automatically for you. (Don’t spend hours figuring this part out, ask me immediately.)\ninstall.packages('tinytex')\ntinytex::install_tinytex()\nhttps://yihui.name/tinytex/\n\nQ1.\nOn which dates are the exams?\nAfter you’ve marked the exam dates in your calendar, answer the following question: what type of calendar do you keep? (e.g., Google calendar, outlook, paper journal, post-its all over your desk, etc.)\n\n\nQ2.\nProvide three pieces of information from the syllabus related to class participation.\n\n\nQ3.\nWill course notes be available and posted? If so, where?\n\n\nQ4.\nWhat is the software program we are using for the class? After watching the R video on our Box video page (see Canvas for the link), ask one question that you have about R. [If you are already familiar with R, you don’t need to watch the video, but you surely can think up one question about R.]\n\n\nQ5.\nRun the code below one line at a time. Provide a few words describing what each line of code is doing.\nThe words of explanation could come before or after the R chunk, just like any sentences written to a client describing the analysis.\n\nmydata <- c(1:10)  # the words of explanation could come after the hashtag\nmydata\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmydata^2\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\nsample(mydata, size = 25, replace = TRUE)\n\n [1] 10  8  4  8  8  6  3  7  7  2  9  5  9  4  2  8  7  6  7  1  4  8  2  5  9\n\nmydata2 <- sample(mydata, size = 25, replace = TRUE)\nmydata2\n\n [1]  2 10  9  1  6  9  8  7 10  5  2  5  6  4  7  5  1  8  1  2  2  5  3  2  5\n\n\n\n\nQ6.\nWhat are the reflection questions and ethics considerations? Where do you find them? What should you do with them?\n\n\nQ7.\nNice job! Run the chunk of code below. You might need to install the praise package. See the top of this file.\n\npraise()\n\n[1] \"You are marvelous!\"\n\n\nNote: if you want any of your output to remain constant, use the set.seed() function. The function will control the randomness associated with the task you’ve asked of R. For example, you asked R to sample from some integers. Do you want the sample of integers to stay the same every time? Well, use set.seed()! The only argument you need for set.seed() is a single integer. You can choose any integer you want. And the function goes before (either right before or at the top of the file) the command where R is dong something random. Here is an example of some code (which won’t be run because I set eval = FALSE) where the randomness is controlled. Try it yourself in your work above.\n\nset.seed(47)\nsample(mydata, size = 25, replace = TRUE)\n\n\n\n\n\n\n\nFootnotes\n\n\nadapted from David White at Denison University.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/MA58_HW1s23.html",
    "href": "slides/MA58_HW1s23.html",
    "title": "HW1 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\n(Actually no R code for this assignment!! You should still compile using the R Markdown format.)\nexperiments vs observational studies & confounding variables\n\n\nQ0. Learning Community\nReport one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.).\n\n\nQ1. Eat better, feel better?1\nIn a public health study on the effects of consumption of fruits and vegetables on psychological well-being in young adults, participants were randomly assigned to three groups: (1) diet-as-usual, (2) an ecological momentary intervention involving text message reminders to increase their fruits and vegetable consumption plus a voucher to purchase them, or (3) a fruit and vegetable intervention in which participants were given two additional daily servings of fresh fruits and vegetables to consume on top of their normal diet. Participants were asked to take a nightly survey on their smartphones. Participants were student volunteers at the University of Otago, New Zealand. At the end of the 14-day study, only participants in the third group showed statistically significant improvements to their psychological well-being across the 14-days relative to the other groups.\n\nWhat type of study is this?\nIdentify the explanatory and response variables.\nComment on whether the results of the study can be generalized to the population.\nComment on whether the results of the study can be used to establish causal relationships.\nA newspaper article reporting on the study states, “The results of this study provide proof that giving young adults fresh fruits and vegetables to eat can have psychological benefits, even over a brief period of time.” How would you suggest revising this statement so that it can be supported by the study?\n\n\n\nQ2. Have a Nice trip2\nAn area of research in biomechanics and gerontology concerns falls and fall-related injuries, especially for elderly people. Recent studies have focused on how individuals respond to large postural disturbances (e.g., tripping, induced slips). One question is whether subjects can be instructed to improve their recovery from such perturbations. Suppose researchers want to compare two such recovery strategies, lowering (making the next step shorter, but in normal step time) and elevating (using a longer or normal step length with normal step time). Subjects will have first been trained on one of these two recovery strategies, and they will be asked to apply it after they feel themselves tripping. The researchers will then induce the subject to trip while walking (but harnessed for safety), using a concealed mechanical obstacle.\nYou will need to access the Randomizing Subjects applet (which simulates a study as described above).\nNote: while gender is not a binary characteristic in humans, there are times when understanding trends in body dimensions (e.g., center of gravity) is useful for understanding biomechanics. We use gender here as an insufficient proxy and not as a claim that all people will fit this model nor as a claim that gender is necessarily the right proxy measure.\n\nOne way to design an experiment for this study would be to assign the eight females to use the elevating strategy and the 16 males to use the lowering strategy. Would this be a reasonable strategy? If not, identify a better method for deciding who uses which strategy.\n\nUse the applet [http://www.rossmanchance.com/applets/2021/sampling/Subjects.html] to randomly assign the participants to the two treatment groups. Repeat the assignment over and over until you get a sense for how the different variables (gender, height, BMI, “gene”) are / can be distributed across the two groups.\n\nDoes random assignment always equally distribute/balance the men and women between the two groups? Give the range of possible discrepancies for the gender imbalance.\nIs there a tendency for there to be a similar proportion of men in the two groups? How are you deciding? What does this tell you about the plausibility of any later difference in the two groups being attributed to females having better balance?\nPrior research has also shown that the likelihood of falling is related to variables such as walking speed, stride rate, and height, so we would like the random assignment to distribute these variables equally between the groups as well. In the applet, use the pull-down menu to switch from the sex variable to the height variable. The dotplot now displays the differences in average height between Group 1 and Group 2 for these 200 repetitions. In the long-run, does random assignment tend to equally distribute the height variable between the two groups? Explain.\nSuppose there is a “balance gene” that is related to people’s ability to recover from a trip. We didn’t know about this gene ahead of time, but if you select the “Reveal gene?” button and then select “gene” from the pull-down menu, the applet shows you this gene information for each subject and also how the proportions with the gene differ in the two groups. Does this variable tend to equalize between the two groups in the long run? Explain.\nSuppose there were other “x-variables” that we could not measure such as BMI, stride rate, or walking speed. Select the “Reveal both?” button and use pull-down menu to display the results for BMI. Does random assignment generally succeed in equalizing this variable between the two groups or is there a tendency for one group to always have higher results for BMI? Explain.\nIs the fact that the people volunteered to participate a “confounding variable” in this study? Explain.\nSuppose the “tripping” study was able to rule out chance as a mechanism for the differences seen in the two strategies. What conclusion would you draw? For what population? What additional information would you need to know?\n\n\n\nQ3. AstraZeneca3\nIn the Wired article about the AstraZeneca vaccine they mention collecting information about age (i.e., one of the variables which was measured was the age of the participant).\n\nIf the vaccine was randomly allocated to the participants, can age be a confounding variable? (Remember, to be a confounding variable you need to have an associating with both the explanatory variable of interest and the response variable of interest).\nCould age be the causal mechanism behind why the vaccine is working? Explain.\nWhy did they collect information about age?\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone.\n\n\n\n\n\n\nFootnotes\n\n\nFrom OpenIntro Statistics, exercise 1.41. Tamlin S Conner et al. “Let them eat fruit! The effect of fruit and vegetable consumption on psychological well-being in young adults: A randomized controlled trial”. In: PloS one 12.2 (2017), e0171206.↩︎\nFrom ISCAM, Inv 3.4↩︎\nhttps://www.wired.com/story/the-astrazeneca-covid-vaccine-data-isnt-up-to-snuff/↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html",
    "href": "slides/lab1_m58_s23_intro_to_data.html",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "",
    "text": "Some define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe the raw information – the data. In this lab we explore flights, specifically a random sample of domestic flights that departed from the three major New York City airport in 2013. We will generate simple graphical and numerical summaries of data on these flights and explore delay times. As this is a large data set, along the way you’ll also learn the indispensable skills of data processing and subsetting.\nThe goals for today include be able to use the following commands:"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab1_m58_s23_intro_to_data.html#advice-for-turning-in-the-assignment",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe ASSIGNMENT part of the lab is ONLY the last four questions at the very bottom. However, you will need many of the commands in the top half, otherwise the commands at the bottom won’t run!"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#getting-started",
    "href": "slides/lab1_m58_s23_intro_to_data.html#getting-started",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nIn this lab we will explore the data using functions that can be found in the tidyverse package. The data can be found in the package nycflights13.\nLet’s load the packages.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\nThe data\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes available transportation data, such as the flights data we will be working with in this lab.\nWe begin by loading the flights data frame. Run the following command by clicking on the green triangle:\n\ndata(flights)\n\nThe data set flights that shows up in your workspace is a data matrix, with each row representing an observation and each column representing a variable. R calls this data format a data frame, which is a term that will be used throughout the labs. For this data set, each observation is a single flight.\nTo view the names of the variables, run the command\n\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\n\nThis returns the names of the variables in this data frame. The codebook (description of the variables) can be accessed by pulling up the help file (run this line with the green triangle and then look at the box on the bottom right of the RStudio screen):\n\n?flights\n\nOne of the variables refers to the carrier (i.e. airline) of the flight, which is coded according to the following system.\n\ncarrier: Two letter carrier abbreviation.\n\n9E: Endeavor Air Inc.\nAA: American Airlines Inc.\nAS: Alaska Airlines Inc.\nB6: JetBlue Airways\nDL: Delta Air Lines Inc.\nEV: ExpressJet Airlines Inc.\nF9: Frontier Airlines Inc.\nFL: AirTran Airways Corporation\nHA: Hawaiian Airlines Inc.\nMQ: Envoy Air\nOO: SkyWest Airlines Inc.\nUA: United Air Lines Inc.\nUS: US Airways Inc.\nVX: Virgin America\nWN: Southwest Airlines Co.\nYV: Mesa Airlines Inc.\n\n\nA very useful function for taking a quick peek at your data frame and viewing its dimensions and data types is glimpse().\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nThe flights data frame is a massive trove of information (336,776 observations!!!). Notice also that the glimpse function let’s you know how the variables are stored: integer, double (a fancy way to say decimal number), character string, date/time, etc. Let’s think about some questions we might want to answer with these data:\n\nHow delayed were flights that were headed to Los Angeles?\nHow do departure delays vary over months?\nWhich of the three major NYC airports has a better on time percentage for departing flights?\n\n\n\nTidy Structure of Data\nFor plotting, analyses, model building, etc., the data should be structured according to certain principles.\n\nTidy Data: rows (cases/observational units) and columns (variables).\nThe key is that every row is a case and every column is a variable.\nNo exceptions.\nCreating tidy data is often not trivial.\n\nWithin R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language.\nSome things to consider:\n\nobject_name <- anything is a way of assigning anything to the new object_name.\nobject_name <- function_name(data_frame, arguments) is a way of using a function to create a new object.\nobject_name <- data_frame %>% function_name(arguments) uses chaining syntax as an extension of the ideas of functions.\nIn chaining, the value on the left side of %>% becomes the first argument to the function on the right side.\n\nobject_name <- data_frame %>%\n                    function_name(arguments) %>% \n                    another_function_name(other_arguments)\nis extended chaining. %>% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %>% is always a data table. * The pipe syntax should be read as and then, %>%.\n\n\nUsing the pipe to chain\nThe pipe syntax (%>%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function.\nFor example:\nx %>% f(y) is the same as f(x, y)\ny %>% f(x, ., z) is the same as f(x,y,z)\nPipes are used commonly with functions to sequentially use data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations."
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#analysis",
    "href": "slides/lab1_m58_s23_intro_to_data.html#analysis",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "Analysis",
    "text": "Analysis\n\nDeparture delays\n\nselect() (chooses which variable(s))\nLet’s start by examining the distribution of departure delays of all flights using the summary() function. The first item (on the left) is the data set. Subsequently, two functions are applied, (1) select() to get only the dep_delay variables, and (2) summary() to produce the numerical summary.\n\nflights %>% \n  select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   : -43.00  \n 1st Qu.:  -5.00  \n Median :  -2.00  \n Mean   :  12.64  \n 3rd Qu.:  11.00  \n Max.   :1301.00  \n NA's   :8255     \n\nsummary(select(flights, dep_delay))\n\n   dep_delay      \n Min.   : -43.00  \n 1st Qu.:  -5.00  \n Median :  -2.00  \n Mean   :  12.64  \n 3rd Qu.:  11.00  \n Max.   :1301.00  \n NA's   :8255     \n\n\n\n\nfilter() (create a smaller dataset, picks rows)\nIf we want to focus only on departure delays of flights headed to Los Angeles, we need to first filter() the data for flights with that destination (dest == \"LAX\"). The departure delay for the LAX flights can then be summarized.\n\nlax_flights <- flights %>%\n  filter(dest == \"LAX\")\n\nlax_flights %>% \n  select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\nLet’s decipher these two commands (It’s common to add a break to a new line after %>% to help readability).\n\nCommand 1: Take the flights data frame, filter() for flights headed to LAX, and save the result as a new data frame called lax_flights.\n\n== means “if it’s equal to”. (notice that there are TWO equals signs)\nLAX is in quotation marks since it is a character string.\n\nCommand 2: Basically the same call for summarizing the departure delay.\n\nNotice that if we only want the summary of dep_delay for the LAX flights (and we don’t need to keep a copy of the dataset), we can perform the above tasks by combining them into fewer steps:\n\nflights %>%\n  filter(dest == \"LAX\") %>%\n  select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\n\nLogical operators:  Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so we use the filter() function and a series of logical operators. The most commonly used logical operators for data analysis are as follows:\n\n== means “equal to”\n!= means “not equal to”\n> or < means “greater than” or “less than”\n>= or <= means “greater than or equal to” or “less than or equal to”\n\n\n\n\nsummarize() (calculate statistics)\nWe can also obtain numerical summaries for these flights:\n\nlax_flights %>%\n  summarize(mean_dd = mean(dep_delay, na.rm = TRUE), \n            median_dd = median(dep_delay, na.rm = TRUE), n_dd = n())\n\n# A tibble: 1 × 3\n  mean_dd median_dd  n_dd\n    <dbl>     <dbl> <int>\n1    9.40        -1 16174\n\n\nNote that in the summarize() function we created a list of three different numerical summaries that we were interested in. The names of these elements are user defined, like mean_dd, median_dd, n_dd, and you could customize these names as you like (but don’t use spaces in your names). Calculating summary statistics also require that you know the function calls. Note that n() reports the sample size.\n\nSummary statistics:  Some useful function calls for summary statistics for a single numerical variable are as follows:\n\nmean()\nmedian()\nsd()\nIQR()\nmin()\nmax()\n\nNote that each of these functions take a single vector as an argument, and returns a single value.\n\nTwo functions you may not be familiar with (and that we will see in more detail in coming weeks) include: \\[\\mbox{sd} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2}\\] \\[\\mbox{IQR} = 75\\% - 25\\%\\]\nWe can also filter based on multiple criteria. Suppose we are interested in flights headed to San Francisco (SFO) in February:\n\nsfo_feb_flights <- flights %>%\n  filter(dest == \"SFO\", month == 2)\n\nNote that we can separate the conditions using commas if we want flights that are both headed to SFO and in February. If we are interested in either flights headed to SFO or in February we can use the | instead of the comma.\n\nCreate a new data frame that includes flights headed to SFO in February, and save this data frame as sfo_feb_flights. How many flights meet these criteria?\n\n\nsfo_feb_flights %>%\n  summarize(n())\n\n# A tibble: 1 × 1\n  `n()`\n  <int>\n1   791\n\n\n\nDescribe the distribution of the arrival delays of these flights using summary and/or appropriate summary statistics.\n\n\nsfo_feb_flights %>%\n  summarize(arrdelmean = mean(arr_delay, na.rm=TRUE), \n            arrdelsd = sd(arr_delay, na.rm=TRUE),\n            arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  arrdelmean arrdelsd arrdelmed arrdeliqr\n       <dbl>    <dbl>     <dbl>     <dbl>\n1      -9.14     31.4       -13        29\n\n\n\n\ngroup_by() (group before summarizing)\nAnother useful technique is quickly calculating summary statistics for various groups in your data frame. For example, we can modify the above command using the group_by() function to get the same summary stats for each origin airport:\n\nsfo_feb_flights %>%\n  group_by(origin) %>%\n  summarize(median_dd = median(dep_delay, na.rm=TRUE), \n            iqr_dd = IQR(dep_delay, na.rm=TRUE), n_flights = n())\n\n# A tibble: 2 × 4\n  origin median_dd iqr_dd n_flights\n  <chr>      <dbl>  <dbl>     <int>\n1 EWR            0      9       194\n2 JFK           -2      8       597\n\n\nHere, we first grouped the data by origin, and then calculated the summary statistics.\n\nCalculate the median and interquartile range for arr_delay of flights in in the sfo_feb_flights data frame, grouped by carrier. Which carrier has the most variable arrival delays (as measured by IQR)?\n\n\nsfo_feb_flights %>% \n  group_by(carrier) %>%\n  summarize(arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE)) %>%\n  arrange(desc(arrdelmed))\n\n# A tibble: 5 × 3\n  carrier arrdelmed arrdeliqr\n  <chr>       <dbl>     <dbl>\n1 AA             -7      35  \n2 UA             -9      27  \n3 B6            -11      25.5\n4 VX            -20      23  \n5 DL            -24      27.5\n\n\n\n\n\narrange() departure delays over months\nWhich month would you expect to have the highest average delay departing from an NYC airport?\nLet’s think about how we would answer this question:\n\nFirst, calculate monthly averages for departure delays. With the new language we are learning, we need to\n\ngroup_by() months, then\nsummarize() mean departure delays.\n\nThen, we need to arrange() these average delays in desc()ending order\nAfter you’ve filled in the blanks, change the chunk option to eval = TRUE to get the code to knit.\n\n\nflights %>%\n  group_by(___) %>%\n  summarize(mean_dd = mean(___, na.rm=TRUE)) %>%\n  arrange(desc(___))\n\n\n\nOn time departure rate for NYC airports\nSuppose you will be flying out of NYC and want to know which of the three major NYC airports has the best on time departure rate of departing flights. Suppose also that for you a flight that is delayed for less than 5 minutes is basically “on time”. You consider any flight delayed for 5 minutes of more to be “delayed”.\nIn order to determine which airport has the best on time departure rate, we need to\n\nfirst classify each flight as “on time” or “delayed”,\nthen group flights by origin airport,\nthen calculate on time departure rates for each origin airport (note that the average of zeros and ones is actually a proportion or rate!),\nand finally arrange the airports in descending order for on time departure percentage.\n\n\nmutate() (create a new variable)\nLet’s start with classifying each flight as “on time” or “delayed” by creating a new variable with the mutate() function.\n\nflights <- flights %>%\n  mutate(dep_type = case_when(\n    dep_delay < 5 ~ \"on time\",\n    TRUE ~ \"delayed\"))\n\nThe first argument in the mutate() function is the name of the new variable we want to create, in this case dep_type. Then if dep_delay < 5 we classify the flight as \"on time\" and \"delayed\" if not, i.e., if the flight is delayed for 5 or more minutes.\nNote that we are also overwriting the flights data frame with the new version of this data frame that includes the new dep_type variable.\nWe can handle all the remaining steps in one code chunk (convince yourself that you understand why a mean() is actually a proportion):\n\nflights %>%\n  group_by(origin) %>%\n  summarize(ot_dep_rate = mean(dep_type == \"on time\", na.rm=TRUE)) %>%\n  arrange(desc(ot_dep_rate))\n\n# A tibble: 3 × 2\n  origin ot_dep_rate\n  <chr>        <dbl>\n1 LGA          0.706\n2 JFK          0.679\n3 EWR          0.622\n\n\n\nIf you were selecting an airport (of the three NYC airports in the dataset) simply based on on time departure percentage, which NYC airport would you choose to fly out of? (How did you define “on time”? 0 min? 5 min? Something else?)"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#to-turn-in",
    "href": "slides/lab1_m58_s23_intro_to_data.html#to-turn-in",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "To Turn In",
    "text": "To Turn In\n\nHave you been able to meet with your learning community yet? Anything I should know about getting things up and running?\nMutate the data frame so that it includes a new variable that contains the average speed, avg_speed traveled by the plane for each flight (in mph). Hint: Average speed can be calculated as distance divided by number of hours of travel, and note that air_time is given in minutes.\nAnother useful filtering helper function is between(). What does it do? Use it to find flights that arrived between 0 and 60 minutes late. How many such flights are there? (Hint: you’ll need to use between() inside one of the wrangling verbs. In the console (down below), type ?between to learn what the function does and what the arguments are for the function.)\nSuppose you really dislike departure delays, and you want to schedule your travel in a month that minimizes your potential departure delay leaving NYC. One option is to choose the month with the lowest mean departure delay. Another option is to choose the month with the lowest median departure delay. What are the pros and cons of these two choices? Find the month with the lowest median departure delay and the month with the lowest mean departure delay. Which month do you choose?\nWhich month has the highest average arrival delay from an NYC airport? What about the highest median arrival delay? Which of these measures is more reliable for deciding which month(s) to avoid flying if you really dislike delayed flights.\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone."
  },
  {
    "objectID": "slides/MA58_HW2s23.html",
    "href": "slides/MA58_HW2s23.html",
    "title": "HW 2 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\ncalculating & interpreting correlations\ncalculating & interpreting linear model\n\nNote: you’ll need many of the skills covered in lab 2 to complete the assignment! After Tuesday, the solutions to Lab 2 will be posted on Sakai.\n\nQ1. LC Q\nDescribe one thing you learned from someone in your learning commmunity this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Breaking Ice1\nNenana is a small, interior Alaskan town that holds a famous competition to predict the exact moment that “spring arrives” every year. The arrival of spring is defined to be the moment when the ice on the Tanana River breaks, which is measured by a tripod erected on the ice with a trigger to an official clock. The minute at which the ice breaks has been recorded in every year since 1917. For example, the dates and times for the years 2000-2004 were:\n\n\n\n\n\n\n\n\n\n\n2000\n2001\n2002\n2003\n2004\n\n\n\n\nMay 1, 10:47am\nMay 8, 1:00pm\nMay 7, 9:27pm\nApril 29, 6:22pm\nApril 24, 2:16pm\n\n\n\nThe data file NenanaIceBreak.txt contains all of the data since 1917. Scientists have examined these data for evidence of global warming, which would suggest that the ice break day should be tending to occur earlier as time goes on.\n\nExamine a scatterplot of the day in which the ice broke (date coded in column 7 with April 1 = 1) vs. year. Does it reveal any association between the two variables? In other words, is there any indication that the day on which spring begins is changing over time? Explain.\n\n(n.b., Don’t worry about the earlier columns coded with month and year. For this problem, the focus is on the number of days since April 1.)\n\n# data available from a URL (not an R package)\nice <- read_delim(\"http://www.rossmanchance.com/iscam2/data/NenanaIceBreak.txt\", \"\\t\")\n\n\nDetermine and report the regression line for predicting ice break day from year. Also calculate the correlation coefficient and the value of \\(R^2\\). Comment on what these reveal, including an interpretation of the slope coefficient.\nLet’s say that the conclusion is strongly that the slope is non-zero (as measured by the p-value, but we haven’t learned that yet). Would you say that it reveals evidence of a strong association or strong evidence of an association? Explain.\nDo the data suggest that one can make better predictions by taking year into account, rather than simply using the average of the ice break days? Explain.\nWhat date would the regression model predict for the ice break-up in the year 2005? What about 2020? Explain why you should regard these predictions cautiously.\n\n\n\nQ3. Identify relationships, Part II., IMS Section 7.5 #42\nFor each of the six plots (see text), identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable.\n\n\nQ4. Graduate degrees and salaries., IMS Section 7.5 #143\nWhat would be the correlation between the annual salaries of people with and without a graduate degree at a company if for a certain type of position someone with a graduate degree always made\n\n$5,000 more than those without a graduate degree?\n25% more than those without a graduate degree?\n15% less than those without a graduate degree?\n\n\n\nQ5. The Coast Starlight, regression. IMS Section 7.5 #214\nHint: look at the text for the scatterplot and also for how to compute the slope and intercept by hand. By hand computing will not be a regular thing, but it doesn’t hurt to do it once so as to understand the computation.\nThe Coast Starlight Amtrak train runs from Seattle to Los Angeles. The scatterplot below displays the distance between each stop (in miles) and the amount of time it takes to travel from one stop to another (in minutes). The mean travel time from one stop to the next on the Coast Starlight is 129 mins, with a standard deviation of 113 minutes. The mean distance traveled from one stop to the next is 108 miles with a standard deviation of 99 miles. The correlation between travel time and distance is 0.636.\n\nWrite the equation of the regression line for predicting travel time.\nInterpret the slope and the intercept in this context.\nCalculate \\(R^2\\) of the regression line for predicting travel time from distance traveled for the Coast Starlight, and interpret \\(R^2\\) in the context of the application.\nThe distance between Santa Barbara and Los Angeles is 103 miles. Use the model to estimate the time it takes for the Starlight to travel between these two cities.\nIt actually takes the Coast Starlight about 168 mins to travel from Santa Barbara to Los Angeles. Calculate the residual and explain the meaning of this residual value.\nSuppose Amtrak is considering adding a stop to the Coast Starlight 500 miles away from Los Angeles. Would it be appropriate to use this linear model to predict the travel time from Los Angeles to this point?\n\n\npraise()\n\n[1] \"You are outstanding!\"\n\n\n\n\n\n\n\n\nFootnotes\n\n\nFrom ISCAM, HW 5.39↩︎\nhttps://openintro-ims.netlify.app/model-slr.html#chp7-exercises↩︎\nhttps://openintro-ims.netlify.app/model-slr.html#chp7-exercises↩︎\nhttps://openintro-ims.netlify.app/model-slr.html#chp7-exercises↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html",
    "href": "slides/lab2_m58_s23_2quant.html",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "",
    "text": "Building on the work we’ve done this week to understand quantitative variables, today’s lab will focus on creating linear models and creating scatterplots. First, however, we build up to scatterplots by focusing on how layers of plots work in the ggplot2 package.\nThe goals for today include:"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab2_m58_s23_2quant.html#advice-for-turning-in-the-assignment",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe assignment part of the lab is ONLY the last six questions at the very bottom. However, the commands in the first half of the assignment are key to doing the second half.\nSave the .Rmd file somewhere you can find it. Don’t keep everything in your downloads folder. Maybe make a folder called StatsHW or something. That folder could live on your Desktop. Or maybe in your Dropbox."
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#getting-started",
    "href": "slides/lab2_m58_s23_2quant.html#getting-started",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Getting started",
    "text": "Getting started\nA really great series of examples to work through by Hadley Wickham: https://r4ds.had.co.nz/data-visualisation.html\nSome things to notice:\n\nwhen layering graph pieces, use +. (When layering for data wrangling, use %>%.)\ngeom_XXX() will put the XXX-type-of-plot onto the graph.\naes() is the function which takes the data columns and puts them onto the graph. aes() is used only with data columns and you always need it if you are working with data variables.\nA full set of types of plots is given here: https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf (and in many other places online).\n\n\nLoad packages\nIn this lab we will explore the data using ggplot() which is included in the tidyverse package. We’ll start by looking at the penguin data in the palmerpenguin package. Remember, you’ll need to install the package the first time you use it!\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n\nLayers\nA ggplot has layers. And each layer is connected by the + symbol. We might want to know if flipper_length_m is correlated to body_mass_g. In order to investigate, we’ll create a scatterplot. Every ggplot has a minimum of two layers. The first layer sets the plot, the second later draws the points.\nRun the first layer first:\n\npenguins %>%\n  ggplot()\n\n\n\n\n\n\n\n\nAnd now add some points (note the difference in the use of %>% vs +):\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g))"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#lets-make-some-plots",
    "href": "slides/lab2_m58_s23_2quant.html#lets-make-some-plots",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Let’s make some plots!",
    "text": "Let’s make some plots!\n\naes()\nThe aes() function plays a very specific role in ggplots, and it is worth discussing. The idea behind aes() (which stands for “aesthetic”, but I don’t find that word helpful) is to pull out the columns of the dataframe (the variables).\nLet’s say we want to color the points based on the island from which the penguins came.\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, color = island))\n\n\n\n\n\n\n\n\nOr maybe what we want is to color all of the points blue:\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, color = \"blue\"))\n\n\n\n\n\n\n\n\nWait, that didn’t work. Why not? It is because blue is not a column in the dataset! (And no, it won’t help if you remove the quotes, but go ahead and try it to see what happens.)\nRemember that the aes() function will only and always pull out the columns of the dataset! So if you want the points to be blue, go ahead and make them blue, just don’t try to color inside the aes()\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g), color = \"blue\")\n\n\n\n\n\n\n\n\nSome other things to try … “size = …” or “shape = …” within the aes() will give you different plotting symbols. Try it out! You might play around with setting size inside aes() with a variable, and setting size outside aes() with a number.\n\n\nAdding layers\nThere are so many layers we can add! Let’s try some…\n\nReformating the title of the x-axis\nSee the code below. What do you think the layer ylab() does? What do you think the layer ggtitle() does? Try them out!\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g)) +\n  xlab(\"Length of Flipper in mm\")\n\n\n\n\n\n\n\n\n\n\nAdding a regression line\nThe “method” used for adding a line to the plot is “lm” which stands for linear model. Try another option: method = \"loess\". Also, se = FALSE means that we don’t want an error bound on the line. What happens when you set se = TRUE?\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n  xlab(\"Length of Flipper in mm\")\n\n$x\n[1] \"Length of Flipper in mm\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\nWhere does the aes() information go?\nConsider the following information: aes(x = flipper_length_mm, y = body_mass_g, color = island). Type that phrase into the ggplot as follows.\n\nAdd only to “A”\nAdd only to “B”\nAdd only to “C”\nAdd to “B” and “C”\nAdd to “B” and “C” but remove the color = islands for “C”\nAdd to “B” and “C” but remove the color = islands for “B”\n\nAlso, the code below won’t run until after you change the chunk to eval = TRUE:\n\npenguins %>%\n  ggplot(\"A\") +\n  geom_point(\"B\") +\n  geom_smooth(\"C\", method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#lets-calculate-some-statistics",
    "href": "slides/lab2_m58_s23_2quant.html#lets-calculate-some-statistics",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Let’s calculate some statistics!!",
    "text": "Let’s calculate some statistics!!\n\nLinear model based on the least squares regression\nRunning a linear model and/or calculating the correlation is a matter of learning the linear model syntax. For most (all?) of our models we will use the following syntax to tell the model which variable is the response variable and which variable is the explanatory variable:\nresponse variable ~ explanatory variable\nThe linear model takes the data as a later argument, so we use the period . to indicate where the dataset gets piped into the function.\nThe only thing we’ve talked about with respect to the linear model is the first column of the output (called “estimate”). The intercept is \\(b_0 = -5780.83\\) g, and the slope is \\(b_1 = 49.69\\) g/mm.\n\npenguins %>%\n  lm(body_mass_g ~ flipper_length_mm, data = .) %>%\n  tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\n\n\nCorrelation\nThe correlation is always calculated across pairs of variables, but R can calculate the correlation for lots of pairs at a time.\n\nThere are explicit instructions on how to calculate the correlation when there are missing values. Here we used use = \"pairwise.complete.obs\".\nWhy are the diagonal entries all equal to 1?\n\n\npenguins %>%\n  select(body_mass_g, flipper_length_mm, bill_length_mm, bill_depth_mm) %>%\n  cor(use = \"pairwise.complete.obs\")\n\n                  body_mass_g flipper_length_mm bill_length_mm bill_depth_mm\nbody_mass_g         1.0000000         0.8712018      0.5951098    -0.4719156\nflipper_length_mm   0.8712018         1.0000000      0.6561813    -0.5838512\nbill_length_mm      0.5951098         0.6561813      1.0000000    -0.2350529\nbill_depth_mm      -0.4719156        -0.5838512     -0.2350529     1.0000000\n\n\nTo calculate the correlations by group, use group_by() and summarize():\n\npenguins %>%\n  group_by(island) %>%\n  summarize(r_mass_flip = cor(body_mass_g, flipper_length_mm, use = \"pairwise.complete.obs\")) \n\n# A tibble: 3 × 2\n  island    r_mass_flip\n  <fct>           <dbl>\n1 Biscoe          0.877\n2 Dream           0.525\n3 Torgersen       0.436"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#some-graphics-bells-and-whistles-you-dont-need-to-know",
    "href": "slides/lab2_m58_s23_2quant.html#some-graphics-bells-and-whistles-you-dont-need-to-know",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Some graphics bells and whistles (you don’t need to know)",
    "text": "Some graphics bells and whistles (you don’t need to know)\n\nAdding the value of the correlation\nUsing the ggpubr package, we can add the correlation to the plot:\n\n# sometimes people have trouble loading the ggpubr package\n# if you are having trouble with ggpubr, change the code chunk\n# to:  ```{r eval = FALSE}\nlibrary(ggpubr)\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  stat_cor(aes(label = ..r.label..)) +\n  xlab(\"Length of Flipper in mm\")\n\n\n\n\n\n\n\n\n\n\nFaceting plots\nOften the visualization is much better if the plots are distinct from one another. We use faceting to create different plots.\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\nWe can facet by more than one variable:\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(year~island)"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#to-turn-in",
    "href": "slides/lab2_m58_s23_2quant.html#to-turn-in",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "To Turn In",
    "text": "To Turn In\nThe data which will be used on the part of the lab to be turned in (called gapminder) can be found in the package dslabs.\nLet’s load the packages.\n\nlibrary(tidyverse)  # ggplot lives in the tidyverse\nlibrary(dslabs)  # dataset for the lab\ndata(gapminder)\n\n\nThe data\nGapminder is a creation of Hans Rosling and his team to collect and visualize country level information over time. The dslabs R package and gapminder dataset provide a few interesting variables for us to work with.\n\nnames(gapminder)\n\n[1] \"country\"          \"year\"             \"infant_mortality\" \"life_expectancy\" \n[5] \"fertility\"        \"population\"       \"gdp\"              \"continent\"       \n[9] \"region\"          \n\n\n\nglimpse(gapminder)\n\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…\n\n\n\nQ1. LC Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2.\nCreate a scatterplot with life expectancy on the y-axis and fertility rate on the x-axis. Color the points based on their continent. Change the x and y axis labels to be more descriptive. (Note: the plot isn’t going to be very beautiful.)\n\n\nQ3.\nMake two graphs which are similar to the plot in the previous questions. First, use only observations from 1962 by filter()ing on year. Make a second scatterplot for observations from 2015. (It is possible, and fun!, to use filter() to get both years and then to apply facet_wrap() to create side-by-side plots.)\nName one difference and one similarity in the pair of plots from 1962 and 2015.\n\n\nQ4.\nOn each of the separate plots (1962 and 2015), add the linear model to the plot (separate linear model for each continent).\nAlso add (to each of the 1962 and 2015 plots separately) a single linear regression which is fit on all countries together simultaneously. Color the overall regression line black.\n\n\nQ5.\nCalculate the correlation between life expectancy and fertility separately for the two years above and each continent (that is, calculate 10 different correlation values). Provide 1-2 sentences describing how the numeric correlation values are consistent with what you observed in the plot above.\nHint1: use filter() only once to get both years using the “or” command: |.\nHint2: group_by() can take two arguments (separated by a comma)!\nYour code should be only four lines and look something like this.\ndata %>%\n   filter(...) %>%\n   group_by(...) %>%\n   summarize(...)\n\n\nQ6.\nCalculate the intercept and slope describing the least squares regression model regressing life expectancy (the response variable) on fertility (the explanatory variable). Use 2015 and all continents. Write down the full model (hint: when writing the model down, either use a hat or add a residual).\n\npraise()\n\n[1] \"You are great!\"\n\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Slack! The sooner (and more often) questions get asked, the better for everyone."
  },
  {
    "objectID": "slides/lab3_m58_s23_infer.html",
    "href": "slides/lab3_m58_s23_infer.html",
    "title": "Lab 3 - Math 58B: Randomization Test with infer",
    "section": "",
    "text": "Building on the work we’ve done this week to run a hypothesis test, we will use the infer package to complete an entire randomization test.\nThe goals for today include:"
  },
  {
    "objectID": "slides/lab3_m58_s23_infer.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab3_m58_s23_infer.html#advice-for-turning-in-the-assignment",
    "title": "Lab 3 - Math 58B: Randomization Test with infer",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe assignment part of the lab is ONLY the last six questions at the very bottom. However, the commands in the first half of the assignment are key to doing the second half.\nSave the .Rmd file somewhere you can find it. Don’t keep everything in your downloads folder. Maybe make a folder called StatsHW or something. That folder could live on your Desktop. Or maybe in your Dropbox."
  },
  {
    "objectID": "slides/lab3_m58_s23_infer.html#getting-started",
    "href": "slides/lab3_m58_s23_infer.html#getting-started",
    "title": "Lab 3 - Math 58B: Randomization Test with infer",
    "section": "Getting started",
    "text": "Getting started\nThe infer vignette is excellent: https://infer-dev.netlify.app/index.html\nAs we go through the lab today, focus on the names of the function to make sure that you connect the name of the function to the action of the function.\n\nLoad packages / data\nIn this lab we will use new syntax from the infer package. The syntax is meant to focus understanding on the hypothesis testing process. So for each line, pay attention to what the code is doing.\nThe data come from a randomized clinical trial to discern the difference between a sugar gargle versus licorice gargle after undergoing elective thoracic surgery on the amount of coughing post-surgery (reference and data here: https://www.causeweb.org/tshs/licorice-gargle/).\n\nlibrary(infer)\nlicorice_study <- readr::read_csv(\"http://pages.pomona.edu/~jsh04747/courses/math58/Licorice.csv\") %>%\n  mutate(gargle = case_when(\n    treat == 0 ~ \"sugar\",\n    treat == 1 ~ \"licorice\")) %>%\n  mutate(cough = case_when(\n    pod1am_cough == 0 ~ \"none\",\n    TRUE ~ \"some\")) %>%\n  select(gargle, cough)\n\nlicorice_study %>%\n  table()\n\n          cough\ngargle     none some\n  licorice   86   32\n  sugar      68   49\n\n\n\nLogic for Hypothesis Testing (spoiler)\n\nWe know that the study was an experiment, so there should be no systematic differences (in other variables) between the group who received the sugar versus the licorice gargle.\nWe hope to rule out random chance as the reason for the difference in proportions of the who is coughing after surgery. (We hope to reject the null hypothesis.)\nIf we can reject the null hypothesis, we conclude that the gargle type and coughing outcome are not independent. That is, the type of gargle used changes the probability that the patient will have coughing after surgery.\n\n\n\n\nA randomization test\n\n# to control the randomness\nset.seed(47)\n\nlicorice_study %>% head()\n\n# A tibble: 6 × 2\n  gargle   cough\n  <chr>    <chr>\n1 licorice none \n2 licorice none \n3 licorice none \n4 licorice none \n5 licorice none \n6 licorice none \n\nlicorice_study %>% table()\n\n          cough\ngargle     none some\n  licorice   86   32\n  sugar      68   49\n\n\n\nStep 1. Observed Statistic\nThe first thing we need to do is to find the observed statistic of interest (here the difference in sample proportions). Note that R is happy to act as a calculator, but we’re going to use the syntax associated with the test to get the value of interest.\nWell, okay, first as a calculator:\n\n(49/117) - (32/118)\n\n[1] 0.147617\n\n\nNow using the infer syntax, we’ll specify the variables of interest (which is response and which is explanatory?). Also, we’ll need to specify what a “success” means and the order of subtraction. Note that we get the same value as we did when using R as a calculator.\n\ndiff_obs <- licorice_study %>%\n    specify(cough ~ gargle, success = \"some\") %>%\n    calculate(stat = \"diff in props\", order = c(\"sugar\", \"licorice\"))\n\ndiff_obs\n\nResponse: cough (factor)\nExplanatory: gargle (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 0.148\n\n\n\n\nStep 2. Shuffle the data under H0\nThe point of the hypothesis test structure is to have an understanding of what types of values might be seen just by chance (if the type of gargle really wasn’t doing anything). The idea is that 81 people were going to have some coughing after surgery anyway (regardless of gargle) – that’s the null hypothesis! And we expect some variability in which group those 81 people end up. How big are the differences in proportions? is 14.8% big? or is it small?\nHint: go back to this applet to see how the shuffling works! http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm?FET=1\nA note on the code… pay attention to the steps here:\n\nspecify() gives the appropriate information on the variable types. Always: responsevariable ~ explanatoryvariable\nhypothesize() gives the null hypothesis of interest. Here we are performing a test of independence, but that will change over the semester as we do different types of tests.\ngenerate() swaps things around as if the null hypothesis were true. Here we are permuting the data (randomly reassigning it) as if the gargle didn’t have an impact.\n\nTake a look at the output of the first three steps, before calculating the difference in proportions. It’s hard to tell just by looking at the dataframe, but really what you see is that different people have been assigned to different treatments (sugar or licorice).\n\nlicorice_study %>%\n  specify(cough ~ gargle, success = \"some\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 4, type = \"permute\") %>%   # set reps=4 just to see the process\n  head()\n\n# A tibble: 6 × 3\n# Groups:   replicate [1]\n  cough gargle   replicate\n  <fct> <fct>        <int>\n1 none  licorice         1\n2 none  licorice         1\n3 some  licorice         1\n4 none  licorice         1\n5 none  licorice         1\n6 some  licorice         1\n\n\nThe last step ties it all together:\n\ncalculate() finds the statistic (here our statistic is the difference in proportions) for each of the permuted datasets.\n\nKeep all of those differences, and take a look at them. Is 14.8% is big? or is it small? can you tell?\n\nset.seed(4774)\nnull_licorice <- licorice_study %>%\n  specify(cough ~ gargle, success = \"some\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%  # need a lot of reps to truly understand\n  calculate(stat = \"diff in props\", order = c(\"sugar\", \"licorice\"))\n\nnull_licorice %>% head()\n\nResponse: cough (factor)\nExplanatory: gargle (factor)\nNull Hypothesis: independence\n# A tibble: 6 × 2\n  replicate     stat\n      <int>    <dbl>\n1         1 -0.0566 \n2         2 -0.125  \n3         3 -0.00558\n4         4 -0.00558\n5         5  0.0285 \n6         6  0.0114 \n\n\n\n\nStep 3. Look at all the differences\nFortunately for us, we saved all the differences in proportions into an object that was called null_licorice. We can visuzlize() the differences!\n\nvisualize(null_licorice) \n\n\n\n\n\n\n\n\nIt is still a little bit hard to tell where that 14.8% falls. It doesn’t seem way outside the range, but it is in the tails. Let’s plot the observed difference on the plot of null differences. Remember, when adding layers to a plot we use + instead of %>%.\nFun activity: the direction argument has different options. Try them out: direction = \"less\" or direction = \"two-sided\". What happens to the plot?\n\nvisualize(null_licorice) +\n  shade_p_value(obs_stat = diff_obs, direction = \"greater\")\n\n\n\n\n\n\n\n\n\n\nStep 4. Calculate the p-value\nRecall that the p-value is the probability of the observed difference or more extreme if the variables are independent (that is, if the null hypothesis is true and licorice doesn’t impact whether or not you are more likely to cough post-surgery).\nFun activity: the direction argument has different options. Try them out: direction = \"less\" or direction = \"two-sided\". What happens to the p-value?\n\nnull_licorice %>%\n  get_p_value(obs_stat = diff_obs, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.017\n\n\n\n\nStep 5. Make a conclusion\nThe p-value here is 0.034. We’re saying that if those 81 people were going to cough anyway, regardless of gargle, only 3.4% of the time would so many of them have landed in the sugar group just by chance. The probability of the observed difference seems pretty small. It makes us think that chance was probably not the mechanism that put so many patients into the sugar group. Instead, this process makes us believe that it was actually the licorice (in contrast to the sugar) that reduced the coughing.\nBecause this was a randomized trial, all other characteristics of the patients are balanced out. And we seem to have ruled out chance as the mechanism.\nConclusion: the difference in probability of coughing is due to the choice of gargle. That is, licorice gargle reduces the probability of coughing post-operative for this particular elective thoracic surgery."
  },
  {
    "objectID": "slides/lab3_m58_s23_infer.html#to-turn-in",
    "href": "slides/lab3_m58_s23_infer.html#to-turn-in",
    "title": "Lab 3 - Math 58B: Randomization Test with infer",
    "section": "To Turn In",
    "text": "To Turn In\n\nThe data\nThe data for the write-up part of the lab is on a diabetes clinical trial1 and available from the openintro package. (Note: in the original study there were three treatments, today we’ll just compare metformin with a lifestyle-intervention program.)\n\nThree treatments were compared to test their relative efficacy (effectiveness) in treating Type 2 Diabetes in patients aged 10-17 who were being treated with metformin. The primary outcome was lack of glycemic control (or not); lacking glycemic control means the patient still needed insulin, which is not the preferred outcome for a patient.\n\n\nlibrary(infer)  # for doing the randomization test\nlibrary(openintro)  # home of the dataset for the lab\ndata(diabetes2)\n\ndiabetes <- diabetes2 %>%\n  filter(treatment != \"rosi\") %>%\n  mutate(treatment = droplevels(treatment))\n\ndiabetes %>% table()\n\n           outcome\ntreatment   failure success\n  lifestyle     109     125\n  met           120     112\n\n\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. The study\nAnswer the following questions with respect to the study (feel free to read about the study more on your own, or you might type into the console: ?diabetes2).\n\nWhat are the observational units?\nWhat are the variables? Label them as explanatory and response.\nIs it an experiment or an observational study?\nWas the treatment randomly assigned? If so, what might you conclude at the end of this lab? If not, what are you unable to conclude? Explain.\nWere the observational units randomly selected from the population? (Please specify the population which you think is most relevant here.). If so, what might you conclude at the end of the lab? If not what are you unable to conclude? Explain.\n\n\n\nQ3. Hypotheses\nWrite out the null and alternative hypotheses. Use words like diabetes in your claims. For this lab, let’s say you don’t have scientific background to think that one treatment over the other might be best. Instead, the goal is to find out whether there is a difference between the two treatments.\n\n\nQ4. Observed test statistic\nUsing the infer syntax, calculate the observed test statistic.\n\n\nQ5. Null test statistics\nCalculate 1000 null test statistics from 1000 different permutations of the data.\n\n\nQ6. Visualize\nUsing the results from Q5, make a histogram to visualize the observed test statistic. Include the observed statistic, and shade the area of the histogram which is “more extreme” from the observed statistic. (Remember that the hypothesis claim here is that the treatments are different.)\n\n\nQ7. p-value\nCalculate the p-value. Remember that the hypothesis claim here is that the treatments are different.\n\n\nQ8. Conclusion\nGive a conclusion about the hypothesis claims. Do you reject H0 or not? Do you have evidence for cause? To what population can you / can’t you generalize the results?\n\npraise()\n\n[1] \"You are amazing!\"\n\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone."
  },
  {
    "objectID": "slides/MA58_HW3s23.html",
    "href": "slides/MA58_HW3s23.html",
    "title": "HW 3 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\npractice setting up hypotheses\npractice conclusions from complete hypothesis test\npractice simulating under the scenario where the null hypothesis is true\n\nNote: you’ll need many of the skills covered in lab 3 to complete the assignment! After Wednesday, the solutions to Lab 3 will be posted on Sakai.\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Kissing the Right Way, (ISCAM, Inv 1.5)\n\nMost people are right-handed and even the right eye is dominant for most people. Researchers have long believed that late-stage human embryos tend to turn their heads to the right. German biopsychologist Onur Gunturkun (Nature 2003) conjectured that this tendency to turn to the right manifests itself in other ways as well, so he studied kissing couples to see if both people tended to lean to their right more often than to their left (and if so, how strong the tendency is). He and his researchers observed couples from age 13 to 70 in public places such as airports, train stations, beaches, and parks in the United States, Germany, and Turkey. They were careful not to include couples who were holding objects such as luggage that might have affected which direction they turned. We will assume these couples are representative of the overall decision making process when kissing.\n\n\nIn total, 124 kissing pairs were observed with 80 couples leaning right.\n\n\nWhat are the\n\n\nobservational units?\nthe variable? is it categorical or quantitative?\nthe statistic?\nthe parameter?\n\n\nDo the data from the kissing study provide convincing evidence that the probability of leaning right is smaller than 0.8?\n\n\nState the hypotheses\nreport the p-value [describing how you determined it]\nclarifying what it is in words / terms of the probability of …\ninterpret the strength of evidence against the null hypothesis).\n\nTo find the p-value, use the simulation structure from the infer package (but this time, note that the null hypothesis is different from lab 3 because we only have one variable!!!). Much of the simulation code is written below. You need to do two thing to the code: (1) toggle to eval=TRUE and (2) fill in all the blank spaces below.\n\nlibrary(infer)\n\n# to control the randomness\nset.seed(47)\n\n# first create a data frame with the kissing data\nkissing <- data.frame(side = c(rep(\"right\", ___ ), rep(\"left\", ___ )))\n\n# then find the proportion who kiss right \np_obs_right <- kissing %>%\n    specify(response = ___ , ___ = \"right\") %>%  # note, only one variable here!!!\n    calculate(stat = \"prop\") \n\np_obs_right   #  does the computer print the correct number?\n\n# now apply the framework to get the p-value\nnull_kiss <- ___  %>%\n  specify(response = ___ , success = \"___ \") %>%\n  hypothesize(null = \"point\", p = .8) %>%    # new null hypothesis!  Here H0: p = 0.8\n  generate(reps = 1000, type = \"simulate\") %>% \n  calculate(stat = \"prop\")\n\n# visualize the null sampling distribution\nvisualize(___ ) +\n  shade_p_value(obs_stat = ___ , direction = \"___ \")\n\n# calculate the actual p-value\nnull_kiss %>%\n  get_p_value(obs_stat = ___ , direction = \"___ \")\n\n\nDo the data from the kissing study provide convincing evidence that the probability of leaning right differs from 0.8? (Aha, the alternative hypothesis has changed!)\n\n(Use the code and instructions from part (b).)\n\nReconsider part (b), the one-sided test: how would your results have differed if you had chosen “kiss left” instead of “kiss right”? Explain in words and re-run the code providing a full conclusion to the study (in words).\nReconsider part (b), again the one-sided test: how would your results have differed if you had the same proportional data but one quarter of the observations (that is, data were 20 right kissers out of 31)? Explain in words and re-run the code providing a full conclusion to the study (in words).\n\n\n\n\nQ3. Side effects of Avandia\n\nRosiglitazone is the active ingredient in the controversial type 2 diabetes medicine Avandia and has been linked to an increased risk of serious cardiovascular problems such as stroke, heart failure, and death. A common alternative treatment is pioglitazone, the active ingredient in a diabetes medicine called Actos. In a nationwide retrospective observational study of 227,571 Medicare beneficiaries aged 65 years or older, it was found that 2,593 of the 67,593 patients using rosiglitazone and 5,386 of the 159,978 using pioglitazone had serious cardiovascular problems. These data are summarized in the contingency table below. [D.J. Graham et al. “Risk of acute myocardial infarction, stroke, heart failure, and death in elderly Medicare patients treated with rosiglitazone or pioglitazone”. In: JAMA 304.4 (2010), p. 411. issn: 0098-7484.]\n\n\n\n\n\n\nCardiovascular\nproblems\n\n\n\n\n\n\n\nyes\nno\ntotal\n\n\nTreatment\nRosiglitazone\n2,593\n65,000\n67,593\n\n\n\nPioglitazone\n5,386\n154,592\n159,978\n\n\n\nTotal\n7,979\n219,592\n227,571\n\n\n\nDetermine if each of the following statements is true or false. If false, explain why. Be careful: The reasoning may be wrong even if the statement’s conclusion is correct. In such cases, the statement should be considered false.\n\nSince more patients on pioglitazone had cardiovascular problems (5,386 vs. 2,593), we can conclude that the rate of cardiovascular problems for those on a pioglitazone treatment is higher.\nThe data suggest that diabetic patients who are taking rosiglitazone are more likely to have cardiovascular problems since the rate of incidence was (2,593 / 67,593 = 0.038) 3.8% for patients on this treatment, while it was only (5,386 / 159,978 = 0.034) 3.4% for patients on pioglitazone.\nThe fact that the rate of incidence is higher for the rosiglitazone group proves that rosiglitazone causes serious cardiovascular problems.\nBased on the information provided so far, we cannot tell if the difference between the rates of incidences is due to a relationship between the two variables or due to chance.\n\n[There is work to be done for part (d). Hint: with such a huge sample size, the simulation is slow, you might try only a few hundred reps.]\n\n\nQ4. Crime concerns in China.\n\nA 2013 poll of 563 people found that 24% of Chinese adults see crime as a very big problem. The standard error for this estimate, which can reasonably be modeled using a normal distribution, is SE = 1.8% (= 0.018)1. Suppose an issue will get special attention from the Chinese government if more than 1-in-5 Chinese adults express concern on an issue.\n\n\nConstruct hypotheses regarding whether or not crime should receive special attention by the Chinese government according to the 1-in-5 guideline. [Remember, each hypothesis is a separate claim (i.e., sentence, although it may look like a mathematical equation) about the population.]\nDiscuss the appropriateness of using a one-sided or two-sided test for this exercise. Consider: for this decision process, would we care about one or both directions?\nConsider the hypothesis testing framework (that is, the last few steps where a probability is found and a conclusion is made).\n\n\nWhat is the probability you would need to calculate for this problem?\nIf you can use any coin (that is, a coin with any particular probability of heads), how could you use the coin to calculate the desired probability? Be specific so someone could follow your instructions.\nWhat are the possible conclusion outcomes for the study given the probability calculation?\n\n\npraise()\n\n[1] \"You are mind-blowing!\"\n\n\n\n\n\n\n\nFootnotes\n\n\nEnvironmental Concerns on the Rise in China. September 19, 2013. Pew Research.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab4_m58_s23_BS.html",
    "href": "slides/lab4_m58_s23_BS.html",
    "title": "Lab 4 - Math 58B: Bootstrap Percentile Confidence Intervals",
    "section": "",
    "text": "Building on the work we’ve done this week to run a hypothesis test, we will use the infer package to complete an entire randomization test.\nThe goals for today include:"
  },
  {
    "objectID": "slides/lab4_m58_s23_BS.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab4_m58_s23_BS.html#advice-for-turning-in-the-assignment",
    "title": "Lab 4 - Math 58B: Bootstrap Percentile Confidence Intervals",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe assignment part of the lab is ONLY the last six questions at the very bottom. However, the commands in the first half of the assignment are key to doing the second half.\nSave the .Rmd file somewhere you can find it. Don’t keep everything in your downloads folder. Maybe make a folder called StatsHW or something. That folder could live on your Desktop. Or maybe in your Dropbox."
  },
  {
    "objectID": "slides/lab4_m58_s23_BS.html#getting-started",
    "href": "slides/lab4_m58_s23_BS.html#getting-started",
    "title": "Lab 4 - Math 58B: Bootstrap Percentile Confidence Intervals",
    "section": "Getting started",
    "text": "Getting started\nThe infer vignette is excellent: https://infer-dev.netlify.app/index.html\nThe syntax for today is very similar to last week, but today the goal is to create confidence intervals instead of concluding hypotheses. As we go through the lab today, focus on the names of the function to make sure that you connect the name of the function to the action of the function.\n\nLoad packages / data\nIn this lab we will continue to use the infer package. The syntax is meant to focus understanding on the bootstrapping process. So for each line, pay attention to what the code is doing.\nThe data come from a randomized clinical trial called Health Evaluation and Linkage to Primary Care.\n\nThe HELP study was a clinical trial for adult inpatients recruited from a detoxification unit. Patients with no primary care physician were randomized to receive a multidisciplinary assessment and a brief motivational intervention or usual care, with the goal of linking them to primary medical care.\n\nFor today, we won’t focus on the treatment variable, instead we will work to understand if those individuals in the study might be representative of a larger population. That is, we will build confidence intervals for parameters that describe the population about which we can generalize the results.\nNote: There are quite a few variables in this dataset which might be uncomfortable to discuss. Feel free to choose any variable that feels comfortable for you to analyze If you can’t find one, or if you feel uncomfortable engaging with this dataset in general, please let me know.\n\nlibrary(mosaic)  # data lives in the mosaic package\ndata(\"HELPfull\")\n\nThe task for today will focus on finding a confidence interval for a proportion parameter (where the population is given by the larger group from which these data were selected). You should choose a single variable from the dataset (there are 788 columns!) that is categorical but is coded as more than two levels. Note, choose a variable with the following constraints:\n\nat least 200 (non-missing) observations\noriginally coded with 3+ levels\nyour choice of “success” happens with frequency between 5% and 95%\n\nTo create the new variable of interest so that it has two levels and is represented by a character string (not a numeric value), we need the mutate() and case_when() functions.\nI chose the variable B11A which represents how often the participants get sick. I used “Definitely true” or “Mostly true” to indicate that they get sick easier than most people. (Note: type ?HELPfull to see all of the potential variables.)\n\n# Renaming the object so that the original data stays intact \nHELP <- HELPfull %>%\n  mutate(\n    sick = case_when(\n      B11A <= 2 ~ \"more\",\n      B11A >= 3 ~ \"nomore\")) %>%\n  drop_na(sick)\n\n  \n\n# check to make sure my code worked\nHELP %>%\n  select(sick, B11A) %>%\n  table(useNA = \"ifany\")\n\n        B11A\nsick       1   2   3   4   5\n  more    97 166   0   0   0\n  nomore   0   0 220 401 581\n\n# without the last info, I can't confirm that the NA values match:\n# check to make sure my code worked\nHELP %>%\n  select(sick, B11A) %>%\n  table()\n\n        B11A\nsick       1   2   3   4   5\n  more    97 166   0   0   0\n  nomore   0   0 220 401 581\n\n\n\nLogic for Bootstrapping\n\nBy resampling from the sample, the distribution (shape & spread / variability) of the sample proportions can be measured.\nThe confidence interval will be created by choosing the bootstrapped sample proportions from the tails of the bootstrap distribution.\nThe process will capture the true parameter in a given percent of datasets. Because we are working here with only one dataset, we have no idea whether we’ve captured the true parameter or not. We hope so!\n\n\n\nStep 1. Observed Statistic\nThe first thing we need to do is to find the observed statistic of interest (here the sample proportion of success). Note that R is happy to act as a calculator, but we’re going to use the syntax associated with the confidence interval creation to get the value of interest.\nWell, okay, first as a calculator:\n\n(97+166) / (97+166+220+401+581)\n\n[1] 0.1795222\n\n\nNow using the infer syntax, we’ll specify the variable of interest (only one variable today!). Also, we’ll need to specify what a “success” means. Note that we get the same value for the sample proportion as we did when using R as a calculator. Whew.\n\nHELP %>%\n    specify( response = sick, success = \"more\") %>%\n    calculate(stat = \"prop\")\n\nResponse: sick (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1 0.180\n\n\n\n\nStep 2. Resample the data with replacement (from the original sample)\nThe point of bootstrapping is to understand the variability of the sample proportions. We can understand that variability through taking new samples from the original dataset.\nHint: spend some time with the applets shown in class! * bootstrapping: http://www.lock5stat.com/StatKey/bootstrap_1_cat/bootstrap_1_cat.html * confidence intervals: http://www.rossmanchance.com/applets/2021/confsim/ConfSim.html\nA note on the code… pay attention to the steps here… note that we don’t have the hypothesize() step because we aren’t hypothesizing anything!\n\nspecify() gives the appropriate information on the variable types. With only one variable, the syntax is: response = yourvariablename.\ngenerate() resample from the original sample with replacement.\n\nTake a look at the output of the first three steps, before calculating the proportion. It’s hard to tell just by looking at the dataframe, but really what you see is that for each of the 4 replicates, new people are being selected into each resample.\n\nHELP %>%\n  specify(response = sick, success = \"more\") %>%\n  generate(reps = 4, type = \"bootstrap\") %>%   # set reps=4 just to see the process\n  head()\n\n# A tibble: 6 × 2\n# Groups:   replicate [1]\n  replicate sick  \n      <int> <fct> \n1         1 nomore\n2         1 nomore\n3         1 nomore\n4         1 nomore\n5         1 nomore\n6         1 nomore\n\n\nThe last step ties it all together:\n\ncalculate() finds the statistic (here our statistic is the proportion) for each of the resampled datasets.\n\nKeep all of those differences, and take a look at them. Are they all the same? Do they vary a lot? Do they vary a small amount?\n\nset.seed(4747)\nboot_sick <- HELP %>%\n  specify(response = sick, success = \"more\") %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%  # need a lot of reps to truly understand\n  calculate(stat = \"prop\")\n\nboot_sick %>% head()\n\nResponse: sick (factor)\n# A tibble: 6 × 2\n  replicate  stat\n      <int> <dbl>\n1         1 0.186\n2         2 0.168\n3         3 0.177\n4         4 0.207\n5         5 0.175\n6         6 0.173\n\n\n\n\nStep 3. Look at all the differences\nFortunately for us, we saved all the differences in proportions into an object that was called boot_sick. We can visualize the differences using ggplot()!\n\nboot_sick %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nWe can tell that the bootstrapped proportions fall roughly between 0.15 and 0.20 (ish). But we can quantify the interval of interest by finding the middle 90% or 95% or 99% of the bootstrapped proportions to correspond to the confidence interval values.\nRemember, when adding layers to a plot use + instead of %>%.\n\nci_sick_90 <- boot_sick %>%\n  summarize(lower = quantile(stat, probs = 0.05),\n         upper = quantile(stat, probs = 0.95))\n\nci_sick_95 <- boot_sick %>%\n  summarize(lower = quantile(stat, probs = 0.025),\n         upper = quantile(stat, probs = 0.975))\n           \nboot_sick %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram() + \n  geom_vline(data = ci_sick_90, aes(xintercept = lower), color = \"red\") +\n  geom_vline(data = ci_sick_90, aes(xintercept = upper), color = \"red\") +\n  geom_vline(data = ci_sick_95, aes(xintercept = lower), color = \"blue\") +\n  geom_vline(data = ci_sick_95, aes(xintercept = upper), color = \"blue\") \n\n\n\n\n\n\n\n\n\n\nStep 4. Calculate the confidence interval\nThe confidence interval(s) has(have) already been calculated above! We just need to print them out to see the actual values. Note that as seen on the graph (and as expected given the lower confidence), the 90% interval is narrower than then 95% interval.\n\nci_sick_90\n\n# A tibble: 1 × 2\n  lower upper\n  <dbl> <dbl>\n1 0.163 0.197\n\nci_sick_95\n\n# A tibble: 1 × 2\n  lower upper\n  <dbl> <dbl>\n1 0.160 0.201\n\n\n\n\nStep 5. Make a conclusion\nConclusion: We are 90% (95%) confident that the in the larger population of people from whom these data were selected (e.g., patients with no primary care physician who were seeking help from a detoxification unit), the true proportion of people who get sick more often than others (as defined by a response of definitely true or mostly true) is between 0.163 and 0.197 (95% interval: 0.160 to 0.201)."
  },
  {
    "objectID": "slides/lab4_m58_s23_BS.html#to-turn-in",
    "href": "slides/lab4_m58_s23_BS.html#to-turn-in",
    "title": "Lab 4 - Math 58B: Bootstrap Percentile Confidence Intervals",
    "section": "To Turn In",
    "text": "To Turn In\n\nThe data\nThe write-up will use the same data as the main lab. But you choose a new variable now.\nThe data come from a randomized clinical trial called Health Evaluation and Linkage to Primary Care.\n\nThe HELP study was a clinical trial for adult inpatients recruited from a detoxification unit. Patients with no primary care physician were randomized to receive a multidisciplinary assessment and a brief motivational intervention or usual care, with the goal of linking them to primary medical care.\n\nFor today, we won’t focus on the treatment variable, instead we will work to understand if those individuals in the study might be representative of a larger population. That is, we will build confidence intervals for parameters that describe the population about which we can generalize the results.\n\nlibrary(openintro)\ndata(\"HELPfull\")\n\nThe task for today will focus on finding a confidence interval for a proportion parameter (where the population is given by the larger group from which these data were selected). You should choose a single variable from the dataset (there are 788 columns!) that is categorical but is coded as more than two levels. Note, choose a variable with the following constraints:\n\nat least 200 (non-missing) observations\noriginally coded with 3+ levels\nyour choice of “success” happens with frequency between 5% and 95%\n\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. The study\nChoose a variable of interest with the following characteristics:\n\nat least 200 (non-missing) observations\noriginally coded with 3+ levels\nyour choice of “success” happens with frequency between 5% and 95%\n\nAfter choosing the variable of interest and re-coding the variable so that it is binary, use the table() function to confirm that your code is correct.\n\n\nQ3. Parameter\nIn words, describe the parameter of interest.\n\n\nQ4. Visualize the 99% CI for your parameter\nUsing the infer syntax, calculate and visualize a 99% CI for your parameter of interest.\n\n\nQ5. Conclusion\nIn words, provide a conclusion for your parameter of interest (be sure to include the value of the endpoints of your interval).\n\n\nQ6. Reflection\nLet’s say that there was an alternative universe with other people who are similar in characteristics to the ones in the dataset at hand. And a second dataset was collected under identical conditions. The researchers came up with a confidence interval for the same parameter as you chose using the same bootstrap method.\nDo you think their interval would contain the parameter of interest? Why or why not? Explain.\n\npraise()\n\n[1] \"You are lovely!\"\n\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone."
  },
  {
    "objectID": "slides/MA58_HW4s23.html",
    "href": "slides/MA58_HW4s23.html",
    "title": "HW 4 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\narticulation of what the confidence interval is capturing\nknowledge of the impact of sample size on the resulting confidence interval\nunderstanding of confidence level\n\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. TRUE / FALSE\nLabel each of the following as TRUE or FALSE. If FALSE, explain why not. (You might want to come back to this problem after completing Q4.)\n\nA 95% confidence interval contains 95% of all the sample proportions.\nA 95% confidence interval contains 95% of the observations.\nA 95% confidence interval contains 95% of the population parameters.\n95% of confidence intervals contain their own sample proportion.\n95% of confidence intervals contain the population proportion.\n\n\n\nQ3. Extreme Poverty\nThis question comes from Hans Rosling (see his TED talk here) who asked a group of people whether the percentage of the world’s population who live in extreme poverty doubled, halved, or remained the same over the past twenty years?1\nIn a sample of 1005 US adults in 2017, 59% of the sample thought that the rate had doubled (the true answer is that the rate has halved).\nHere, we define “success” to be when an individual answered that the proportion of people in extreme poverty had doubled.\n\nWhat is the parameter of interest?\nUse R and bootstrapping (as in Lab 4) to find a 95% confidence interval for the true parameter value.\nInterpret the confidence interval using words like “extreme poverty”.\n\n\n\nQ4. Confidence\nUsing an applet, we will explore what “confidence” really means.2\nNote: the applet constructs a confidence interval in a different method from boostrapping (we will see the method in the coming weeks). However, the ideas of and interpretation of a CI are the same regardless of the method of construction.\nThe confidence interval applet: http://www.rossmanchance.com/applets/2021/confsim/ConfSim.html\nScenario: let’s say we are taking samples from a college where 40% of the student body has a tattoo. Hypothetical samples of size 75 will be taken.\nTo start, set \\(\\pi = 0.4\\) (in this applet, the true proportion is given by the notation \\(\\pi\\), we use \\(p\\) in class, they are the same thing). Set n= 75. Click “Sample” to generate a single sample and a single confidence interval.\n\nProvide the value of the confidence interval generated by the applet. Does the interval succeed in capturing the true population proportion? Explain (include the numerical value of the true population proportion?)\n\nClick on the “sample” button a few times.\n\n\n\nIs the sample always the same? What is the range of values for the number of students in your sample of 75 with a tattoo? (That is, do you ever get a number of successes of 3? What about 72? What is the range of values you are seeing?)\nDo all of your intervals capture the parameter? Explain how the intervals vary around the parameter line.\n\n\nClick on the “sample” button until the line turns red. Why is the interval red?\nNow change the number of intervals from 1 to 100. Click on “sample”. What proportion of the intervals are green?\nClick on “sample” again (100 new intervals). Do you always get 95% green intervals? After a few more clicks, report your running total of the percent of green intervals.\nSurvey researchers typically select only one random sample from a population, and then they produce a confidence interval based on that sample. How do we know whether the resulting confidence interval is successful in capturing the unknown value of the population parameter?\nIf we can’t know for sure whether the confidence interval contains the value of the population parameter, on what grounds can we be confident about the process of creating an interval estimate for the parameter?\nChange the sample size (n) from 75: first try n at 20, then try n at 200. Report on how changing the sample size changes/doesn’t change the width of the interval as well as the rate of parameter capture.\nChange the conf level from 95%: first try conf level at 80%, then try conf level at 99%. Report on how changing the confidence level changes/doesn’t change the width of the interval as well as the rate of parameter capture.\n\n\npraise()\n\n[1] \"You are majestic!\"\n\n\n\n\n\n\n\n\nFootnotes\n\n\nExample comes from Allan Rossman https://askgoodquestions.blog/↩︎\nExample comes from Allan Rossman https://askgoodquestions.blog/↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab5_m58_s23_norm.html",
    "href": "slides/lab5_m58_s23_norm.html",
    "title": "Lab 5 - Math 58B: One Proportion CIs w Normality",
    "section": "",
    "text": "Using the normal distribution, confidence intervals for any confidence level will be created! And we’ll keep trying to understand what “confidence” means."
  },
  {
    "objectID": "slides/lab5_m58_s23_norm.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab5_m58_s23_norm.html#advice-for-turning-in-the-assignment",
    "title": "Lab 5 - Math 58B: One Proportion CIs w Normality",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe assignment part of the lab is ONLY the last six questions at the very bottom. However, the commands in the first half of the assignment are key to doing the second half.\nSave the .Rmd file somewhere you can find it. Don’t keep everything in your downloads folder. Maybe make a folder called StatsHW or something. That folder could live on your Desktop. Or maybe in your Dropbox."
  },
  {
    "objectID": "slides/lab5_m58_s23_norm.html#getting-started",
    "href": "slides/lab5_m58_s23_norm.html#getting-started",
    "title": "Lab 5 - Math 58B: One Proportion CIs w Normality",
    "section": "Getting started",
    "text": "Getting started\nThe mosaic package provides great functions for calculating normal probabilities and normal cutoffs: xpnorm() and xqnorm() respectively."
  },
  {
    "objectID": "slides/lab5_m58_s23_norm.html#lab-goals",
    "href": "slides/lab5_m58_s23_norm.html#lab-goals",
    "title": "Lab 5 - Math 58B: One Proportion CIs w Normality",
    "section": "Lab Goals",
    "text": "Lab Goals\nComputing a confidence interval for a single proportion\n\nwait, what is a confidence interval?\nusing xqnorm() (find the number on the x-axis!) [Note: feel free to use plot=FALSE if you understand what is going on and don’t want the visual representation.]\npractice with ggplot()\n\n\nLoad packages\nIn lab #5 we will use the xpnorm() and xqnorm() functions which are in the mosaic package.\nLet’s load the packages.\n\nlibrary(tidyverse)  # ggplot lives in the tidyverse\nlibrary(mosaic)     # where xqnorm and xpnorm live\nlibrary(infer)      # for a function we are only using today\n\n\n\nThe data\nConsider the research study done in 2017 describing the support of marijuana legalization in Washington State.1\nFrom the abstract of the paper:\n\nData come from geographically representative general population samples of adult (aged 18 and over) Washington residents collected over five timepoints (every six months) between January 2014 and April 2016 (N=4101). Random Digit Dial was used for recruitment. Statistical analyses involved bivariate comparisons of proportions across timepoints and subgroups (defined by age, gender, and marijuana user status), and multivariable logistic regression controlling for timepoint (time) to formally test for trend while controlling for demographic and substance use covariates. All analyses adjusted for probability of selection.\n\nThe results are given as:\n\nSupport for legalization in Washington has significantly increased: support was 64.0% (95% CI: 61.2%-67.8%) at timepoint 1 and 77.9% (95% CI: 73.2%-81.9%) at timepoint 5. With each six months’ passing, support increased 19% on average. We found no statistically significant change in support for home-growing.\n\nFor lab #5, we’re going to pretend to have a population. Here, it is the population of Washington State in 2016 (about 7 million people), 75% of whom think that marijuana should be legalized. (I made up those numbers, but they seem reasonable!)\n\nlegal <- data.frame(support = c(rep(\"yes\", 5250000), rep(\"no\", 1750000)))\n\n\nStep 1. Take a random sample.\nLet’s start with a random sample of 50 people. How many of them support legalization? Is it the same proportion as your neighbor?\n\nsamp_n <- 50\nlegal_samp <- sample_n(legal, samp_n)\ntable(legal_samp)\n\nsupport\n no yes \n 11  39 \n\n\n\n\nStep 2. Find the Z value associated with 92%\nFind the quantile associated with 92% of the statistics (here \\(\\hat{p}\\)) in the sampling distribution. That is, how many SDs from the mean will 92% of \\(\\hat{p}\\) statistics be? It might be best to run each line one at a time.\n\nxqnorm(0.92, 0, 1)\n\n\n\n\n\n\n\n\n[1] 1.405072\n\nxqnorm(0.96, 0, 1)\n\n\n\n\n\n\n\n\n[1] 1.750686\n\nxqnorm(0.04, 0, 1)\n\n\n\n\n\n\n\n\n[1] -1.750686\n\n\nLet’s check to find the probabilities associated with each of the quantile cutoffs. Which one is the correct one to use for the 92% CI? Again, try running the lines one at a time.\n\nxpnorm(1.405, 0, 1)\n\n\n\n\n\n\n\n\n[1] 0.9199894\n\nxpnorm(c(-1.405, 1.405), 0, 1)\n\n\n\n\n\n\n\n\n[1] 0.08001064 0.91998936\n\nxpnorm(1.751, 0, 1)\n\n\n\n\n\n\n\n\n[1] 0.960027\n\nxpnorm(c(-1.751, 1.751), 0, 1)\n\n\n\n\n\n\n\n\n[1] 0.03997296 0.96002704\n\n\nNote that there are two “x” functions:\n\nThe first function has a “q” in it for “quantile” (xqnorm()). The quantile provides a cutoff value for a particular percentage. The median is the 50%. You may also be familiar with the quantile associated with the 90% on the SAT.\nThe second function has a “p” in it for “probability” (xpnorm()). The probability function provides the appropriate percentages of the distribution at the given quantiles.\n\n\nmargin of error is the quantile times the standard error:\n\n\\[me = Z^* \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\nStep 3. Create a CI\nUsing the following formula & your sample of 50 people, find a 92% CI for the true proportion of people who support legalizing marijuana use. Feel free to use R as a calculator.\n\\[\\hat{p} \\pm 1.751*\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\nNote a few things… 1.751 is used instead of 2 for Z* (because we wanted a 92% interval instead of a 95% interval). And \\(\\hat{p}\\) was used instead of p in the SE (because we don’t usually know \\(p\\) and we use the closest thing we have, \\(\\hat{p}\\)).\nDoes your CI capture the true value of p (which we know / we set to be 0.75)? Does your neighbor’s CI capture the true value of p?\n\n\nStep 4. Create a tidy CI\nNow use the following “tidy” formula to calculate the CI one more time. Did you get the same interval as in #2?\n\n(z_star_92 <- xqnorm(0.96, 0, 1))\n\n\n\n\n\n\n\n\n[1] 1.750686\n\nlegal_samp %>%\n  summarize(p_hat = sum(support == \"yes\") / samp_n, \n            se = sqrt(p_hat*(1-p_hat)/samp_n),  # se stands for \"standard error\"\n            me = z_star_92 * se,  # me stands for \"margin of error\"\n            lower = p_hat - me,   # lower is for the lower bound of the CI\n            upper = p_hat + me)   # upper is for the upper bound of the CI\n\n  p_hat         se        me     lower     upper\n1  0.78 0.05858327 0.1025609 0.6774391 0.8825609\n\n\n\n\nStep 5. Force reproducibility\nIf you have run your code a few different times, you may have gotten different samples of size 50. If you want to get the same exact sample (e.g., so that you can write about your results), set your randomness by using set.seed() before you sample, where the argument to the set.seed() function is your favorite integer. Try it and see if you can get the same repeated results.\n\n\nStep 6. Create 100 CIs\nBecause each student took a different random sample, we’d expect 92% of the classroom intervals to capture the true parameter value.\nUsing R, we’re going to collect many samples to learn more about how sample means and confidence intervals vary from one sample to another.\nHere is the rough outline:\n\nObtain a random sample.\nCalculate the sample proportion and use it to calculate and store the lower and upper bounds of the confidence intervals.\nRepeat 100 times.\n\nWe can get many CIs using the rep_sample_n() function. The following lines of code take 100 random samples of size samp_n from the population (and remember samp_n \\(= 50\\) as defined earlier) and computes the upper and lower bounds of the confidence intervals separately for each of the 100 samples.\nNote: you don’t need to memorize the code, but you should understand what it is doing.\n\nset.seed(47)\nci <- legal %>%\n  infer::rep_sample_n(size = samp_n, reps = 100, replace = FALSE) %>%\n  summarize(p_hat = sum(support == \"yes\") / samp_n, \n            se = sqrt(p_hat*(1-p_hat)/samp_n),  # se stands for \"standard error\"\n            me = z_star_92 * se,  # me stands for \"margin of error\"\n            lower = p_hat - me,   # lower is for the lower bound of the CI\n            upper = p_hat + me)   # upper is for the upper bound of the CI\n\nLet’s view the first five intervals:\n\nci %>%\n  head(5)\n\n# A tibble: 5 × 6\n  replicate p_hat     se     me lower upper\n      <int> <dbl>  <dbl>  <dbl> <dbl> <dbl>\n1         1  0.8  0.0566 0.0990 0.701 0.899\n2         2  0.72 0.0635 0.111  0.609 0.831\n3         3  0.76 0.0604 0.106  0.654 0.866\n4         4  0.76 0.0604 0.106  0.654 0.866\n5         5  0.7  0.0648 0.113  0.587 0.813\n\n\nNext we’ll create a plot similar to the Confidence Interval applet and also to Figure 13.11 in Introduction to Modern Statistics. The first step will be to create a new variable in the ci data frame that indicates whether the interval does or does not capture the true population mean. Note that capturing the true parameter value would mean the lower bound of the confidence interval is below the value and upper bound of the confidence interval is above the value. Remember that new variables are created using the mutate() function.\n\nci <- ci %>%\n  mutate(capture_p = case_when(\n    lower < 0.75 & upper > 0.75 ~ \"yes\",\n    TRUE ~ \"no\"))\n\nci %>% select(p_hat, lower, upper, capture_p) %>% head(5)\n\n# A tibble: 5 × 4\n  p_hat lower upper capture_p\n  <dbl> <dbl> <dbl> <chr>    \n1  0.8  0.701 0.899 yes      \n2  0.72 0.609 0.831 yes      \n3  0.76 0.654 0.866 yes      \n4  0.76 0.654 0.866 yes      \n5  0.7  0.587 0.813 yes      \n\n\nAll that is left is to create the plot.\nNote that the geom_errorbar() function only understands y values, and thus we have used the coord_flip() function to flip the coordinates of the entire plot back to the more familiar vertical orientation.\n\nggplot(ci, aes(x = replicate, y = p_hat, color = capture_p)) +\n  geom_point(size = 0.5) +\n  geom_errorbar(aes(ymin = lower, ymax = upper)) + \n  geom_hline(yintercept = 0.75, color = \"darkgray\") + # draw vertical line\n  coord_flip()\n\n\n\n\n\n\n\n\nYou can count how many confidence intervals capture the true proportion by using the summarize function. [Aside: what is the difference between = and == in the code below?]\n\nci %>% \n  summarize(capturecount = count(capture_p == \"yes\"))\n\n# A tibble: 1 × 1\n  capturecount\n         <int>\n1           94\n\n\n\n\nReview of logic\nOne more time with the logic of how the CI was created:\n\n92% of the \\(\\hat{p}\\) are within plus or minus 1.751*SE ( = margin of error) of the true \\(p\\). If you add / subtract the exact same amount (i.e., add and subtract the margin of error) around \\(\\hat{p}\\) (instead of around \\(p\\)), then the statistics which are close enough to the true value will “capture” the true value. The 8% which are far out, will not be close enough to touch the true parameter.\n\nAsk in class / talk to your neighbor / use the applet to make sure the logic of the CI creation makes sense to you."
  },
  {
    "objectID": "slides/lab5_m58_s23_norm.html#to-turn-in",
    "href": "slides/lab5_m58_s23_norm.html#to-turn-in",
    "title": "Lab 5 - Math 58B: One Proportion CIs w Normality",
    "section": "To Turn In",
    "text": "To Turn In\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Original Study\nConsider the original study on marijuana legalization. Provide one sentence (not a number) describing each of the following:\n\nobservational unit:\n\nvariable:\n\nstatistic:\n\nparameter:\n\n\n\nQ3.Bar plot\nCreate a bar plot of the first sample you took (above) from the hypothetical population that I made up. Use geom_bar().\n\n\nQ4. Multiplier\nPick a confidence level of your choosing other than 92% or 95%. What is the number you will use for multiplying the SE? (What is Z*, aalled the “multiplier”?)\n\n\nQ5. 100 CIs\nCalculate 100 confidence intervals at the confidence level you chose in the previous question, plot all intervals on one plot, and calculate the proportion of intervals that include the true population proportion. How does the percentage compare to the confidence level selected (in the previous question) for the intervals? Make sure to include the plot in your answer.\n\n\nQ6. 1000 CIs\nTo get a better sense of the actual coverage rate, calculate 1000 intervals (don’t try to plot 1000 intervals on one graphic). How many of your intervals captured the true population value? Was it close to the confidence level you chose?\n\n\nQ7. Bigger sample\nRepeat the previous two questions using \\(n=610\\) (as in the original study). Answer the following:\n\nHow is the plot different across \\(n=50\\) and \\(n=610\\)?\n\nDoes the coverage rate (how often a sample will capture the true value) depends on \\(n\\)? Explain."
  },
  {
    "objectID": "slides/MA58_HW5s23.html",
    "href": "slides/MA58_HW5s23.html",
    "title": "HW 5 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nworking with Z scores (calculating and interpreting)\ncalculating normal probabilities (with Z scores and with \\(\\hat{p}\\))\ncalculating CIs using the Z* value from the normal curve\n\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Distribution of \\(\\hat{p}\\)\nSuppose the true population proportion were p = 0.1. The figure below (see the pdf version of the assignment with the image) shows what the distribution of a sample proportion looks like when the sample size is \\(n = 20\\), \\(n = 100\\), and \\(n = 500\\).\n\nWhat does each point (observation) in each of the samples represent?\nDescribe how the distribution of the sample proportion, \\(\\hat{p}\\), changes as n becomes larger.\n\n\n\nQ3. Area under the curve\nWhat percent of a standard normal distribution [the one with: N( \\(\\mu = 0; \\sigma = 1\\))] is found in each region? Be sure to draw a graph [or use a function in R (xpnorm() in the mosaic package) that provides a graph].\n\nZ < -1.35\nZ > 1.48\n-0.4 < Z < 1.5\n|Z| > 2\n\n\n\nQ4. Scores on the GRE\nA college senior who took the Graduate Record Examination exam scored 620 on the Verbal Reasoning section and 670 on the Quantitative Reasoning section. The mean score for Verbal Reasoning section was 462 with a standard deviation of 119, and the mean score for the Quantitative Reasoning was 584 with a standard deviation of 151. Suppose that both distributions are nearly normal.\n\nWrite down the short-hand for the two normal distributions.\nWhat is her Z score on the Verbal Reasoning section? On the Quantitative Reasoning section? Draw a standard normal distribution curve and mark the two Z scores.\nWhat do the Z scores tell you?\nRelative to others, which section did she do better on?\nFind her percentile scores for the two exams.\nWhat percent of the test takers did better than her on the Verbal Reasoning section? On the Quantitative Reasoning section?\nExplain why simply comparing her raw scores from the two sections would lead to the incorrect conclusion that she did better on the Quantitative Reasoning section.\nIf the distributions of the scores on the exams are not nearly normal, would your answers to parts (b) - (f) change? Explain your reasoning.\n\n\n\nQ5. Chronic illness\nIn 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”. However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting.\n\nUsing the normal distribution as a probability model, create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study.\n\nIdentify each of the following statements as TRUE or FALSE. You don’t need to justify it here, but it’s always a good idea (read: exam coming soon!) to practice putting into words why something is true or false.\n\nWe can say with certainty that the confidence interval contains the true percentage of U.S. adults who suffer from a chronic illness.\nIf we repeated this study 1,000 times and constructed a 95% confidence interval for each study, then approximately 950 of those confidence intervals would contain the true fraction of U.S. adults who suffer from chronic illnesses.\nThe poll provides statistically significant evidence (at the \\(\\alpha = 0.05\\) level) that the percentage of U.S. adults who suffer from chronic illnesses is below 50%. [You don’t need to do the entire hypothesis test here, you should be able to get the answer from the CI. However, you should also know how to do the entire hypothesis test!]\nSince the standard error is 1.2%, only 1.2% of people in the study communicated uncertainty about their answer.\n\n\npraise()\n\n[1] \"You are lovely!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/MA58_HW6s23.html",
    "href": "slides/MA58_HW6s23.html",
    "title": "Math 58B - HW6",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nsampling\nerrors\np-values\n\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Tattoos, Part I1\nSuppose that you want to test the null hypothesis that one-third of all adults in your county have a tattoo, against a two-sided alternative. For each of the following parts, create your own example of a sample of 100 people that satisfies the indicated property. Do this by providing the sample numbers with a tattoo and without a tattoo. Also report the test statistic and p-value from a one-proportion z-test.\n\nUse the normal distribution and the following information to calculate a p-value for the appropriate Z-score\n\n\np-value = probability a N(0,1) (standard normal) will be more extreme than the Z score you calculate below.\n\n\nCreate your own example of a sample of 100 people such that: the two-sided p-value is less than 0.001.\nCreate your own example of a sample of 100 people such that: the two-sided p-value is greater than 0.20.\n\n\nFor each of i. and ii. above, explain whether you may have committed a Type 1 or a Type 2 error. (Hint: you will not know whether or not you committed an error, but you will know the type of error you may have committed.)\n\n\n\nQ3. Tattoos, Part II2\nSuppose that you want to estimate the proportion of all adults in your county who have a tattoo. For each of the following parts, create your own example to satisfy the indicated property. Do this by specifying the sample size and the number of people in the sample with a tattoo. Also determine the confidence interval.\n\nCreate your own example such that the sample proportion with a tattoo is 0.30, and a 95% confidence interval for the population proportion includes the value 0.35.\nCreate your own example such that the sample proportion with a tattoo is 0.30, and a 99% confidence interval for the population proportion does not include the value 0.35.\n\n\n\nQ4. Gettysburg Address3\nConsider the activity done in class on the Gettysburg Address by Abraham Lincoln. (Click on “Gettysburg”: http://www.rossmanchance.com/applets/2021/sampling/OneSample.html?population=gettysburg)\n\nFor any of the samples (e.g., your sample) taken in class, describe (in words) the:\n\n\n\nan observational unit\na variable\nthe statistic\nthe parameter\n\nAfter you click on “show sampling options,” the applet shows histograms at the bottom. For each of the three histograms (i.e., provide three answers), what is the thing that is varying?\nSuppose that you select a sample of Claremont students by standing in front of the library and approaching 50 students who pass by. Would this constitute a random sample of Claremont students? What if you stand in front of the gym and approach 50 students who pass by? Explain.\nEven though the convenience sampling (at the library) described above is not random, could it nevertheless result in a sample that is representative of the population of Claremont students? Identify a variable for which you would not be willing to consider such a convenience sample (as described above) to be representative of the population of Claremont students. Also identify a variable for which you would be willing to consider such a sample (as described above) to be representative of the population of Claremont students.\n\n\n\n\n\n\n\n\nFootnotes\n\n\nhttps://askgoodquestions.blog/↩︎\nhttps://askgoodquestions.blog/↩︎\nInv 1.12, ISCAM↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab6_m58_s23_errors.html",
    "href": "slides/lab6_m58_s23_errors.html",
    "title": "Lab 6 - Math 58b: error rates",
    "section": "",
    "text": "By assessing our own comfort with randomness, we investigate 0.05. Additionally, we analyze data without a clear decision as to whether or not the null hypothesis should be rejected.\n\nwhy 0.05?\nwhat does a p-value really mean?\nhow / why is there (is there not) always a single result for a research questions?"
  },
  {
    "objectID": "slides/lab6_m58_s23_errors.html#getting-started",
    "href": "slides/lab6_m58_s23_errors.html#getting-started",
    "title": "Lab 6 - Math 58b: error rates",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nIn this lab we will continue to use infer syntax and the xpnorm() function which is in the mosaic package.\nLet’s load the packages.\n\nlibrary(tidyverse)  # ggplot and %>% \nlibrary(mosaic)     # xpnorm and xqnorm \nlibrary(infer)      # simulation inference code \n\n\nPreliminary personal level of significance\nFollow the link here and click through the scatterplot images: https://www.openintro.org/book/stat/why05/\n\nWhat is your personal level of significance?\nWhen you hear new information, do you consider yourself on the skeptical side or on the believing side? (There is no right answer!)\n\n\n\n\nThe data\n(The data is only for Q2. The other questions do not require data.)\nResearchers have conjectured that the use of the word “forbid” is more off-putting than the word “allow” (in affecting people’s responses to survey questions). In particular, the suggestion is that people do not like to “forbid” anything. Students in an introductory statistics class were randomly assigned to answer one of the following questions:\n\nShould your college allow speeches on campus that might incite violence?\n\nShould your college forbid speeches on campus that might incite violence?\n\nOf the 14 students who received the first question, 8 responded yes. Of the 15 students who received the second question, 13 said no. Think carefully about the response variable. It should not be coded as “yes” and “no” as answered on the questionnaire.\n\nWhat do the data look like?\nUsing the information above, create a 2x2 table (with your pencil) describing the dataset. What are your two variables? How many people are in each group? Again, do not code with “yes” and “no”, use descriptive words.\n\n\nHypothesis test\nIn order to formally test the researchers’ conjecture (that the words can be off-putting), a null sampling distribution must be created (in order to compare the observed data against). You will create the null sampling distribution in two ways:\n\nUsing the infer syntax.\nUsing the Central Limit Theorem\n\n\nThe Central Limit Theorem: the sample average (e.g., sample mean, sample proportion, etc.) in repeated random samples taken from a population will be normally distributed if the sample size is large enough.\n\nNote that for the two proportion case, if the sample sizes are large enough the CLT says:\n\\[\\hat{p}_1 - \\hat{p}_2 \\sim N\\bigg(p_1 - p_2, \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p_1})}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p_2})}{n_2}}\\bigg)\\]\nIt isn’t a hard and fast rule, but generally the CLT approximation is good if there are at least 10 (or possibly 5) successes and at least 10 (possibly 5) failures in each group."
  },
  {
    "objectID": "slides/lab6_m58_s23_errors.html#to-turn-in",
    "href": "slides/lab6_m58_s23_errors.html#to-turn-in",
    "title": "Lab 6 - Math 58b: error rates",
    "section": "To Turn In",
    "text": "To Turn In\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Speeches and Violence\nIn this first question, you will analyze the data described above using some old ideas and some new ideas.\nTo create a dataset, use the rep() function to replicate an appropriate number of times. Use the c() function to create a column. You should be able to do this on your own. For now, fill in the blanks (and then change to eval = TRUE).\n\n# first create a data frame with the survey data\ndecision <- data.frame( ___ = c(rep(\"___\", 14), rep(\"___\", 15)), \n                         ___  = c(rep(\"___\", 8), rep(\"___\", 6),\n                                   rep(\"___\", 13), rep(\"___\", 2)))\n\ntable(decison)\n\n\nPlot the observed data using geom_bar() and use fill = response to fill the bars in with appropriate colors, where the word response represents whatever you called the variable representing how the students responded to the survey.\nUse infer to analyze the data. Report the one-sided p-value (you will report the conclusion in words below in part d.).\nUse the following formula to create a Z-score for the same test (different p-value calculation) as was done with infer. Use R as a calculator to find the relevant Z-score, and find the one-sided p-value (you will report the conclusion in words below in part d.).\n\n\\[\\mbox{Z score} = \\frac{(\\hat{p}_1 - \\hat{p}_2) - 0}{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p_1})}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p_2})}{n_2}}}\\]\n\nGive a complete conclusion to the data analysis / hypothesis test (that is, conclude what you think is most appropriate). State the null and alternative hypotheses, provide what you believe is the most accurate significance result (compare parts b. and c. above), and give a sense of to whom (what population, if any) the results can be applied.\n\n\n\nQ3. p-values\nRead the ASA’s statement on p-values: http://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108\nChoose two different principles (pg 131-132) and explain each (separately) as if to a peer in a science class who is making conclusions about a recent study. Explain in your own words.\n\n\nQ4. p-values, take two\nWhy use p-values at all? That is, what is benefit of having a p-value (as opposed to simply descriptive statistics or graphs of the data)?\n\n\n\n\nIf you are still curious about the ideas in this lab (not part of the assignment):\nNone of the queries below are part of the lab. I offer them here for people who are intrigued by the ideas we’ve covered and want to know more. Indeed, the article linked below (which has been cited 3000+ times and viewed almost 3 million times) has a provocative title (and is incredibly well written).\nQ5. Read Ioannidis (2005), “Why Most Published Research Findings are False” http://www.plosmedicine.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pmed.0020124&representation=PDF\n\nConsider table 1. Suppose that the level of significance is taken to be 0.05 and the power is 0.8. Also, set R (the number of true to not true relationships) to be 2 (for every 3 experiments, one is null). What percent of research findings (i.e., “significant” findings) are actually true (i.e., Ha is true)? [hint: for ease of calculation, you can set c to be something like 10,000.]\n\nSolution:\n\n\n\nFinding\nTrue Yes\nTrue No\nTotal\n\n\n\n\nYes\n5333\n167\n5500\n\n\nNo\n1334\n3166\n4500\n\n\nTotal\n6667\n3333\n10000\n\n\n\nOut of the 5500 significant findings, it turns out the 5333 of them are true. Therefore, the proportion of significant findings is 5333/5500 = 0.9696364.\n\nConsider table 1. Suppose that the level of significance is taken to be 0.05 and the power is 0.3. Also, set R (the number of true to not true relationships) to be 0.1 (for every 11 experiments, 10 are null). What percent of research findings (i.e., “significant” findings) are actually true (i.e., Ha is true)? [hint: for ease of calculation, you can set c to be something like 10,000.]\n\nSolution:\n\n\n\nFinding\nTrue Yes\nTrue No\nTotal\n\n\n\n\nYes\n273\n455\n728\n\n\nNo\n636\n8636\n9272\n\n\nTotal\n909\n9091\n10000\n\n\n\nOut of the 728 significant findings, it turns out the 5333 of them are true. Therefore, the proportion of significant findings is 273/728 = 0.375.\nWe notice that the proportion of significant findings is very dependent on not only the power, but also on the proportion of experiments which are null to start with. The 6 important corollaries in the Ioannidis paper all follow from the tables (table 1 and others like it) which reflect on the state of testing beyond just controlling for type I errors using a level of significance of 0.05. The 6 corollaries contain important ideas to think about in doing science.\nQ6. In the Dance of the p-values (https://www.youtube.com/watch?v=5OL1RqHrZQ8), what is the narrator arguing?\nSolution:\nThe point being made in the Dance of the p-values is that the p-value gives virtually no information about the effect of interest. A confidence interval gives not only the magnitude of the effect but also the variability associated with the estimate. That is, the degree of uncertainty in the effect is captured by the width of the interval. Additionally, in replicates of the same experiment, the p-value can vary widely (from significant to non-significant) whereas a confidence interval captures the true parameter 95% of the time (albeit sometimes overlapping the null value, too)."
  },
  {
    "objectID": "slides/lab7_m58_s23_RROR.html",
    "href": "slides/lab7_m58_s23_RROR.html",
    "title": "Lab 7 - Math 58B: Relative Risk & Odds Ratios",
    "section": "",
    "text": "Just like any statistic which is measured on a dataset, the variability of the sample relative risk (\\(\\widehat{RR}\\)) and the sample odds ratio (\\(\\widehat{OR}\\)) is described by a sampling distribution. The goals for today include being able to:\n\ndescribe the null sampling distribution of (\\(\\widehat{RR}\\) and) \\(\\widehat{OR}\\) under the null hypothesis using a randomization test\ndescribe the sampling distribution of (\\(\\widehat{RR}\\) and) \\(\\widehat{OR}\\) for general setting using bootstrapping\ndescribe the sampling distribution of (\\(\\widehat{RR}\\) and) \\(\\widehat{OR}\\) (under the null or the alternative) using the Central Limit Theorem (normal distribution!)\nuse the randomization distribution to run a complete hypothesis test\nuse the bootstrap distribution to create a confidence interval\nuse the normal distribution to run a complete hypothesis test"
  },
  {
    "objectID": "slides/lab7_m58_s23_RROR.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab7_m58_s23_RROR.html#advice-for-turning-in-the-assignment",
    "title": "Lab 7 - Math 58B: Relative Risk & Odds Ratios",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe assignment part of the lab is ONLY the last six questions at the very bottom. However, the commands in the first half of the assignment are key to doing the second half.\nSave the .Rmd file somewhere you can find it. Don’t keep everything in your downloads folder. Maybe make a folder called StatsHW or something. That folder could live on your Desktop. Or maybe in your Dropbox."
  },
  {
    "objectID": "slides/lab7_m58_s23_RROR.html#getting-started",
    "href": "slides/lab7_m58_s23_RROR.html#getting-started",
    "title": "Lab 7 - Math 58B: Relative Risk & Odds Ratios",
    "section": "Getting started",
    "text": "Getting started\n\nthe infer package will be used for the R analysis\nthe following two applets may help with forming intuition\n\nRandomization test applet: http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm\nBootstrapping applet: http://www.rossmanchance.com/applets/2021/twopopprop/twopopprop.html\n\n\n\nLoad packages / data\nIn this lab we will continue to use the infer package, along with the applets. The infer syntax is meant to focus understanding on the randomization and bootstrapping processes. For each line, pay attention to what the code is doing.\n\nA study in 1994 examined 491 dogs that had developed cancer and 945 dogs as a control group to determine whether there is an increased risk of cancer in dogs that are exposed to the herbicide 2,4-Dichlorophenoxyacetic acid (2,4-D).\n\n\nHayes HM, Tarone RE, Cantor KP, Jessen CR, McCurnin DM, and Richardson RC. 1991. Case- Control Study of Canine Malignant Lymphoma: Positive Association With Dog Owner’s Use of 2, 4- Dichlorophenoxyacetic Acid Herbicides. Journal of the National Cancer Institute 83(17):1226-1231.\n\nThe dog data are in the openintro package.\n\ndata(\"cancer_in_dogs\")\n\ncancer_in_dogs %>%\n  table()\n\n          response\norder      cancer no cancer\n  2,4-D       191       304\n  no 2,4-D    300       641\n\n\n\n\nVariablity of \\(\\widehat{OR}\\) when \\(H_0\\) is true\nThe randomization process (which leads to the randomization test) creates different possible values of \\(\\widehat{RR}\\) and \\(\\widehat{OR}\\) assuming the null hypothesis is true.\nSee: Randomization test applet: http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm\nNote that the variability of the statistics (the sampling distribution) can be displayed by applying the same ideas that were used on the difference in sample proportions.\nWhat are the steps needed to create a sampling distribution?\n\ncancer_in_dogs %>%\n  ___(response ~ order, success = \"cancer\") %>%\n  ___(null = \"independence\") %>%\n  ___(reps = 1000, type = \"permute\") %>%\n  ___(stat = \"odds ratio\", order = c(\"2,4-D\", \"no 2,4-D\")) %>%\n  ___() +\n  xlab(\"statistic = odds ratio\")\n\n\n\nQUESTION\n\nWhat is the computer doing to generate the null sampling distribution of \\(\\widehat{OR}\\)?\nCan you describe a tactile method that could mimic what the computer code does?\n\n\n\nHypothesis test of OR when \\(H_0\\) is true\nBecause the data come from a case-control study, we’ll choose to investigate the OR (instead of the RR). Note that the test is one-sided because the researchers suspected that the herbicide was associated with higher cancer rates.\n\\(H_0: OR = 1\\)\n\\(H_A: OR > 1\\)\nThe p-value is 0.007 (you might have gotten a slightly different answer if you set a different seed). We can reject the null hypothesis and claim that the true odds (NOT “risk”) of cancer is higher for those dogs with exposure to the 2, 4-D herbicide.\n\nset.seed(4774)\nobs_or <- cancer_in_dogs %>%\n  specify(___ ~ ___, ___ = \"___\") %>%\n  calculate(stat = \"___\", order = c(\"___\", \"___\")) \n\nobs_or\n  \nnull_or <- cancer_in_dogs %>%\n  specify(___ ~ ___, ___ = \"___\") %>%\n  hypothesize(___ = \"___\") %>%\n  generate(___ = ___, ___ = \"___\") %>%\n  calculate(stat = \"___\", order = c(\"___\", \"___\"))\n\nnull_or %>%\n  ___() +\n  xlab(\"statistic = odds ratio\") + \n  shade_p_value(obs = ___, direction = \"___\")\n\nnull_or %>%\n  ___(obs = obs_or, direction = \"___\")\n\n\n\nVariablity of \\(\\widehat{OR}\\) with no hypothesis\nThe bootstrap process (which leads to a bootstrap confidence interval) creates different possible values of \\(\\widehat{OR}\\) without assuming the null hypothesis is true.\nSee: Bootstrapping applet: http://www.rossmanchance.com/applets/2021/twopopprop/twopopprop.html\nNote that the variability of the statistics (the sampling distribution) can be displayed by applying the same ideas that were used on the difference in sample proportions. It is important to observe that the distribution (for \\(\\widehat{OR}\\)) is not centered at 1.\n\nset.seed(5)\ncancer_in_dogs %>%\n  ___(response ~ order, success = \"cancer\") %>%\n  ___(reps = 1000, type = \"bootstrap\") %>%  # no hypothesize step!!!\n  ___(stat = \"odds ratio\", order = c(\"2,4-D\", \"no 2,4-D\")) %>%\n  ___() +\n  xlab(\"statistic = odds ratio\")\n\n\n\nQUESTION\n\nWhat is the difference in the code from the \\(H_0\\) curve vs the no hypothesis curve?\nWhat is the difference in the look of the histograms from the \\(H_0\\) curve vs the no hypothesis curve?\n\n\n\nBootstrap Confidence Interval for OR\nUsing the histogram above, the CI for the OR can be taken directly from the bootstrap sampling distributions.\nWe are 93% confident that the true odds of lymphoma is between 1.09 and 1.64 times higher for those dogs who have been exposed to 2, 4-D than those dogs who have not.\n\nset.seed(314)\nboot_or <- ___ %>%\n  specify(___ ~ ___, success = \"___\") %>%\n  generate(___ = ___, type = \"___\") %>%  # no hypothesize step!!!\n  calculate(stat = \"___\", order = c(\"___\", \"___\")) \n\nci_or_93 <- get_ci(boot_or, level = ___)\nci_or_93\n\nvisualize(boot_or) +\n  shade_confidence_interval(endpoints = ci_or_93, color = \"red\", fill = \"pink\") +\n  xlab(\"statistic = odds ratio\")\n\n\n\nVariability of \\(\\widehat{OR}\\) assuming \\(H_0\\) is true.\nAs with other statistics we’ve seen, the central limit theorem tells us about the distribution of \\(\\ln(\\widehat{OR})\\), if the sample size is big enough. Indeed, the sampling distribution of \\(\\ln(\\widehat{OR})\\) can be written as:\n\\[\\ln(\\widehat{OR}) \\sim N\\bigg(\\ln(OR), \\sqrt{\\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\bigg)\\]\nIt is hard to compare the Normal curve to the (computational) curves above because the computational curves describe the (slightly skewed) distribution of \\(\\widehat{OR}\\) and the normal curve describes the distribution of \\(\\ln(\\widehat{OR})\\). However, you should be able to sketch (with a pencil) the theoretical sampling distribution of \\(\\ln(\\widehat{OR})\\), given the information above.\n\nWrite the null and alternative hypotheses in terms of the true parameter ln(OR).\n\n\nFind a Z-score\n\n\nUsing the Z-score, find the p-value\n\n\nConclude\nUsing the p-value, conclude the test using words like odds, herbicide, dogs, cancer.\n\n\n\nQUESTION\n\nWhat is the difference in the computational sampling distribution vs. the normal theory sampling distribution?\nWhat is the difference in the hypothesis test conclusions for the computational sampling distribution vs. the normal theory sampling distribution?"
  },
  {
    "objectID": "slides/lab7_m58_s23_RROR.html#to-turn-in",
    "href": "slides/lab7_m58_s23_RROR.html#to-turn-in",
    "title": "Lab 7 - Math 58B: Relative Risk & Odds Ratios",
    "section": "To Turn In",
    "text": "To Turn In\n\nThe data\nConsider the article (and data therein) on sleepy driving: (Connor et al. “Driver sleepiness and risk of serious injury to car occupants: population based case control study”, British Medical Journal, 2002, https://www.bmj.com/content/bmj/324/7346/1125.1.full.pdf )\nAlthough the researchers look at many variables, consider the two following two variables: (1) driver sleepiness score of 1-3 vs 4-7, (2) driver involved in an “injury crash” or not. Note that there seems to be some missing information with respect to the sleepiness score. [See Table 1 in the paper.]\nYou will need to enter the data as you did in Lab 6 (after looking at Table 1 in the paper).\n\nQ1. PodQ\nDescribe one thing you learned from someone in your pod this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\n\nQ2. Variables\nIn the study on sleepy driving, which is the explanatory and which is the response variable? What is an observational unit?\n\n\nQ3. Study\nHow were the data selected, as a case-control study or a cohort study?\nWhat aspect of the population is it impossible to know given how the data were sampled:\n\nis it impossible to estimate the proportion of people who crash (of those who are sleepy)?\n\nOr is it impossible to estimate the proportion of people who are sleepy (of those who crash)?\n\nNote: in a case-control study, the participants are selected based on the value of the response variable. In a cohort study, the participants are selected based on the value of the explanatory variable. Ask yourself, how were the people in this study selected?\n\n\nQ4. OR computational hypothesis test\nUsing a randomization test with the odds ratio statistic, complete a full hypothesis test: state null and alternative hypotheses with parameter values, the p-value from the randomization test, and the conclusion in words of the problem.\n\n\nQ5. OR conmputational confidence interval\nFind and interpret a 90% bootstrap percentile CI for the true OR of the variables above. [Note: to complete this question you will need to write out the words which describe the true OR. Do not use words like “probability” “rate” or “risk”. Instead, use the word “odds”.]\n\n\nQ6. OR normal theory hypothesis test\nUsing central limit theorem, complete a full hypothesis test: state null and alternative hypotheses with parameter values, the Z-score, the p-value, and the conclusion in words of the problem.\n\n\nQ7. “strong” and “evidence”\n\nWhich part(s) of the analysis reveals evidence of a strong association? Explain.\nWhich part(s) of the analysis reveals strong evidence of an association? Explain.\n\n\npraise()\n\n[1] \"You are super-duper!\"\n\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone."
  },
  {
    "objectID": "slides/MA58_HW7s23.html",
    "href": "slides/MA58_HW7s23.html",
    "title": "HW 7 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nworking with relative risk\nworking with odds ratios\n\n\nQ1. PodQ\nDescribe one thing you learned from someone in your pod this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Conserving Hotel Towels? (HW 3.8 ISCAM)\nMany hotels have begun a conservation program that encourages guests to re-use towels rather than have them washed on a daily basis. A recent study examined whether one method of encouragement might work better than another. Different signs explaining the conservation program were placed in the bathrooms of the hotel rooms, with random assignment determining which rooms received which sign. One sign mentioned the importance of environmental protection, whereas another sign claimed that 75% of the hotel’s guests choose to participate in the program. Researchers used the hotel staff (a mid-sized, mid-priced hotel in the Southwest that was part of a well-known national hotel chain) to record whether guests staying for multiple nights agreed to reuse their towel after the first night. “Success”, in the study, means that the guest did agree to reuse the towel.\n\nIdentify the observational units, explanatory variable, and response variable in the study.\nConsider the null hypothesis that states \\(H_0: p_{sn} = p_{ep}\\) (each probability represents the true proportion of all people who will choose to re-use their towel when given the particular prompt). Rewrite the hypothesis in three different ways as functions of the two values: \\(p_{sn}\\) and \\(p_{ep}\\). That is, each time, the null hypothesis will be written with a different parameter value. Each null hypothesis should be a function of \\(p_{sn}\\) and \\(p_{ep}\\) (i.e., a parameter) on the left of the equal sign and a single number on the right of the equal sign.\n\n\n\\(H_0:\\) the difference in proportions is ____\n\\(H_0:\\) the relative risk is ____\n\\(H_0:\\) the odds ratio is ____\n\n\nRecall that a natural log transformation creates a sampling distribution of \\(\\widehat{RR}\\) and \\(\\widehat{OR}\\) which are approximately normal. Therefore, we find the SE of the natural log transformed statistics in those cases.\n\nWrite down the general formula for the standard error (which is used when the mathematical (instead of computational) approximation is used) for each of the respective statistics:\n\n\\(SE(\\hat{p}_{sn} - \\hat{p}_{ep}) =\\)\n\n\\(SE\\bigg(\\ln(\\hat{p}_{sn}/ \\hat{p}_{ep})\\bigg) =\\)\n\n\\(SE\\Bigg(\\ln\\bigg(\\frac{\\hat{p}_{sn}/(1-\\hat{p}_{sn})}{\\hat{p}_{ep}/(1-\\hat{p}_{ep})}\\bigg)\\Bigg) =\\)\n\n\nCalculate the statistic (and natural log of the statistic), the SE, and the Z score for each of the three measures.\n\nThe following table displays the observed data in the study:\n\n\n\ntowel\nSocial norm\nEnvironmental protection\nTotal\n\n\n\n\nGuest opted to re-use towel\n98\n74\n172\n\n\nGuest did not opt to re-use towel\n124\n137\n261\n\n\nTotal\n222\n211\n433\n\n\n\n\nUsing any one of the calculated Z-scores (they should all three be very close to one another), find the associated two-sided p-value (use xpnorm()) and conclude the problem in the words of the study.\n\n\n\nQ3. Effectiveness of AZT (cont.) (HW 3.12 ISCAM)\nIn 1993, one of the first studies aimed at preventing maternal transmission of AIDS to infants gave the drug AZT to pregnant, HIV-infected women (Connor et al., 1994). Roughly half of the women were randomly assigned to receive the drug AZT, and the others received a placebo (a “fake” treatment, same appearance as the drug but with no active ingredients). The HIV-infection status was then determined for 363 babies, 180 from the AZT group and 183 from the placebo group. Of the 180 babies whose mothers had received AZT, 13 were HIV-infected, compared to 40 of the 183 babies in the placebo group.\n\nCalculate and interpret the relative risk of HIV positive baby comparing the placebo group (numerator) to the AZT group (denominator).\nSuggest why the researchers might prefer to look at the relative risk in the study rather than the difference in conditional proportions.\n[Part (c) won’t be asked on the exam, we might get to it after spring break.] Use the mathematical approximation (i.e., the normal distribution) to calculate and interpret a 95% confidence interval for the relative risk of HIV transmission between the population of placebo takers and the population of AZT takers. (Include/show your work.)\nBriefly explain, in your own words, why we are working with the log of the relative risk in the calculations.\nUse the bootstrap process to calculate and interpret a 95% confidence interval for the relative risk of HIV transmission between the population of placebo takers and the population of AZT takers. (Include/show your work.)\nBased on your confidence interval, do the data provide convincing evidence that placebo takers are more likely to transmit HIV to their babies than AZT takers? Explain how you are deciding.\nTo what population are you willing to generalize the results?\nDoes the design of the study allow you to conclude that use of AZT causes a lower risk of HIV-transmission? Justify your answer.\n\n\npraise()\n\n[1] \"You are geometric!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/MA58_HW8s23.html",
    "href": "slides/MA58_HW8s23.html",
    "title": "HW 8 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nchi-squared tests of independence\n\n\nQ1. Learning Community Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. True or false, Part I1\nDetermine if the statements below are true or false. For each false statement, suggest an alternative wording to make it a true statement. (a) The chi-squared distribution, just like the normal distribution, has two parameters, mean and standard deviation. (b) The chi-squared distribution is always right skewed, regardless of the value of the degrees of freedom parameter. (c) The chi-squared statistic is always positive. (d) As the degrees of freedom increases, the shape of the chi-squared distribution becomes more skewed.\n\n\nQ3. True or false, Part II2\nDetermine if the statements below are true or false. For each false statement, suggest an alternative wording to make it a true statement.\n\nAs the degrees of freedom increases, the mean of the chi-squared distribution increases.\nIf you found \\(X^2 = 10\\) with df = 5 you would fail to reject \\(H_0\\) at the 5% significance level. [Use xpchisq().]\nWhen finding the p-value of a chi-squared test, we always shade the tail areas in both the upper and the lower tails.\n\n\n\nQ4. US Volunteerism I3\nThe 2003 study on volunteerism conducted by the Bureau of Labor Statistics reported the sample percentages who performed volunteer work, broken down by many other variables. For example, respondents were categorized by age. The following reports the percentage of sample respondents in each age group who had performed volunteer work in the previous year:\n\n\n\n\n\n\n\n\n\n\n\n\nAge group\n16–24 years\n25–34 years\n35–44 years\n45–54 years\n55–64 years\n65 or more\n\n\n\n\n% volunteer\n21.9%\n24.8%\n34.1%\n31.3%\n27.5%\n22.7%\n\n\n\n\nIs this information sufficient to construct a segmented bar graph for comparing the proportions of volunteers across the various age categories?\nExplain why the information (the group proportions only) is not sufficient to conduct a chi-squared test of whether these sample proportions differ significantly across the age categories.\n\nThe sample sizes in each age group are not given in the report, but based on other information we can estimate them to be as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nAge group\n16–24 years\n25–34 years\n35–44 years\n45–54 years\n55–64 years\n65 or more\n\n\n\n\nSample size\n9719\n10613\n12070\n10959\n7329\n9310\n\n\n\n\nUse this information to produce a table of counts with age groups in rows and volunteer status (yes or no) in columns. Make sure that you know how to create the 6x2 table of counts by hand. You can use the table() function in R.\n\n\nvol_data <- matrix(c(2128, 2632, 4116, 3430, 2015, 2113, 7591, 7981, 7954, 7529, 5314, 7197), \n                   ncol=6, byrow=TRUE)\nvol <- rep(rep(c(\"Vol\", \"noVol\"), 6), times = vol_data)\nyear <- rep(rep(c(\"y1624\",\"y2534\",\"y3544\",\"y4554\",\"y5564\", \"y65plus\"), each = 2), times = vol_data)\nvolunteer <- data.frame(vol, year)\n\n\nCreate a segmented barplot. The x-axis should be the age (look at the data to see what the variables are called!), the fill should be the information about whether or not each person volunteers\n\nThe R code will look something like this:\nggplot(___) +\n  geom_bar(aes(x = ___, fill = ____ ), position = \"fill\") +\n  xlab(\"Age Group\")+\n  ylab(\"Percent\")\n\nConduct the chi-squared test. Report the hypotheses, check of technical conditions (for the distribution to work out, we need at least 5 observations in each cell of the table), sampling distribution, chi-squared test statistic, and chi-squared p-value. (Provide the details of your calculations and/or relevant computer output.) Summarize your conclusion.\n\nThe R code for running a chi-squared test will look something like this:\nyour_data %>%\n  select(var_1, var_2) %>%\n  table() %>%\n  chisq.test()\n\nConstruct a 6 \\(\\times\\) 2 table with the same row and column headings as in (c), but containing only + and – signs indicating whether the observed count is larger (+) or smaller (–) than expected in that cell. Does this table reveal a pattern? Explain what that pattern suggests about the relationship between age group and volunteerism.\n\nNote that if we keep (using <-) the output of chisq.test, we can pull out the observed and expected tables from the output. Try the following: name_of_your_X2_output$observed and name_of_your_X2_output$expected.\n\nRun a randomization test which uses the same test statistic (\\(X^2\\)) but does not use the chi-squared probability (mathematical) distribution to find a p-value. Note that the change from “chi-squared test” to “randomization test” is not the statistic you calculate (the statistic is the same as the full chi-squared test above!). The difference is how the p-value is calculated (mathematical function vs. computer simulation). Ask if that doesn’t make sense! The data are permuted (shuffled) to find the sampling distribution of the \\(X^2\\) statistic assuming \\(H_0\\) is true.\n\nThe only change to previous randomization tests with this syntax (e.g., type = \"permute\") is that the statistic is now specified as stat = \"Chisq\".\nAs with the mathematical chi-squared test, for the permutation test: state the hypotheses, find the observed \\(X^2\\) value, calculate the p-value, and provide a conclusion in words of the problem.\nNote: although the two tests use the same statistic (\\(X^2\\)), the name of the test comes from the mathematical distribution which is called the “chi-squared distribution.” When we use the computational method (and not the mathematical distribution), we no longer call it a chi-sq test. For ease in understanding, the developers of the infer package that we use call the \\(X^2\\) statistic “Chisq” because it is the same statistic used in the Chi-sq test.\n\n\nQ5. US Volunteerism II4\nReconsider the previous question about volunteerism. Suppose that the sample sizes had all been smaller by a factor of 100 (so that the entire study included only about 600 subjects) but that the conditional proportions of volunteerism within each age group had all turned out the same.\n\nHow (if at all) would you expect the segmented bar graph to change? Explain.\nHow (if at all) would you expect the test statistic to change? Explain.\nHow (if at all) would you expect the p-value to change? Explain.\nHow (if at all) would you expect your conclusion to change? Explain.\nRepeat the chi-squared analysis (no need to repeat the randomization test) with this greatly reduced sample size (round the observed counts in the new table to the nearest integer). Confirm or correct your answers to (b)–(d) in light of this analysis.\n\n\npraise()\n\n[1] \"You are mathematical!\"\n\n\n\n\n\n\n\n\nFootnotes\n\n\nFrom OpenIntro Statistics, exercise 3.37↩︎\nFrom OpenIntro Statistics, exercise 3.38↩︎\nFrom ISCAM, HW 5.6↩︎\nFrom ISCAM, HW 5.7↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "clicker_slides.html#approx-35sqrt100",
    "href": "clicker_slides.html#approx-35sqrt100",
    "title": "Introduction to Biostatistics",
    "section": "[^111]: 5 ( \\(\\approx 35/\\sqrt{100}\\))",
    "text": "[^111]: 5 ( \\(\\approx 35/\\sqrt{100}\\))\n\nThe standard deviation of average weights (mean = 167 lbs) in a sample of size 1000 is approximately109\n\n1\n5\n10\n35\n100"
  }
]