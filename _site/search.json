[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester! Each week, follow the general process outlined below:\n\nBefore class on Tuesday, read the suggested article \nEnjoy the notes / readings \nAttend class, review the warm-up  if you have any questions after completing it during class.\nComplete the Lab  and HW  assignments\n\nLab due every Tuesday by 11:59pm\nHW due every Thursday by 11:59pm\n\nDiscuss the reflection questions  and ethics considerations  (see class notes) with your classmates, mentor, and professor\nIMS: Introduction to Modern Statistics by Çetinkaya-Rundel and Hardin.\nISCAM: Investigating Statistics, Concepts, Applications, and Methods by Chance and Rossman. Many of the examples come from ISCAM, a pdf of which can be purchased for $5.\n\nISCAM applets\nStatKey (boostrapping) applets\n\n\n\n\n\n\n\n\n\n \n  \n    date \n    topic \n    agenda \n    readings \n    article (Tues) \n    assignments \n  \n \n\n  \n    Week 1  1.17.23 \n    Intro +  variables &   studies  SLR \n    • course info  • tests  • studies  • causation \n     Introduction   Studies    ISCAM 1.1   IMS 1 + 2 \n     none  Felicity Enders \n     WU1   WU2   HW0 pdf   HW0 Rmd \n  \n  \n    Week 2  1.24.23 \n    Correlation + Leastsquares \n    • correlation • linear model  • R^2 \n     Cor   Least Sq    ISCAM 5.6, 5.7, 5.8   IMS 7   guess corr   least sq \n     AstraZeneca  W.E.B. Du Bois \n     WU3   WU4   HW1 pdf   HW1 Rmd   Lab1 pdf   Lab1 Rmd \n  \n  \n    Week 3  1.31.23 \n    Hyp Test + RandomizationTest \n    •  structure of hypothesis testing \n     Tests    2x2 Rand   IMS 11 \n     chocolate  \n     WU5   WU6   HW2 pdf   HW2 Rmd   Lab2 pdf   Lab2 Rmd \n  \n  \n    Week 4  2.7.23 \n    Bootstrapping \n    • boot samp dist  • boot CI \n     Bootstrapping   Boot CIs    StatKey    CIs   IMS 12 \n     confounding  \n     \n  \n  \n    Week 5  2.14.23 \n    normality +CLT \n    • normal dist • CLT • Z-score  • empirical rule  • conf int  • norm prob  • hyp test \n    CLT    Norm dist    Samp Dist    IMS  \n     efficacy   vaccines \n     \n  \n  \n    Week 6  2.21.23 \n    Errors + Power +Sampling \n    • Type I error  • Type II error  • Power  • biased samples \n     Errors   Sampling    Power   IMS 14 + 2 \n     type II   type I \n     \n  \n  \n    Week 7  2.28.23 \n    Rel Risk +Odds Ratios \n    •  RR  • OR • Case-control  • Cohort \n     diff props   RR    OR    ISCAM 3.9-3.11   IMS 17 \n     interaction  \n     \n  \n  \n    Week 8  3.7.23 \n    catch-up \n     \n     study materials  (see Canvas) \n     \n     \n  \n  \n    3.9.23 \n    Exam 1 \n     \n     \n     \n     \n  \n  \n    3.14.23 \n    spring break \n     \n     \n     \n     \n  \n  \n    Week 9  3.21.23 \n    χ^2 Test \n    • expected counts  • X^2 statistic  • χ^2 test \n     χ^2 Test    ISCAM 3.2   IMS 18 \n     Data sovereignty  \n     \n  \n  \n    Week 10  3.28.23 \n    1 mean +pred interval \n    • dist of X-bar  • SE(X-bar)  • t-test of μ • t-CI for μ  • PI for X_{n+1} \n     1 mean   pred int    IMS 19 \n     masks  \n     \n  \n  \n    3.28.23 \n    Proj 1: Pilot \n     \n     \n     \n     Proj 1: Pilot \n  \n  \n    Week 11  4.4.23 \n    2 means +t-proced \n    • dist of X-bar_1 - X-bar_2  • SE(X-bar_1 - X-bar_2)  • t-test of μ_1 - μ_2  • t-CI for μ_1 - μ_2 \n     2 means    2 samples   IMS 20 \n     clinical trials \n     \n  \n  \n    4.4.23 \n    Proj 2: Power Analysis \n     \n     \n     \n     Proj 2: Power Analysis \n  \n  \n    Week 12  4.11.23 \n    ANOVA \n    • MSG  • MSE • F-test \n     ANOVA    Inf on β_1    slopes   IMS 22   IMS 24 \n     p-hacking  \n     \n  \n  \n    Week 13  4.18.23 \n    Inference β_1 + Tech cond + MLR \n    •  SE(β_1)  • residual plots  • multiple X \n     Tech Cond    MLR   IMS 25 \n     1918 flu  \n     \n  \n  \n    4.18.23 \n    Proj 3: Analysis \n     \n     \n     \n     Proj 3: Preliminary Analysis \n  \n  \n    Week 14  4.25.23 \n    catch-up \n     \n     study materials  (see Canvas)   when to use what \n     \n     \n  \n  \n    4.27.23 \n    Exam 2 \n     \n     \n     \n     when to use what \n  \n  \n    5.2.23 \n    Proj 4: Full draft \n     \n     \n     \n     Proj 4: Full draft \n  \n  \n    5.4 + 12.23 \n    Proj Presentation \n     \n     \n     \n     Proj Presentation \n  \n  \n    5.12.23 \n    Proj 5: Final Project \n     \n     \n     \n     Proj 5: Final Project \n  \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Semester Project",
    "section": "",
    "text": "Artwork by @allison_horst."
  },
  {
    "objectID": "project.html#pilot",
    "href": "project.html#pilot",
    "title": "Semester Project",
    "section": "Pilot",
    "text": "Pilot\nThe pilot project will be based on a preliminary data collection. Details about the structure of the data are given in the assignment, note that the response variable must be numerical. You will have an opportunity to collect more data as we go, but the better the original data are, the better the follow-up data collection will be. The goal of the pilot study assignment is to develop a preliminary research question and to collect some initial data, both in preparation for your group Island project."
  },
  {
    "objectID": "project.html#power",
    "href": "project.html#power",
    "title": "Semester Project",
    "section": "Power",
    "text": "Power\nThe power analysis will use the pilot data to estimate the sample size which will be needed to see the effect of interest. Most of the code will be written by the professor, but you will need to understand both why the power analysis is important and how the power analysis is implemented. The goal of the power analysis assignment is to estimate how many observations are needed for an Islands project to reach significance."
  },
  {
    "objectID": "project.html#preliminary-analysis",
    "href": "project.html#preliminary-analysis",
    "title": "Semester Project",
    "section": "Preliminary Analysis",
    "text": "Preliminary Analysis\nThe preliminary analysis will contain numerical and graphical summaries. Additionally, one regression model should be run. The goal of the preliminary data analysis assignment is to start working with the variables including visualizations, numerical summaries, and modeling."
  },
  {
    "objectID": "project.html#full-draft",
    "href": "project.html#full-draft",
    "title": "Semester Project",
    "section": "Full Draft",
    "text": "Full Draft\nThe complete draft of the project will contain a full set of analyses designed to answer the research question (and practice with multiple regression). The paper will continue to have sections which communicate the study background and methods to the reader. All plots and tables should be well labelled. The goal of the assignment is to provide a complete analysis of the data collected on the Islands as a research project."
  },
  {
    "objectID": "project.html#presentation",
    "href": "project.html#presentation",
    "title": "Semester Project",
    "section": "Presentation",
    "text": "Presentation\nThe presentation will bring together your work and showcase your ideas. You will have 5 minutes to report on your study, using slides (visualizations from your analyses). Each group member will be expected to talk. The goal of this assignment is to synthesize your study and your results into a clear and succinct presentation to your peers.\nYour group may choose whether to present during reading days (May 6, 2022) or during the time allocated to our course for the final exam (May 13, 2022)."
  },
  {
    "objectID": "project.html#final-project",
    "href": "project.html#final-project",
    "title": "Semester Project",
    "section": "Final Project",
    "text": "Final Project\nThe final project will compile all of the project pieces together into a beautiful and clear report. The paper will have sections which communicate the study background and methods to the reader as well a conclusions of the experiment. All plots and tables should be well labelled. The goal of the assignment is to provide a complete analysis of the data collected on the Islands as a research project.\nAlong with the final project report, each student will fill out an individual reflection piece and a group dynamic report."
  },
  {
    "objectID": "roles.html",
    "href": "roles.html",
    "title": "Group Dynamic Roles",
    "section": "",
    "text": "Artwork by @allison_horst."
  },
  {
    "objectID": "roles.html#roles-for-the-projects",
    "href": "roles.html#roles-for-the-projects",
    "title": "Group Dynamic Roles",
    "section": "Roles for the Projects",
    "text": "Roles for the Projects\nEach individual will have (at least) 2 roles; one role to help foster the group dynamics and the other role to divvy up the responsibilities involved in completing the project.\n\nGroup Dynamic Roles:\n\nProject Manager: Makes sure that the group is organized and has a clear plan for completing the project. This includes scheduling meetings and having a plan for what needs to be done before the next meeting.\n\nTask Manager: Makes sure that everyone knows what they are expected to do before the next meeting and makes sure that they do it. This might involve calling or emailing each person between meetings to discuss what they have/haven’t done. If someone can’t do the work that needs to be done, the task manager is responsible for calling another meeting if needed.\n\nFacilitator: Makes sure that every member of the group is participating and being listened to and heard. This might involve asking questions of a member that’s been silent and stopping others when someone’s comment is being overlooked.\n\n\n\nProject Roles\nIn each case, the person assuming that role is responsible for that aspect of the project. It doesn’t mean that they will do all that part of the project by themselves; it means that they are responsible for dividing that work up among the members of the group and ensuring that it is done and recorded correctly.\n\nDirector of Research: Is responsible for the literature searches. The Director of Research identifies what needs to be searched for in the literature, divvies up the literature searches to be performed among the group members, and coordinates changes in the searches based on information gathered and changes in direction. They are also responsible for making sure that the citations in the project are complete and accurate.\n\nDirector of Computation: Is responsible for the computer programs involved in the project. The Director of Computation is responsible for designing the code so that different people can write different parts of the code. The programmer is responsible for making sure that any code written by different people can be integrated.\n\nReporter: Is responsible for the written report. This involves taking notes during the complete process in order to keep a record of what has been done. The reporter may also gather everyone’s individual notes and put them together. The reporter is also responsible for editing the final report and making sure that the various pieces (that may have been written by different people) fit well together.\n\n\n\nThings to think about:\n\nIf all members of a group think that one member isn’t pulling their weight then you can come and talk to me about it. I can fire that member and have them do the project on their own.\n\n(Generally) All members of the group get the same grade for the project.\n\nGrade on projects (a rubric for grading will be posted soon, but generally the grade will be based on the following):\n\nGrade for technical depth and sophistication.\n\nGrade for quality of write-up (organization, clarity, grammatical correctness, appropriate use of graphs, tables, formulas).\n\nGrade for quality of oral presentation (organization, clarity, appropriate use of graphs, tables formula, ability to answer questions).\n\nGrade for quality of group work and distribution of labor.\n\n\nAttendance: you should keep track of who is or isn’t showing up to group project meetings\n\nStudents should keep a record of all the times that they worked on the project and the work that they did. Every time that you spend more than 15 minutes on the project you should write down the start and stop time and what you did. Group members should be spending roughly the same amount of time on the project (I will ask you to report on time spent on the project so far at the check-in in November.)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "syllabus",
    "section": "",
    "text": "Class: Tuesdays & Thursdays, 9:35 - 10:50 am\nLab: Fridays, 11-11:50am\nJo Hardin\n2351 Estella\njo.hardin@pomona.edu\n\n\nMonday 1:30-3pm\nTuesday 2:30-3:30pm\nWednesday 9-11am\nThursday 3-4pm\nor by appointment\n\n\nMentors: Sara Colando, Anu Krishnan, Naomi Meurice, and Kyle Torres\nMonday 8-10pm\nWednesday 6-8pm\nEstella 2131\n\n\n\n\n\nArtwork by @allison_horst.\n\n\n\n\n\n\n\n\n\nIntroduction to Biostatistics is an introduction to statistical ideas using R. We will cover the majority of statistical methods which are used in standard analyses (e.g., t-tests, chi-squared analysis, regression, confidence intervals, binomial tests, etc.). The main inferential techniques will be covered using both theoretical approximations (e.g., the central limit theorem) as well as computational methods (e.g., randomization tests and bootstrapping). Focus will be on understanding the methods and interpreting results.\n\n\n\n\n\n\nAnonymous Feedback\n\n\n\nAs someone who is, myself, constantly learning and growing in many ways, I welcome your feedback about the course, the classroom dynamics, or anything else you’d like me to know. There is a link to an anonymous feedback form on the landing page of our Canvas webpage. Please provide me with feedback at any time!\n\n\n\n\n\nBy the end of the semester, students will be able to do the following:\n\nGiven a study, identify population, sample, parameter, statistic, observational unit, and variable.\nDescribe the differences between, benefits of each, and conclusions which can be drawn in observational studies versus experiments.\nGiven a dataset and research query, create an appropriate figure in R.\nGiven a dataset and research query, compute appropriate statistics in R.\nDescribe the difference between the distribution of a sample of data and a sampling distribution of a particular statistic.\nFor a particular research question, identify whether the task requires descriptive analysis / model, graphic, confidence interval, or hypothesis test,\nApply the empirical rule to as an approximation to confidence intervals and hypothesis testing in settings of means and proportions.\nBe able to describe in words what a p-value is and what it is not.\nWrite down appropriate null and alternative hypotheses, and choose the correct analysis technique.\nRun the hypothesis test / confidence interval analysis in R.\nIdentify when it is and when it is not appropriate to summarize the relationship between two variables using a least squares line. Describe the optimization procedure the leads to a least squares fit (although not necessarily to do the calculations).\nProvide the settings in which a causal claim is warranted, and when a strong correlation is possibly due to spurious relationships.\nUse a regression line to make predictions and distinguish between a prediction interval for an independent response as compared to a confidence interval for the slope parameter.\nFor each descriptive analysis, visualization, confidence interval, or hypothesis test, in words communicate the conclusion of the analysis in the original context of the data.\nUse R Markdown to run reproducible analyses that include all aspects of the data analysis.\n\n\n\n\nIn an ideal world, science would be objective. However, much of science is subjective and is historically built on a small subset of privileged voices. In this class, we will make an effort to recognize how science (and statistics!) has played a role in both understanding diversity as well as in promoting systems of power and privilege. I acknowledge that there may be both overt and covert biases in the material due to the lens with which it was written, even though the material is primarily of a scientific nature. Integrating a diverse set of experiences is important for a more comprehensive understanding of science. I would like to discuss issues of diversity in statistics as part of the course from time to time.\nPlease contact me if you have any suggestions to improve the quality of the course materials.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official records, please let me know!\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. You can also relay information to me via your mentors. I want to be a resource for you. If you prefer to speak with someone outside of the course, the math liaisons, Dean of Students, or QSC staff are all excellent resources.\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. As a participant in course discussions, you should also strive to honor the diversity of your classmates.\n\n\n\n\n\n\nIMS: Introduction to Modern Statistics by Çetinkaya-Rundel and Hardin.\nISCAM: Investigating Statistics, Concepts, Applications, and Methods by Chance and Rossman. Many of the examples come from ISCAM, a pdf of which can be purchased for $5.\n\nISCAM applets\nStatKey (boostrapping) applets\n\n\n\n\n\n\n\n\nImportant dates\n\n\n\n\n3/9/23 (Thursday) Exam 1\n3/28/23 (Tuesday) Project 1: Pilot\n4/4/23 (Tuesday) Project 2: Power\n4/18/23 (Tuesday) Project 3: Analysis\n4/27/23 (Thursday) Exam 2\n5/2/23 (Tuesday) Project 4: Full draft\n5/4 or 12/23 (Friday) Project presentation\n5/12/23 (Friday) Project 5: Final (edited) Project\n\n\n\n\n\n\n\nEnough R\nR tutorial\nGreat tutorials through the Coding Club\nA true beginner’s introduction to the tidyverse, the introverse.\nfor a good start to R in general\nA fantastic ggplot2 tutorial\nGreat tutorials through the Coding Club\nGoogle for R\nsome R ideas that I wrote up\nIncredibly helpful cheatsheets from RStudio.\n\ndata wrangling\nggplot2\nR Markdown\nRStudio IDE\n\n\n\n\n\nR will be used for all homework assignments. You can use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to get Pomona login information.)\nAlternatively, feel free to download R onto your own computer. R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, you are required to install RStudio and turn in all R assignments using RMarkdown. http://rstudio.org/. (You can use the LaTeX compiler at: https://yihui.name/tinytex/)\n\n\n\nThis course uses Canvas as the main learning management system. The Canvas login is http://canvas.pomona.edu/. If you haven’t used Canvas before, I recommend bookmarking Canvas Student Guides and Canvas Student Videos for easy reference to tips and tutorials. If you run into an issue with Canvas, help is available.\n\nFrom anywhere in Canvas, select the Help button, located in the blue Global Navigation menu on the left.\n\nClick on Pomona Service Desk - Canvas Support to report a problem by submitting a service request ticket. Be sure to include “Canvas Issue” in your subject line.\nFor additional assistance, you can click on Ask Your Instructor or simply send me an email.\n\n\nPlease be proactive and reach out for help as soon as possible to resolve the issue you are experiencing.\n\n\n\n\n\n\nThere are no statistics prerequisites for this class; nor is there an R prerequisite. Instead, the prerequisite is that you should feel comfortable with mathematical notation and computational thinking. Although we do not actually do any calculus, having had at least a semester of calculus indicates that you are at the right level mathematically.\n\n\n\nHomework will be assigned from the text and due every Wednesday at 11:59pm. One homework grade will be automatically dropped, so there are no late assignments. Homework will be turned in via Gradescope on Canvas.\n\n\nHomework assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n[5] All problems completed with detailed solutions provided and 75% or more of the problems are fully correct. Additionally, there are no extraneous messages, warnings, or printed lists of numbers.\n[4] All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. Or all problems are completed and there are extraneous messages, warnings, or printed lists of numbers.\n[3] Close to all problems completed with less than 75% correct.\n[2] More than half but fewer than all problems completed and > 75% correct.\n[1] More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed.\n[0] No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\n\n\nThere will be a semester long group project. Your task is to use data to tell us something interesting. This project is deliberately open-ended to allow you to fully explore your creativity. There are three main rules that must be followed: (1) data centered, (2) tell us something, (3) do something new. The project information is available here: Math 58B Semester Project\n\n\n\n\nThis class will be interactive, and your participation is expected (every day in class). Although notes will be posted, your participation is an integral part of the in-class learning process.\nIn class: after answering one question, wait until 5 other people have spoken before answering another question. [Feel free to ask as many questions as often as you like!]\n\n\n\n\nThroughout the semester, you will be challenged, and you may find yourself stuck. Every single one of us has been there, I promise. Below, I’ve provided Pomona’s academic honesty policy. But before the policy, I’ve given some thoughts on cheating which I have taken from Nick Ball’s CHEM 147 Collective (thank you, Prof Ball!). Prof Ball gives us all something to think about when we are learning in a classroom as well as on our journey to become scientists and professionals:\n\n\n\n\n\n\nWhy Cheat?\n\n\n\nThere are many known reasons why we may feel the need to “cheat” on problem sets or exams:\n\nAn academic environment that values grades above learning.\nFinancial aid is critical for remaining in school that places undue pressure on maintaining a high GPA.\nNavigating school, work, and/or family obligations that have diverted focus from class.\nChallenges balancing coursework and mental health.\nBalancing academic, family, peer, or personal issues.\n\nBeing accused of cheating – whether it has occurred or not – can be devastating for students. The college requires me to respond to potential academic dishonesty with a process that is very long and damaging. As your instructor, I care about you and want to offer alternatives to prevent us from having to go through this process.\n\n\nIf you find yourself in a situation where “cheating” seems like the only option, please come talk to me. We will figure this out together.\nPomona College is an academic community, all of whose members are expected to abide by ethical standards both in their conduct and in their exercise of responsibilities toward other members of the community. The college expects students to understand and adhere to basic standards of honesty and academic integrity. These standards include, but are not limited to, the following:\n\nIn projects and assignments prepared independently, students never represent the ideas or the language of others as their own.\nStudents do not destroy or alter either the work of other students or the educational resources and materials of the College.\nStudents neither give nor receive assistance in examinations.\nStudents do not take unfair advantage of fellow students by representing work completed for one course as original work for another or by deliberately disregarding course rules and regulations.\nIn laboratory or research projects involving the collection of data, students accurately report data observed and do not alter these data for any reason.\n\n\n\n\nPlease email and / or set up a time to talk if you have any questions about or difficulty with the material, the computing, or the course. Talk to me as soon as possible if you find yourself struggling. The material will build on itself, so it will be much easier to catch up if the concepts get clarified earlier rather than later. This semester is going to be fun. Let’s do it.\n\n\n\n\n\n\nGrading\n\n\n\n\n20% Homework\n50% Midterms\n25% Group Project & Presentation\n5% Class Participation"
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Rubric for Semester Project",
    "section": "",
    "text": "The grading rubric tries to lay out the main aspects of the project inluding the analysis parts and the communication parts. Please ask if you have any questions.\n\n\n\n\nThe introduction should include:\n\nBrief background (Has an explanation been given for why the research was done? Why is the work important? What is its relevance?)\nObjective or Research question (Is the brief description of the hypothesis/goals and findings of the paper clearly stated for the reader?)\n\n\n\n\nMethods should include description of:\n\nStudy design (enough info for someone to replicate)\nSampling method and eligibility criteria\nTreatment (i.e., explanatory variable)\nOutcome definition(s) (i.e., response variable)\nSample size justification\nStatistical methods which were applied\n\n\n\n\nResults should communicate / describe / summarize:\n\nBaseline data (including numbers analyzed – explain the explanatory variables, summarize the explanatory variables, plot the explanatory variables, etc.)\nOutcomes (explain the response variable, summarize the response variable, plot the response variable). Is your treatment liable to do harm? Are you worried about type I vs type II errors when considering the outcome?\nEach model is completely described\nEach figure / table is completely described\nConnections between analyses; e.g., between figures and model results\nDiscussion\nConclusion (i.e., answer to the research question), interpretation\nLimitations\nIdeas for future studies\n\n\n\n\nFull analysis should be clear, concise, well-organized, reproducible, and engaging:\n\nDataset is complete, available, tidy (rows are observations, columns are variables, no exceptions)\nAll information was provided\nPresentation was organized\nPresentation was clear, understandable\nTables and figures were used well (axes labeled, etc.)\nWork is completely reproducible (the instructor must be able to run the .Rmd file on their own computer)\n\n\n\n\n\nEach of you will evaluate your own work and that of your peers keeping the following topics in mind.\n\nMastery of the material (understanding of the material is strong and well communicated)\nBackground, completeness (thoroughly explains all key points)\nPresentation evaluation (organized, communicates effectively with the audience)"
  },
  {
    "objectID": "clicker_study.html",
    "href": "clicker_study.html",
    "title": "Introduction to Biostatistics",
    "section": "",
    "text": "Clicker Q\nto go with Introduction to Modern Statistics by Çentinkaya-Rundel & Hardin. Math 58B - Introduction to Biostatistics.\n\n\n\nIf 16 infants with no genuine preference choose 16 toys, what is the most likely number of “helping” toys that will be chosen?1\n\n4\n7\n8\n9\n10\n\n\n\n\nHow likely is it that exactly 8 helpers will be chosen (if there is no preference)?2\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nWhat if we flipped a coin 160 times? What percent of the time will the simulation flip exactly 80 heads?3\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nIs our actual result of 14 (under the coin model)…4\n\nvery surprising?\nsomewhat surprising?\nnot very surprising?\n\n\n\n\nBased on the first handwriting study, can we conclude that cursive causes higher scores (on average)?5\n\nYes\nNo\nIt depends\n\n\n\n\nBased on the second handwriting study, can we conclude that cursive causes higher scores (on average)?6\n\nYes\nNo\nIt depends\n\n\n\n\nA possible confounding variable for the handwriting study is:7\n\ngrade of the student (age)\nregion of country where the SAT was taken\nacademic ability of the student\ngender identity of the student\nnumber of siblings of the student.\n\n\n\n\nThe main reason we randomly assign the explanatory variable is:8\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nThe main reason we take random samples from the population is:9\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the expected causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nAre there effects of second-hand smoke on the health of children?10\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people tend to spend more money in stores located next to food outlets with pleasing smells?11\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDoes cell phone use increase the rate of automobile accidents?12\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people consume different amounts of ice cream depending on the size of bowl used?13\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nWhich is more effective: diet A or diet B?14\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored ten points higher on the final than on the midterm:15\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored five points lower on the final than on the midterm:16\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored twice as many points on the final than on the midterm:17\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose you guessed every value correctly (guess the correlation applet), what would be the value of the correlation coefficient between your guesses and the actual correlations?18\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose each of your guesses was too high by 0.2 from the actual value of the correlation coefficient, what would be the value of the correlation coefficient between your guesses and the actual correlations?19\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nA correlation coefficient equal to 1 indicates that you are a good guesser.20\n\nTRUE\nFALSE\n\n\n\n\nPerfect Correlation… if not for a single outlier\nn = 101 observations: 1 observation in top left, 25 observations in each in of the points near the bottom right.\nThe value of the correlation, r, is:21\n\n-1 < r < -0.9\n-0.9 < r < -0.5\n-0.5 < r < 0.5\n0.5 < r < 0.9\n0.9 < r < 1\n\n\n\n\n\n\n\n\n\n\nThe sum of residuals from the sample mean (no X):22 \\[\\sum_{i=1}^n(Y_i - \\overline{Y})\\]\n\nis positive\nis negative\nis zero\nis different for every dataset\n\n\n\n\nA good measure of how well the prediction (of the sample mean) fits the data is:23\n\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})\\)\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\overline{Y}|\\)\n\\(\\mbox{median}(Y_i - \\overline{Y})\\)\n\\(\\mbox{median}|Y_i - \\overline{Y}|\\)\n\n\n\n\nA good measure of how well the prediction (of the regression line) fits the data is:24\n\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)\\)\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\hat{Y}_i|\\)\n\\(\\mbox{median}(Y_i -\\hat{Y}_i)\\)\n\\(\\mbox{median}|Y_i -\\hat{Y}_i|\\)\n\n\n\n\nWhat math is used to find the value of \\(m\\) that minimizes:25 \\[\\sum_{i=1}^n(Y_i - m)^2\\]\n\ncombinatorics\nderivative\nintegral\nlinear algebra\n\n\n\n\n\\(\\sum_i(Y_i - \\overline{Y})^2\\) is sometimes \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)26\n\nTRUE\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is never \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?27\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\n\n8\n\n↩︎\n\n0.196 (19.6% of the time)\n\n↩︎\n\n0.063 (6.3% of the time)\n\n↩︎\n\nvery surprising (prob of 14 or more is 0.0021)\n\n↩︎\n\nNo, we can’t establish causation from an observational study.\n\n↩︎\n\nYes. For the exam(s?) under study, cursive caused higher scores on average.\n\n↩︎\nYou must connect the variable to both the explanatory and response variable. For me, that is easiest to do with c. academic ability of the student.↩︎\n\nTo balance every possible variable except the causal mechanism across the two groups\n\n↩︎\n\nSo that our sample is representative of the population\n\n↩︎\n\nunhappily obs study (becuase we want to establish causation)\n\n↩︎\n\ndefinitely obs study (do we care about causation? maybe. maybe not.)\n\n↩︎\n\nunhappily obs study\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nFALSE. You could get every single value wrong and still have a correlation of one.\n\n↩︎\n\nr = -0.416\n\n↩︎\n\nalways zero\n\n↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\n\nderivative\n\n↩︎\n\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "clicker.html",
    "href": "clicker.html",
    "title": "Introduction to Biostatistics",
    "section": "",
    "text": "Clicker Q\nto go with Introduction to Modern Statistics by Çentinkaya-Rundel & Hardin. Math 58B - Introduction to Biostatistics.\n\n\n\nIf 16 infants with no genuine preference choose 16 toys, what is the most likely number of “helping” toys that will be chosen?1\n\n4\n7\n8\n9\n10\n\n\n\n\nHow likely is it that exactly 8 helpers will be chosen (if there is no preference)?2\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nWhat if we flipped a coin 160 times? What percent of the time will the simulation flip exactly 80 heads?3\n\n0-15%\n16-30%\n31-49%\n50%\n51-100%\n\n\n\n\nIs our actual result of 14 (under the coin model)…4\n\nvery surprising?\nsomewhat surprising?\nnot very surprising?\n\n\n\n\nBased on the first handwriting study, can we conclude that cursive causes higher scores (on average)?5\n\nYes\nNo\nIt depends\n\n\n\n\nBased on the second handwriting study, can we conclude that cursive causes higher scores (on average)?6\n\nYes\nNo\nIt depends\n\n\n\n\nA possible confounding variable for the handwriting study is:7\n\ngrade of the student (age)\nregion of country where the SAT was taken\nacademic ability of the student\ngender identity of the student\nnumber of siblings of the student.\n\n\n\n\nThe main reason we randomly assign the explanatory variable is:8\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nThe main reason we take random samples from the population is:9\n\nTo get the smallest p-value possible\nTo balance the expected causal mechanism across the two groups\nTo balance every possible variable except the expected causal mechanism across the two groups\nSo that our sample is representative of the population\nSo that the sampling process is unbiased\n\n\n\n\nAre there effects of second-hand smoke on the health of children?10\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people tend to spend more money in stores located next to food outlets with pleasing smells?11\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDoes cell phone use increase the rate of automobile accidents?12\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nDo people consume different amounts of ice cream depending on the size of bowl used?13\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nWhich is more effective: diet A or diet B?14\n\ndefinitely obs study\ndefinitely experiment\nunhappily obs study\nunhappily experiment\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored ten points higher on the final than on the midterm:15\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored five points lower on the final than on the midterm:16\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose that we record the midterm exam score and the final exam score for every student in a class. What would the value of the correlation coefficient be if every student in the class scored twice as many points on the final than on the midterm:17\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose you guessed every value correctly (guess the correlation applet), what would be the value of the correlation coefficient between your guesses and the actual correlations?18\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nSuppose each of your guesses was too high by 0.2 from the actual value of the correlation coefficient, what would be the value of the correlation coefficient between your guesses and the actual correlations?19\n\nr = -1\n-1 < r < 0\nr = 0\n0 < r < 1\nr = 1\n\n\n\n\nA correlation coefficient equal to 1 indicates that you are a good guesser.20\n\nTRUE\nFALSE\n\n\n\n\nPerfect Correlation… if not for a single outlier\nn = 101 observations: 1 observation in top left, 25 observations in each in of the points near the bottom right.\nThe value of the correlation, r, is:21\n\n-1 < r < -0.9\n-0.9 < r < -0.5\n-0.5 < r < 0.5\n0.5 < r < 0.9\n0.9 < r < 1\n\n\n\n\n\n\n\n\n\n\nThe sum of residuals from the sample mean (no X):22 \\[\\sum_{i=1}^n(Y_i - \\overline{Y})\\]\n\nis positive\nis negative\nis zero\nis different for every dataset\n\n\n\n\nA good measure of how well the prediction (of the sample mean) fits the data is:23\n\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})\\)\n\\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\overline{Y}|\\)\n\\(\\mbox{median}(Y_i - \\overline{Y})\\)\n\\(\\mbox{median}|Y_i - \\overline{Y}|\\)\n\n\n\n\nA good measure of how well the prediction (of the sample mean) fits the data is:24\n\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)\\)\n\\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\)\n\\(\\sum_{i=1}^n|Y_i - \\hat{Y}_i|\\)\n\\(\\mbox{median}(Y_i -\\hat{Y}_i)\\)\n\\(\\mbox{median}|Y_i -\\hat{Y}_i|\\)\n\n\n\n\nWhat math is used to find the value of \\(m\\) that minimizes:25 \\[\\sum_{i=1}^n(Y_i - m)^2\\]\n\ncombinatorics\nderivative\nintegral\nlinear algebra\n\n\n\n\n\\(\\sum_i(Y_i - \\overline{Y})^2\\) is sometimes \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)26\n\nTRUE\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is never \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?27\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\n\n8\n\n↩︎\n\n0.196 (19.6% of the time)\n\n↩︎\n\n0.063 (6.3% of the time)\n\n↩︎\n\nvery surprising (prob of 14 or more is 0.0021)\n\n↩︎\n\nNo, we can’t establish causation from an observational study.\n\n↩︎\n\nYes. For the exam(s?) under study, cursive caused higher scores on average.\n\n↩︎\nYou must connect the variable to both the explanatory and response variable. For me, that is easiest to do with c. academic ability of the student.↩︎\n\nTo balance every possible variable except the causal mechanism across the two groups\n\n↩︎\n\nSo that our sample is representative of the population\n\n↩︎\n\nunhappily obs study (becuase we want to establish causation)\n\n↩︎\n\ndefinitely obs study (do we care about causation? maybe. maybe not.)\n\n↩︎\n\nunhappily obs study\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\ndefinitely experiment\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nr = 1\n\n↩︎\n\nFALSE. You could get every single value wrong and still have a correlation of one.\n\n↩︎\n\nr = -0.416\n\n↩︎\n\nalways zero\n\n↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\overline{Y})^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\nwe usually use b. \\(\\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2\\) (for calculus and historical reasons), but c. and e. are also totally reasonably answers.↩︎\n\nderivative\n\n↩︎\n\nFALSE, \\(\\sum_i(Y_i - \\overline{Y})^2\\) is always \\(\\geq \\sum_i(Y_i - \\hat{Y}_i)^2\\)\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Class notes can be found at http://st47s.com/Math58/Notes/.\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/MA58_HW0s23.html",
    "href": "slides/MA58_HW0s23.html",
    "title": "HW0 – Math 58B",
    "section": "",
    "text": "The syllabus is an agreement between the professor and students. It explains your responsibilities, lays out the structure, and gives you information on how best to achieve the course goals. Like any agreement, it must be read carefully and referenced frequently to answer questions. This fun activity will highlight some important parts of the syllabus and give you a chance to try out R!1\n\nInstructions\nAnswer each question below by typing a response. Knit the file to a compiled pdf, and submit it to Gradescope (via Canvas).\nHint: To compile to pdf, you might need to use the tinytex package (you’ll know you need it if you can knit to html but not to pdf). If that is the case, then in the console (below, do you see where it says “Console”?) only one time ever, type the following. Ask me if it doesn’t work automatically for you. (Don’t spend hours figuring this part out, ask me immediately.)\ninstall.packages('tinytex')\ntinytex::install_tinytex()\nhttps://yihui.name/tinytex/\n\nQ1.\nOn which dates are the exams?\nAfter you’ve marked the exam dates in your calendar, answer the following question: what type of calendar do you keep? (e.g., Google calendar, outlook, paper journal, post-its all over your desk, etc.)\n\n\nQ2.\nProvide three pieces of information from the syllabus related to class participation.\n\n\nQ3.\nWill course notes be available and posted? If so, where?\n\n\nQ4.\nWhat is the software program we are using for the class? After watching the R video on our Box video page (see Canvas for the link), ask one question that you have about R. [If you are already familiar with R, you don’t need to watch the video, but you surely can think up one question about R.]\n\n\nQ5.\nRun the code below one line at a time. Provide a few words describing what each line of code is doing.\nThe words of explanation could come before or after the R chunk, just like any sentences written to a client describing the analysis.\n\nmydata <- c(1:10)  # the words of explanation could come after the hashtag\nmydata\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmydata^2\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\nsample(mydata, size = 25, replace = TRUE)\n\n [1] 10  8  4  8  8  6  3  7  7  2  9  5  9  4  2  8  7  6  7  1  4  8  2  5  9\n\nmydata2 <- sample(mydata, size = 25, replace = TRUE)\nmydata2\n\n [1]  2 10  9  1  6  9  8  7 10  5  2  5  6  4  7  5  1  8  1  2  2  5  3  2  5\n\n\n\n\nQ6.\nWhat are the reflection questions and ethics considerations? Where do you find them? What should you do with them?\n\n\nQ7.\nNice job! Run the chunk of code below. You might need to install the praise package. See the top of this file.\n\npraise()\n\n[1] \"You are marvelous!\"\n\n\nNote: if you want any of your output to remain constant, use the set.seed() function. The function will control the randomness associated with the task you’ve asked of R. For example, you asked R to sample from some integers. Do you want the sample of integers to stay the same every time? Well, use set.seed()! The only argument you need for set.seed() is a single integer. You can choose any integer you want. And the function goes before (either right before or at the top of the file) the command where R is dong something random. Here is an example of some code (which won’t be run because I set eval = FALSE) where the randomness is controlled. Try it yourself in your work above.\n\nset.seed(47)\nsample(mydata, size = 25, replace = TRUE)\n\n\n\n\n\n\n\nFootnotes\n\n\nadapted from David White at Denison University.↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/MA58_HW1s23.html",
    "href": "slides/MA58_HW1s23.html",
    "title": "HW1 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\n(Actually no R code for this assignment!! You should still compile using the R Markdown format.)\nexperiments vs observational studies & confounding variables\n\n\nQ0. Learning Community\nReport one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.).\n\n\nQ1. Eat better, feel better?1\nIn a public health study on the effects of consumption of fruits and vegetables on psychological well-being in young adults, participants were randomly assigned to three groups: (1) diet-as-usual, (2) an ecological momentary intervention involving text message reminders to increase their fruits and vegetable consumption plus a voucher to purchase them, or (3) a fruit and vegetable intervention in which participants were given two additional daily servings of fresh fruits and vegetables to consume on top of their normal diet. Participants were asked to take a nightly survey on their smartphones. Participants were student volunteers at the University of Otago, New Zealand. At the end of the 14-day study, only participants in the third group showed statistically significant improvements to their psychological well-being across the 14-days relative to the other groups.\n\nWhat type of study is this?\nIdentify the explanatory and response variables.\nComment on whether the results of the study can be generalized to the population.\nComment on whether the results of the study can be used to establish causal relationships.\nA newspaper article reporting on the study states, “The results of this study provide proof that giving young adults fresh fruits and vegetables to eat can have psychological benefits, even over a brief period of time.” How would you suggest revising this statement so that it can be supported by the study?\n\n\n\nQ2. Have a Nice trip2\nAn area of research in biomechanics and gerontology concerns falls and fall-related injuries, especially for elderly people. Recent studies have focused on how individuals respond to large postural disturbances (e.g., tripping, induced slips). One question is whether subjects can be instructed to improve their recovery from such perturbations. Suppose researchers want to compare two such recovery strategies, lowering (making the next step shorter, but in normal step time) and elevating (using a longer or normal step length with normal step time). Subjects will have first been trained on one of these two recovery strategies, and they will be asked to apply it after they feel themselves tripping. The researchers will then induce the subject to trip while walking (but harnessed for safety), using a concealed mechanical obstacle.\nYou will need to access the Randomizing Subjects applet (which simulates a study as described above).\nNote: while gender is not a binary characteristic in humans, there are times when understanding trends in body dimensions (e.g., center of gravity) is useful for understanding biomechanics. We use gender here as an insufficient proxy and not as a claim that all people will fit this model nor as a claim that gender is necessarily the right proxy measure.\n\nOne way to design an experiment for this study would be to assign the eight females to use the elevating strategy and the 16 males to use the lowering strategy. Would this be a reasonable strategy? If not, identify a better method for deciding who uses which strategy.\n\nUse the applet [http://www.rossmanchance.com/applets/2021/sampling/Subjects.html] to randomly assign the participants to the two treatment groups. Repeat the assignment over and over until you get a sense for how the different variables (gender, height, BMI, “gene”) are / can be distributed across the two groups.\n\nDoes random assignment always equally distribute/balance the men and women between the two groups? Give the range of possible discrepancies for the gender imbalance.\nIs there a tendency for there to be a similar proportion of men in the two groups? How are you deciding? What does this tell you about the plausibility of any later difference in the two groups being attributed to females having better balance?\nPrior research has also shown that the likelihood of falling is related to variables such as walking speed, stride rate, and height, so we would like the random assignment to distribute these variables equally between the groups as well. In the applet, use the pull-down menu to switch from the sex variable to the height variable. The dotplot now displays the differences in average height between Group 1 and Group 2 for these 200 repetitions. In the long-run, does random assignment tend to equally distribute the height variable between the two groups? Explain.\nSuppose there is a “balance gene” that is related to people’s ability to recover from a trip. We didn’t know about this gene ahead of time, but if you select the “Reveal gene?” button and then select “gene” from the pull-down menu, the applet shows you this gene information for each subject and also how the proportions with the gene differ in the two groups. Does this variable tend to equalize between the two groups in the long run? Explain.\nSuppose there were other “x-variables” that we could not measure such as BMI, stride rate, or walking speed. Select the “Reveal both?” button and use pull-down menu to display the results for BMI. Does random assignment generally succeed in equalizing this variable between the two groups or is there a tendency for one group to always have higher results for BMI? Explain.\nIs the fact that the people volunteered to participate a “confounding variable” in this study? Explain.\nSuppose the “tripping” study was able to rule out chance as a mechanism for the differences seen in the two strategies. What conclusion would you draw? For what population? What additional information would you need to know?\n\n\n\nQ3. AstraZeneca3\nIn the Wired article about the AstraZeneca vaccine they mention collecting information about age (i.e., one of the variables which was measured was the age of the participant).\n\nIf the vaccine was randomly allocated to the participants, can age be a confounding variable? (Remember, to be a confounding variable you need to have an associating with both the explanatory variable of interest and the response variable of interest).\nCould age be the causal mechanism behind why the vaccine is working? Explain.\nWhy did they collect information about age?\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone.\n\n\n\n\n\n\nFootnotes\n\n\nFrom OpenIntro Statistics, exercise 1.41. Tamlin S Conner et al. “Let them eat fruit! The effect of fruit and vegetable consumption on psychological well-being in young adults: A randomized controlled trial”. In: PloS one 12.2 (2017), e0171206.↩︎\nFrom ISCAM, Inv 3.4↩︎\nhttps://www.wired.com/story/the-astrazeneca-covid-vaccine-data-isnt-up-to-snuff/↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html",
    "href": "slides/lab1_m58_s23_intro_to_data.html",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "",
    "text": "Some define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe the raw information – the data. In this lab we explore flights, specifically a random sample of domestic flights that departed from the three major New York City airport in 2013. We will generate simple graphical and numerical summaries of data on these flights and explore delay times. As this is a large data set, along the way you’ll also learn the indispensable skills of data processing and subsetting.\nThe goals for today include be able to use the following commands:"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab1_m58_s23_intro_to_data.html#advice-for-turning-in-the-assignment",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe ASSIGNMENT part of the lab is ONLY the last four questions at the very bottom. However, you will need many of the commands in the top half, otherwise the commands at the bottom won’t run!"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#getting-started",
    "href": "slides/lab1_m58_s23_intro_to_data.html#getting-started",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nIn this lab we will explore the data using functions that can be found in the tidyverse package. The data can be found in the package nycflights13.\nLet’s load the packages.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\nThe data\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes available transportation data, such as the flights data we will be working with in this lab.\nWe begin by loading the flights data frame. Run the following command by clicking on the green triangle:\n\ndata(flights)\n\nThe data set flights that shows up in your workspace is a data matrix, with each row representing an observation and each column representing a variable. R calls this data format a data frame, which is a term that will be used throughout the labs. For this data set, each observation is a single flight.\nTo view the names of the variables, run the command\n\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\n\nThis returns the names of the variables in this data frame. The codebook (description of the variables) can be accessed by pulling up the help file (run this line with the green triangle and then look at the box on the bottom right of the RStudio screen):\n\n?flights\n\nOne of the variables refers to the carrier (i.e. airline) of the flight, which is coded according to the following system.\n\ncarrier: Two letter carrier abbreviation.\n\n9E: Endeavor Air Inc.\nAA: American Airlines Inc.\nAS: Alaska Airlines Inc.\nB6: JetBlue Airways\nDL: Delta Air Lines Inc.\nEV: ExpressJet Airlines Inc.\nF9: Frontier Airlines Inc.\nFL: AirTran Airways Corporation\nHA: Hawaiian Airlines Inc.\nMQ: Envoy Air\nOO: SkyWest Airlines Inc.\nUA: United Air Lines Inc.\nUS: US Airways Inc.\nVX: Virgin America\nWN: Southwest Airlines Co.\nYV: Mesa Airlines Inc.\n\n\nA very useful function for taking a quick peek at your data frame and viewing its dimensions and data types is glimpse().\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nThe flights data frame is a massive trove of information (336,776 observations!!!). Notice also that the glimpse function let’s you know how the variables are stored: integer, double (a fancy way to say decimal number), character string, date/time, etc. Let’s think about some questions we might want to answer with these data:\n\nHow delayed were flights that were headed to Los Angeles?\nHow do departure delays vary over months?\nWhich of the three major NYC airports has a better on time percentage for departing flights?\n\n\n\nTidy Structure of Data\nFor plotting, analyses, model building, etc., the data should be structured according to certain principles.\n\nTidy Data: rows (cases/observational units) and columns (variables).\nThe key is that every row is a case and every column is a variable.\nNo exceptions.\nCreating tidy data is often not trivial.\n\nWithin R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language.\nSome things to consider:\n\nobject_name <- anything is a way of assigning anything to the new object_name.\nobject_name <- function_name(data_frame, arguments) is a way of using a function to create a new object.\nobject_name <- data_frame %>% function_name(arguments) uses chaining syntax as an extension of the ideas of functions.\nIn chaining, the value on the left side of %>% becomes the first argument to the function on the right side.\n\nobject_name <- data_frame %>%\n                    function_name(arguments) %>% \n                    another_function_name(other_arguments)\nis extended chaining. %>% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %>% is always a data table. * The pipe syntax should be read as and then, %>%.\n\n\nUsing the pipe to chain\nThe pipe syntax (%>%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function.\nFor example:\nx %>% f(y) is the same as f(x, y)\ny %>% f(x, ., z) is the same as f(x,y,z)\nPipes are used commonly with functions to sequentially use data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations."
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#analysis",
    "href": "slides/lab1_m58_s23_intro_to_data.html#analysis",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "Analysis",
    "text": "Analysis\n\nDeparture delays\n\nselect() (chooses which variable(s))\nLet’s start by examining the distribution of departure delays of all flights using the summary() function. The first item (on the left) is the data set. Subsequently, two functions are applied, (1) select() to get only the dep_delay variables, and (2) summary() to produce the numerical summary.\n\nflights %>% \n  select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   : -43.00  \n 1st Qu.:  -5.00  \n Median :  -2.00  \n Mean   :  12.64  \n 3rd Qu.:  11.00  \n Max.   :1301.00  \n NA's   :8255     \n\nsummary(select(flights, dep_delay))\n\n   dep_delay      \n Min.   : -43.00  \n 1st Qu.:  -5.00  \n Median :  -2.00  \n Mean   :  12.64  \n 3rd Qu.:  11.00  \n Max.   :1301.00  \n NA's   :8255     \n\n\n\n\nfilter() (create a smaller dataset, picks rows)\nIf we want to focus only on departure delays of flights headed to Los Angeles, we need to first filter() the data for flights with that destination (dest == \"LAX\"). The departure delay for the LAX flights can then be summarized.\n\nlax_flights <- flights %>%\n  filter(dest == \"LAX\")\n\nlax_flights %>% \n  select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\nLet’s decipher these two commands (It’s common to add a break to a new line after %>% to help readability).\n\nCommand 1: Take the flights data frame, filter() for flights headed to LAX, and save the result as a new data frame called lax_flights.\n\n== means “if it’s equal to”. (notice that there are TWO equals signs)\nLAX is in quotation marks since it is a character string.\n\nCommand 2: Basically the same call for summarizing the departure delay.\n\nNotice that if we only want the summary of dep_delay for the LAX flights (and we don’t need to keep a copy of the dataset), we can perform the above tasks by combining them into fewer steps:\n\nflights %>%\n  filter(dest == \"LAX\") %>%\n  select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\n\nLogical operators:  Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so we use the filter() function and a series of logical operators. The most commonly used logical operators for data analysis are as follows:\n\n== means “equal to”\n!= means “not equal to”\n> or < means “greater than” or “less than”\n>= or <= means “greater than or equal to” or “less than or equal to”\n\n\n\n\nsummarize() (calculate statistics)\nWe can also obtain numerical summaries for these flights:\n\nlax_flights %>%\n  summarize(mean_dd = mean(dep_delay, na.rm = TRUE), \n            median_dd = median(dep_delay, na.rm = TRUE), n_dd = n())\n\n# A tibble: 1 × 3\n  mean_dd median_dd  n_dd\n    <dbl>     <dbl> <int>\n1    9.40        -1 16174\n\n\nNote that in the summarize() function we created a list of three different numerical summaries that we were interested in. The names of these elements are user defined, like mean_dd, median_dd, n_dd, and you could customize these names as you like (but don’t use spaces in your names). Calculating summary statistics also require that you know the function calls. Note that n() reports the sample size.\n\nSummary statistics:  Some useful function calls for summary statistics for a single numerical variable are as follows:\n\nmean()\nmedian()\nsd()\nIQR()\nmin()\nmax()\n\nNote that each of these functions take a single vector as an argument, and returns a single value.\n\nTwo functions you may not be familiar with (and that we will see in more detail in coming weeks) include: \\[\\mbox{sd} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2}\\] \\[\\mbox{IQR} = 75\\% - 25\\%\\]\nWe can also filter based on multiple criteria. Suppose we are interested in flights headed to San Francisco (SFO) in February:\n\nsfo_feb_flights <- flights %>%\n  filter(dest == \"SFO\", month == 2)\n\nNote that we can separate the conditions using commas if we want flights that are both headed to SFO and in February. If we are interested in either flights headed to SFO or in February we can use the | instead of the comma.\n\nCreate a new data frame that includes flights headed to SFO in February, and save this data frame as sfo_feb_flights. How many flights meet these criteria?\n\n\nsfo_feb_flights %>%\n  summarize(n())\n\n# A tibble: 1 × 1\n  `n()`\n  <int>\n1   791\n\n\n\nDescribe the distribution of the arrival delays of these flights using summary and/or appropriate summary statistics.\n\n\nsfo_feb_flights %>%\n  summarize(arrdelmean = mean(arr_delay, na.rm=TRUE), \n            arrdelsd = sd(arr_delay, na.rm=TRUE),\n            arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  arrdelmean arrdelsd arrdelmed arrdeliqr\n       <dbl>    <dbl>     <dbl>     <dbl>\n1      -9.14     31.4       -13        29\n\n\n\n\ngroup_by() (group before summarizing)\nAnother useful technique is quickly calculating summary statistics for various groups in your data frame. For example, we can modify the above command using the group_by() function to get the same summary stats for each origin airport:\n\nsfo_feb_flights %>%\n  group_by(origin) %>%\n  summarize(median_dd = median(dep_delay, na.rm=TRUE), \n            iqr_dd = IQR(dep_delay, na.rm=TRUE), n_flights = n())\n\n# A tibble: 2 × 4\n  origin median_dd iqr_dd n_flights\n  <chr>      <dbl>  <dbl>     <int>\n1 EWR            0      9       194\n2 JFK           -2      8       597\n\n\nHere, we first grouped the data by origin, and then calculated the summary statistics.\n\nCalculate the median and interquartile range for arr_delay of flights in in the sfo_feb_flights data frame, grouped by carrier. Which carrier has the most variable arrival delays (as measured by IQR)?\n\n\nsfo_feb_flights %>% \n  group_by(carrier) %>%\n  summarize(arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE)) %>%\n  arrange(desc(arrdelmed))\n\n# A tibble: 5 × 3\n  carrier arrdelmed arrdeliqr\n  <chr>       <dbl>     <dbl>\n1 AA             -7      35  \n2 UA             -9      27  \n3 B6            -11      25.5\n4 VX            -20      23  \n5 DL            -24      27.5\n\n\n\n\n\narrange() departure delays over months\nWhich month would you expect to have the highest average delay departing from an NYC airport?\nLet’s think about how we would answer this question:\n\nFirst, calculate monthly averages for departure delays. With the new language we are learning, we need to\n\ngroup_by() months, then\nsummarize() mean departure delays.\n\nThen, we need to arrange() these average delays in desc()ending order\nAfter you’ve filled in the blanks, change the chunk option to eval = TRUE to get the code to knit.\n\n\nflights %>%\n  group_by(___) %>%\n  summarize(mean_dd = mean(___, na.rm=TRUE)) %>%\n  arrange(desc(___))\n\n\n\nOn time departure rate for NYC airports\nSuppose you will be flying out of NYC and want to know which of the three major NYC airports has the best on time departure rate of departing flights. Suppose also that for you a flight that is delayed for less than 5 minutes is basically “on time”. You consider any flight delayed for 5 minutes of more to be “delayed”.\nIn order to determine which airport has the best on time departure rate, we need to\n\nfirst classify each flight as “on time” or “delayed”,\nthen group flights by origin airport,\nthen calculate on time departure rates for each origin airport (note that the average of zeros and ones is actually a proportion or rate!),\nand finally arrange the airports in descending order for on time departure percentage.\n\n\nmutate() (create a new variable)\nLet’s start with classifying each flight as “on time” or “delayed” by creating a new variable with the mutate() function.\n\nflights <- flights %>%\n  mutate(dep_type = case_when(\n    dep_delay < 5 ~ \"on time\",\n    TRUE ~ \"delayed\"))\n\nThe first argument in the mutate() function is the name of the new variable we want to create, in this case dep_type. Then if dep_delay < 5 we classify the flight as \"on time\" and \"delayed\" if not, i.e., if the flight is delayed for 5 or more minutes.\nNote that we are also overwriting the flights data frame with the new version of this data frame that includes the new dep_type variable.\nWe can handle all the remaining steps in one code chunk (convince yourself that you understand why a mean() is actually a proportion):\n\nflights %>%\n  group_by(origin) %>%\n  summarize(ot_dep_rate = mean(dep_type == \"on time\", na.rm=TRUE)) %>%\n  arrange(desc(ot_dep_rate))\n\n# A tibble: 3 × 2\n  origin ot_dep_rate\n  <chr>        <dbl>\n1 LGA          0.706\n2 JFK          0.679\n3 EWR          0.622\n\n\n\nIf you were selecting an airport (of the three NYC airports in the dataset) simply based on on time departure percentage, which NYC airport would you choose to fly out of? (How did you define “on time”? 0 min? 5 min? Something else?)"
  },
  {
    "objectID": "slides/lab1_m58_s23_intro_to_data.html#to-turn-in",
    "href": "slides/lab1_m58_s23_intro_to_data.html#to-turn-in",
    "title": "Lab 1 - Math 58B: Introduction to Data",
    "section": "To Turn In",
    "text": "To Turn In\n\nHave you been able to meet with your learning community yet? Anything I should know about getting things up and running?\nMutate the data frame so that it includes a new variable that contains the average speed, avg_speed traveled by the plane for each flight (in mph). Hint: Average speed can be calculated as distance divided by number of hours of travel, and note that air_time is given in minutes.\nAnother useful filtering helper function is between(). What does it do? Use it to find flights that arrived between 0 and 60 minutes late. How many such flights are there? (Hint: you’ll need to use between() inside one of the wrangling verbs. In the console (down below), type ?between to learn what the function does and what the arguments are for the function.)\nSuppose you really dislike departure delays, and you want to schedule your travel in a month that minimizes your potential departure delay leaving NYC. One option is to choose the month with the lowest mean departure delay. Another option is to choose the month with the lowest median departure delay. What are the pros and cons of these two choices? Find the month with the lowest median departure delay and the month with the lowest mean departure delay. Which month do you choose?\nWhich month has the highest average arrival delay from an NYC airport? What about the highest median arrival delay? Which of these measures is more reliable for deciding which month(s) to avoid flying if you really dislike delayed flights.\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Discord! The sooner (and more often) questions get asked, the better for everyone."
  },
  {
    "objectID": "slides/MA58_HW2s23.html",
    "href": "slides/MA58_HW2s23.html",
    "title": "HW 2 – Math 58B",
    "section": "",
    "text": "Assignment Summary (Goals)\n\ncalculating & interpreting correlations\ncalculating & interpreting linear model\n\nNote: you’ll need many of the skills covered in lab 2 to complete the assignment! After Tuesday, the solutions to Lab 2 will be posted on Sakai.\n\nQ1. LC Q\nDescribe one thing you learned from someone in your learning commmunity this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Breaking Ice1\nNenana is a small, interior Alaskan town that holds a famous competition to predict the exact moment that “spring arrives” every year. The arrival of spring is defined to be the moment when the ice on the Tanana River breaks, which is measured by a tripod erected on the ice with a trigger to an official clock. The minute at which the ice breaks has been recorded in every year since 1917. For example, the dates and times for the years 2000-2004 were:\n\n\n\n\n\n\n\n\n\n\n2000\n2001\n2002\n2003\n2004\n\n\n\n\nMay 1, 10:47am\nMay 8, 1:00pm\nMay 7, 9:27pm\nApril 29, 6:22pm\nApril 24, 2:16pm\n\n\n\nThe data file NenanaIceBreak.txt contains all of the data since 1917. Scientists have examined these data for evidence of global warming, which would suggest that the ice break day should be tending to occur earlier as time goes on.\n\nExamine a scatterplot of the day in which the ice broke (date coded in column 7 with April 1 = 1) vs. year. Does it reveal any association between the two variables? In other words, is there any indication that the day on which spring begins is changing over time? Explain.\n\n(n.b., Don’t worry about the earlier columns coded with month and year. For this problem, the focus is on the number of days since April 1.)\n\n# data available from a URL (not an R package)\nice <- read_delim(\"http://www.rossmanchance.com/iscam2/data/NenanaIceBreak.txt\", \"\\t\")\n\n\nDetermine and report the regression line for predicting ice break day from year. Also calculate the correlation coefficient and the value of \\(R^2\\). Comment on what these reveal, including an interpretation of the slope coefficient.\nLet’s say that the conclusion is strongly that the slope is non-zero (as measured by the p-value, but we haven’t learned that yet). Would you say that it reveals evidence of a strong association or strong evidence of an association? Explain.\nDo the data suggest that one can make better predictions by taking year into account, rather than simply using the average of the ice break days? Explain.\nWhat date would the regression model predict for the ice break-up in the year 2005? What about 2020? Explain why you should regard these predictions cautiously.\n\n\n\nQ3. Identify relationships, Part II., IMS Section 7.5 #42\nFor each of the six plots (see text), identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable.\n\n\nQ4. Graduate degrees and salaries., IMS Section 7.5 #143\nWhat would be the correlation between the annual salaries of people with and without a graduate degree at a company if for a certain type of position someone with a graduate degree always made\n\n$5,000 more than those without a graduate degree?\n25% more than those without a graduate degree?\n15% less than those without a graduate degree?\n\n\n\nQ5. The Coast Starlight, regression. IMS Section 7.5 #214\nHint: look at the text for the scatterplot and also for how to compute the slope and intercept by hand. By hand computing will not be a regular thing, but it doesn’t hurt to do it once so as to understand the computation.\nThe Coast Starlight Amtrak train runs from Seattle to Los Angeles. The scatterplot below displays the distance between each stop (in miles) and the amount of time it takes to travel from one stop to another (in minutes). The mean travel time from one stop to the next on the Coast Starlight is 129 mins, with a standard deviation of 113 minutes. The mean distance traveled from one stop to the next is 108 miles with a standard deviation of 99 miles. The correlation between travel time and distance is 0.636.\n\nWrite the equation of the regression line for predicting travel time.\nInterpret the slope and the intercept in this context.\nCalculate \\(R^2\\) of the regression line for predicting travel time from distance traveled for the Coast Starlight, and interpret \\(R^2\\) in the context of the application.\nThe distance between Santa Barbara and Los Angeles is 103 miles. Use the model to estimate the time it takes for the Starlight to travel between these two cities.\nIt actually takes the Coast Starlight about 168 mins to travel from Santa Barbara to Los Angeles. Calculate the residual and explain the meaning of this residual value.\nSuppose Amtrak is considering adding a stop to the Coast Starlight 500 miles away from Los Angeles. Would it be appropriate to use this linear model to predict the travel time from Los Angeles to this point?\n\n\npraise()\n\n[1] \"You are outstanding!\"\n\n\n\n\n\n\n\n\nFootnotes\n\n\nFrom ISCAM, HW 5.39↩︎\nhttps://openintro-ims.netlify.app/model-slr.html#chp7-exercises↩︎\nhttps://openintro-ims.netlify.app/model-slr.html#chp7-exercises↩︎\nhttps://openintro-ims.netlify.app/model-slr.html#chp7-exercises↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html",
    "href": "slides/lab2_m58_s23_2quant.html",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "",
    "text": "Building on the work we’ve done this week to understand quantitative variables, today’s lab will focus on creating linear models and creating scatterplots. First, however, we build up to scatterplots by focusing on how layers of plots work in the ggplot2 package.\nThe goals for today include:"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#advice-for-turning-in-the-assignment",
    "href": "slides/lab2_m58_s23_2quant.html#advice-for-turning-in-the-assignment",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Advice for turning in the assignment",
    "text": "Advice for turning in the assignment\n\nknit early and often. In fact, go ahead and knit your .Rmd file right now. Maybe set a timer so that you knit every 5 minutes. Do not wait until you are done with the assignment to knit.\nThe assignment part of the lab is ONLY the last six questions at the very bottom. However, the commands in the first half of the assignment are key to doing the second half.\nSave the .Rmd file somewhere you can find it. Don’t keep everything in your downloads folder. Maybe make a folder called StatsHW or something. That folder could live on your Desktop. Or maybe in your Dropbox."
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#getting-started",
    "href": "slides/lab2_m58_s23_2quant.html#getting-started",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Getting started",
    "text": "Getting started\nA really great series of examples to work through by Hadley Wickham: https://r4ds.had.co.nz/data-visualisation.html\nSome things to notice:\n\nwhen layering graph pieces, use +. (When layering for data wrangling, use %>%.)\ngeom_XXX() will put the XXX-type-of-plot onto the graph.\naes() is the function which takes the data columns and puts them onto the graph. aes() is used only with data columns and you always need it if you are working with data variables.\nA full set of types of plots is given here: https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf (and in many other places online).\n\n\nLoad packages\nIn this lab we will explore the data using ggplot() which is included in the tidyverse package. We’ll start by looking at the penguin data in the palmerpenguin package. Remember, you’ll need to install the package the first time you use it!\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n\nLayers\nA ggplot has layers. And each layer is connected by the + symbol. We might want to know if flipper_length_m is correlated to body_mass_g. In order to investigate, we’ll create a scatterplot. Every ggplot has a minimum of two layers. The first layer sets the plot, the second later draws the points.\nRun the first layer first:\n\npenguins %>%\n  ggplot()\n\n\n\n\n\n\n\n\nAnd now add some points (note the difference in the use of %>% vs +):\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g))"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#lets-make-some-plots",
    "href": "slides/lab2_m58_s23_2quant.html#lets-make-some-plots",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Let’s make some plots!",
    "text": "Let’s make some plots!\n\naes()\nThe aes() function plays a very specific role in ggplots, and it is worth discussing. The idea behind aes() (which stands for “aesthetic”, but I don’t find that word helpful) is to pull out the columns of the dataframe (the variables).\nLet’s say we want to color the points based on the island from which the penguins came.\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, color = island))\n\n\n\n\n\n\n\n\nOr maybe what we want is to color all of the points blue:\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g, color = \"blue\"))\n\n\n\n\n\n\n\n\nWait, that didn’t work. Why not? It is because blue is not a column in the dataset! (And no, it won’t help if you remove the quotes, but go ahead and try it to see what happens.)\nRemember that the aes() function will only and always pull out the columns of the dataset! So if you want the points to be blue, go ahead and make them blue, just don’t try to color inside the aes()\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g), color = \"blue\")\n\n\n\n\n\n\n\n\nSome other things to try … “size = …” or “shape = …” within the aes() will give you different plotting symbols. Try it out! You might play around with setting size inside aes() with a variable, and setting size outside aes() with a number.\n\n\nAdding layers\nThere are so many layers we can add! Let’s try some…\n\nReformating the title of the x-axis\nSee the code below. What do you think the layer ylab() does? What do you think the layer ggtitle() does? Try them out!\n\npenguins %>%\n  ggplot() +\n  geom_point(aes(x = flipper_length_mm, y = body_mass_g)) +\n  xlab(\"Length of Flipper in mm\")\n\n\n\n\n\n\n\n\n\n\nAdding a regression line\nThe “method” used for adding a line to the plot is “lm” which stands for linear model. Try another option: method = \"loess\". Also, se = FALSE means that we don’t want an error bound on the line. What happens when you set se = TRUE?\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n  xlab(\"Length of Flipper in mm\")\n\n$x\n[1] \"Length of Flipper in mm\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\nWhere does the aes() information go?\nConsider the following information: aes(x = flipper_length_mm, y = body_mass_g, color = island). Type that phrase into the ggplot as follows.\n\nAdd only to “A”\nAdd only to “B”\nAdd only to “C”\nAdd to “B” and “C”\nAdd to “B” and “C” but remove the color = islands for “C”\nAdd to “B” and “C” but remove the color = islands for “B”\n\nAlso, the code below won’t run until after you change the chunk to eval = TRUE:\n\npenguins %>%\n  ggplot(\"A\") +\n  geom_point(\"B\") +\n  geom_smooth(\"C\", method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#lets-calculate-some-statistics",
    "href": "slides/lab2_m58_s23_2quant.html#lets-calculate-some-statistics",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Let’s calculate some statistics!!",
    "text": "Let’s calculate some statistics!!\n\nLinear model based on the least squares regression\nRunning a linear model and/or calculating the correlation is a matter of learning the linear model syntax. For most (all?) of our models we will use the following syntax to tell the model which variable is the response variable and which variable is the explanatory variable:\nresponse variable ~ explanatory variable\nThe linear model takes the data as a later argument, so we use the period . to indicate where the dataset gets piped into the function.\nThe only thing we’ve talked about with respect to the linear model is the first column of the output (called “estimate”). The intercept is \\(b_0 = -5780.83\\) g, and the slope is \\(b_1 = 49.69\\) g/mm.\n\npenguins %>%\n  lm(body_mass_g ~ flipper_length_mm, data = .) %>%\n  tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\n\n\nCorrelation\nThe correlation is always calculated across pairs of variables, but R can calculate the correlation for lots of pairs at a time.\n\nThere are explicit instructions on how to calculate the correlation when there are missing values. Here we used use = \"pairwise.complete.obs\".\nWhy are the diagonal entries all equal to 1?\n\n\npenguins %>%\n  select(body_mass_g, flipper_length_mm, bill_length_mm, bill_depth_mm) %>%\n  cor(use = \"pairwise.complete.obs\")\n\n                  body_mass_g flipper_length_mm bill_length_mm bill_depth_mm\nbody_mass_g         1.0000000         0.8712018      0.5951098    -0.4719156\nflipper_length_mm   0.8712018         1.0000000      0.6561813    -0.5838512\nbill_length_mm      0.5951098         0.6561813      1.0000000    -0.2350529\nbill_depth_mm      -0.4719156        -0.5838512     -0.2350529     1.0000000\n\n\nTo calculate the correlations by group, use group_by() and summarize():\n\npenguins %>%\n  group_by(island) %>%\n  summarize(r_mass_flip = cor(body_mass_g, flipper_length_mm, use = \"pairwise.complete.obs\")) \n\n# A tibble: 3 × 2\n  island    r_mass_flip\n  <fct>           <dbl>\n1 Biscoe          0.877\n2 Dream           0.525\n3 Torgersen       0.436"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#some-graphics-bells-and-whistles-you-dont-need-to-know",
    "href": "slides/lab2_m58_s23_2quant.html#some-graphics-bells-and-whistles-you-dont-need-to-know",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "Some graphics bells and whistles (you don’t need to know)",
    "text": "Some graphics bells and whistles (you don’t need to know)\n\nAdding the value of the correlation\nUsing the ggpubr package, we can add the correlation to the plot:\n\n# sometimes people have trouble loading the ggpubr package\n# if you are having trouble with ggpubr, change the code chunk\n# to:  ```{r eval = FALSE}\nlibrary(ggpubr)\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  stat_cor(aes(label = ..r.label..)) +\n  xlab(\"Length of Flipper in mm\")\n\n\n\n\n\n\n\n\n\n\nFaceting plots\nOften the visualization is much better if the plots are distinct from one another. We use faceting to create different plots.\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\nWe can facet by more than one variable:\n\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(year~island)"
  },
  {
    "objectID": "slides/lab2_m58_s23_2quant.html#to-turn-in",
    "href": "slides/lab2_m58_s23_2quant.html#to-turn-in",
    "title": "Lab 2 - Math 58B: Two Quantitative Variables",
    "section": "To Turn In",
    "text": "To Turn In\nThe data which will be used on the part of the lab to be turned in (called gapminder) can be found in the package dslabs.\nLet’s load the packages.\n\nlibrary(tidyverse)  # ggplot lives in the tidyverse\nlibrary(dslabs)  # dataset for the lab\ndata(gapminder)\n\n\nThe data\nGapminder is a creation of Hans Rosling and his team to collect and visualize country level information over time. The dslabs R package and gapminder dataset provide a few interesting variables for us to work with.\n\nnames(gapminder)\n\n[1] \"country\"          \"year\"             \"infant_mortality\" \"life_expectancy\" \n[5] \"fertility\"        \"population\"       \"gdp\"              \"continent\"       \n[9] \"region\"          \n\n\n\nglimpse(gapminder)\n\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…\n\n\n\nQ1. LC Q\nDescribe one thing you learned from someone in your learning community this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2.\nCreate a scatterplot with life expectancy on the y-axis and fertility rate on the x-axis. Color the points based on their continent. Change the x and y axis labels to be more descriptive. (Note: the plot isn’t going to be very beautiful.)\n\n\nQ3.\nMake two graphs which are similar to the plot in the previous questions. First, use only observations from 1962 by filter()ing on year. Make a second scatterplot for observations from 2015. (It is possible, and fun!, to use filter() to get both years and then to apply facet_wrap() to create side-by-side plots.)\nName one difference and one similarity in the pair of plots from 1962 and 2015.\n\n\nQ4.\nOn each of the separate plots (1962 and 2015), add the linear model to the plot (separate linear model for each continent).\nAlso add (to each of the 1962 and 2015 plots separately) a single linear regression which is fit on all countries together simultaneously. Color the overall regression line black.\n\n\nQ5.\nCalculate the correlation between life expectancy and fertility separately for the two years above and each continent (that is, calculate 10 different correlation values). Provide 1-2 sentences describing how the numeric correlation values are consistent with what you observed in the plot above.\nHint1: use filter() only once to get both years using the “or” command: |.\nHint2: group_by() can take two arguments (separated by a comma)!\nYour code should be only four lines and look something like this.\ndata %>%\n   filter(...) %>%\n   group_by(...) %>%\n   summarize(...)\n\n\nQ6.\nCalculate the intercept and slope describing the least squares regression model regressing life expectancy (the response variable) on fertility (the explanatory variable). Use 2015 and all continents. Write down the full model (hint: when writing the model down, either use a hat or add a residual).\n\npraise()\n\n[1] \"You are great!\"\n\n\n\n\nHW & Lab assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n\n\n\nScore & Description\n5 points: All problems completed with detailed solutions provided and 75% or more of the problems are fully correct.\n4 points: All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. An assignment will earn a 4 if there is superfluous information printed out on the assignment.\n3 points: Close to all problems completed with less than 75% correct\n2 points: More than half but fewer than all problems completed and > 75% correct\n1 point: More than half but fewer than all problems completed and < 75% correct; OR less than half of problems completed\n0 points: No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions.\n\n\nGeneral notes on homework assignments (also see syllabus for policies and suggestions):\n\nplease be neat and organized, this will help me, the grader, and you (in the future) to follow your work.\nbe sure to include your name on the assignment\nplease include at least the number of the problem, or a summary of this question (this will also be helpful to you in the future to prepare for exams).\nfor R problems, it is required to use R Markdown. You can write out other problems with pencil and combine pdf as appropriate.\nplease do not print errors, messages, warnings, or anything else that makes your homework unwieldy. You will be graded down for superfluous printouts.\nin case of questions, or if you get stuck please don’t hesitate to email me or DM on Slack! The sooner (and more often) questions get asked, the better for everyone."
  }
]